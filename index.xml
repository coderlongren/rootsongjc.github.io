<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/</link>
    <description>Recent content on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 May 2017 12:59:19 +0800</lastBuildDate>
    
	<atom:link href="http://rootsongjc.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubernetes的边缘节点配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</link>
      <pubDate>Tue, 09 May 2017 12:59:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</guid>
      <description>（题图：南屏晚钟@圆明园 May 6,2017）
前言 为了配置kubernetes中的traefik ingress的高可用，对于kubernetes集群以外只暴露一个访问入口，需要使用keepalived排除单点问题。本文参考了kube-keepalived-vip，但并没有使用容器方式安装，而是直接在node节点上安装。
本文已同步到gitbook kubernetes-handbook的第2章第5节。
定义 首先解释下什么叫边缘节点（Edge Node），所谓的边缘节点即集群内部用来向集群外暴露服务能力的节点，集群外部的服务通过该节点来调用集群内部的服务，边缘节点是集群内外交流的一个Endpoint。
边缘节点要考虑两个问题
 边缘节点的高可用，不能有单点故障，否则整个kubernetes集群将不可用 对外的一致暴露端口，即只能有一个外网访问IP和端口  架构 为了满足边缘节点的以上需求，我们使用keepalived来实现。
在Kubernetes集群外部配置nginx来访问边缘节点的VIP。
选择Kubernetes的三个node作为边缘节点，并安装keepalived。
准备 复用kubernetes测试集群的三台主机。
172.20.0.113
172.20.0.114
172.20.0.115
安装 使用keepalived管理VIP，VIP是使用IPVS创建的，IPVS已经成为linux内核的模块，不需要安装
LVS的工作原理请参考：http://www.cnblogs.com/codebean/archive/2011/07/25/2116043.html
不使用镜像方式安装了，直接手动安装，指定三个节点为边缘节点（Edge node）。
因为我们的测试集群一共只有三个node，所有在在三个node上都要安装keepalived和ipvsadmin。
yum install keepalived ipvsadmin  配置说明 需要对原先的traefik ingress进行改造，从以Deployment方式启动改成DeamonSet。还需要指定一个与node在同一网段的IP地址作为VIP，我们指定成172.20.0.119，配置keepalived前需要先保证这个IP没有被分配。。
 Traefik以DaemonSet的方式启动 通过nodeSelector选择边缘节点 通过hostPort暴露端口 当前VIP漂移到了172.20.0.115上 Traefik根据访问的host和path配置，将流量转发到相应的service上  配置keepalived 参考基于keepalived 实现VIP转移，lvs，nginx的高可用，配置keepalived。
keepalived的官方配置文档见：http://keepalived.org/pdf/UserGuide.pdf
配置文件/etc/keepalived/keepalived.conf文件内容如下：
! Configuration File for keepalived global_defs { notification_email { root@localhost } notification_email_from kaadmin@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 172.</description>
    </item>
    
    <item>
      <title>在kubernetes中使用glusterfs做持久化存储</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</link>
      <pubDate>Thu, 04 May 2017 20:06:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</guid>
      <description>（题图：无题@北京奥林匹克森林公园 May 1,2017）
前言 本文章已同步到kubernetes-handbook 7.1章节。
Kubernetes集群沿用跟我一起部署kubernetes1.6集群中的三台机器。
我们复用kubernetes集群的这三台主机做glusterfs存储。
以下步骤参考自：https://www.xf80.com/2017/04/21/kubernetes-glusterfs/
安装glusterfs 我们直接在物理机上使用yum安装，如果你选择在kubernetes上安装，请参考：https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md
# 先安装 gluster 源 $ yum install centos-release-gluster -y # 安装 glusterfs 组件 $ yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel ## 创建 glusterfs 目录 $ mkdir /opt/glusterd ## 修改 glusterd 目录 $ sed -i &#39;s/var\/lib/opt/g&#39; /etc/glusterfs/glusterd.vol # 启动 glusterfs $ systemctl start glusterd.service # 设置开机启动 $ systemctl enable glusterd.service #查看状态 $ systemctl status glusterd.service  配置 glusterfs # 配置 hosts $ vi /etc/hosts 172.</description>
    </item>
    
    <item>
      <title>Kubernetes网络和集群性能测试</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</link>
      <pubDate>Tue, 25 Apr 2017 22:14:49 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</guid>
      <description>（题图：无题@安贞门 Jun 18,2016）
前言 该测试是为了测试在不同的场景下，访问kubernetes的延迟以及kubernetes的性能。进行以下测试前，你需要有一个部署好的kubernetes集群，关于如何部署kuberentes1.6集群，请参考kubernetes-handbook。
准备 测试环境
在以下几种环境下进行测试：
 Kubernetes集群node节点上通过Cluster IP方式访问 Kubernetes集群内部通过service访问 Kubernetes集群外部通过traefik ingress暴露的地址访问  测试地址
Cluster IP: 10.254.149.31
Service Port：8000
Ingress Host：traefik.sample-webapp.io
测试工具
 Locust：一个简单易用的用户负载测试工具，用来测试web或其他系统能够同时处理的并发用户数。 curl kubemark 测试程序：sample-webapp，源码见Github kubernetes的分布式负载测试  测试说明
通过向sample-webapp发送curl请求获取响应时间，直接curl后的结果为：
$ curl &amp;quot;http://10.254.149.31:8000/&amp;quot; Welcome to the &amp;quot;Distributed Load Testing Using Kubernetes&amp;quot; sample web app  网络延迟测试 场景一、 Kubernetes集群node节点上通过Cluster IP访问 测试命令
curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://10.254.149.31:8000/&amp;quot;  10组测试结果
   No time_connect time_starttransfer time_total     1 0.</description>
    </item>
    
    <item>
      <title>运用kubernetes进行分布式负载测试</title>
      <link>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 21:32:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</guid>
      <description>（题图：Kubrick Book Store Mar 25,2016）
前言 Github地址https://github.com/rootsongjc/distributed-load-testing-using-kubernetes
该教程描述如何在Kubernetes中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看Distributed Load Testing Using Kubernetes 。
注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。
准备 不需要GCE及其他组件，你只需要有一个kubernetes集群即可。
如果你还没有kubernetes集群，可以参考kubernetes-handbook部署一个。
部署Web应用 sample-webapp 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像index.tenxcloud.com/jimmy/k8s-sample-webapp:latest。
在kubernetes上部署sample-webapp。
$ cd kubernetes-config $ kubectl create -f sample-webapp-controller.yaml $ kubectl create -f kubectl create -f sample-webapp-service.yaml  部署Locust的Controller和Service locust-master和locust-work使用同样的docker镜像，修改cotnroller中spec.template.spec.containers.env字段中的value为你sample-webapp service的名字。
- name: TARGET_HOST value: http://sample-webapp:8000  创建Controller Docker镜像（可选） locust-master和locust-work controller使用的都是locust-tasks docker镜像。你可以直接下载gcr.io/cloud-solutions-http://olz1di9xf.bkt.clouddn.com/locust-tasks，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。
$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest . $ docker push index.tenxcloud.com/jimmy/locust-tasks:latest  注意：我使用的是时速云的镜像仓库。
每个controller的yaml的spec.template.spec.containers.image 字段指定的是我的镜像：</description>
    </item>
    
    <item>
      <title>Kubernetes中的IP和服务发现体系</title>
      <link>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 16:11:16 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</guid>
      <description>（题图：路边的野花@朝阳公园 Nov 8,2015）
Cluster IP 即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。
Service Name才是对外暴露服务的关键。
在kubeapi的配置中指定该地址范围。
默认配置
--service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=30000-32767  Pod IP 通过配置flannel的network和subnet来实现。
默认配置
FLANNEL_NETWORK=172.30.0.0/16 FLANNEL_SUBNET=172.30.46.1/24  Pod的IP地址不固定，当pod重启时IP地址会变化。
该IP地址也是用户无需关心的。
但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。
主机/Node IP 物理机的IP地址，即kubernetes管理的物理机的IP地址。
$ kubectl get nodes NAME STATUS AGE VERSION 172.20.0.113 Ready 12d v1.6.0 172.20.0.114 Ready 12d v1.6.0 172.20.0.115 Ready 12d v1.6.0  服务发现 集群内部的服务发现
通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。
通过sericename:port即可调用服务。
服务外部的服务发现
通过Ingress来实现，我们是用的Traefik来实现。
参考 Ingress解析
Kubernetes Traefik Ingress安装试用</description>
    </item>
    
    <item>
      <title>Kubernetes中的RBAC支持</title>
      <link>http://rootsongjc.github.io/blogs/rbac-support-in-kubernetes/</link>
      <pubDate>Fri, 21 Apr 2017 19:53:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/rbac-support-in-kubernetes/</guid>
      <description>（题图：无题 Apr 2,2016）
 在Kubernetes1.6版本中新增角色访问控制机制（Role-Based Access，RBAC）让集群管理员可以针对特定使用者或服务账号的角色，进行更精确的资源访问控制。在RBAC中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。在一个组织中，角色是为了完成各种工作而创造，用户则依据它的责任和资格来被指派相应的角色，用户可以很容易地从一个角色被指派到另一个角色。
 前言 本文翻译自RBAC Support in Kubernetes，转载自kubernetes中文社区，译者催总，Jimmy Song做了稍许修改。该文章是5天内了解Kubernetes1.6新特性的系列文章之一。
One of the highlights of the Kubernetes 1.6中的一个亮点时RBAC访问控制机制升级到了beta版本。RBAC，基于角色的访问控制机制，是用来管理kubernetes集群中资源访问权限的机制。使用RBAC可以很方便的更新访问授权策略而不用重启集群。
本文主要关注新特性和最佳实践。
RBAC vs ABAC 目前kubernetes中已经有一系列l 鉴权机制。鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。
ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。
而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。
因为 Kubernetes 社区的投入和偏好，相对于 ABAC 而言，RBAC 是更好的选择。</description>
    </item>
    
    <item>
      <title>Kubernetes traefik ingress安装试用</title>
      <link>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</link>
      <pubDate>Thu, 20 Apr 2017 22:38:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</guid>
      <description>（题图：🐟@鱼缸 Sep 15,2016）
前言 昨天翻了下Ingress解析，然后安装试用了下traefik，过程已同步到kubernetes-handbook上，Github地址https://github.com/rootsongjc/kubernetes-handbook
Ingress简介 如果你还不了解，ingress是什么，可以先看下我翻译的Kubernetes官网上ingress的介绍Kubernetes Ingress解析。
理解Ingress
简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。
理解Ingress Controller
Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。
部署Traefik 介绍traefik
Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。
以下配置文件可以在kubernetes-handbookGitHub仓库中的manifests/traefik-ingress/目录下找到。
创建ingress-rbac.yaml
将用于service account验证。
apiVersion: v1 kind: ServiceAccount metadata: name: ingress namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ingress subjects: - kind: ServiceAccount name: ingress namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes ingress解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</link>
      <pubDate>Wed, 19 Apr 2017 21:05:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</guid>
      <description>（题图：朝阳门银河SOHO Jan 31,2016）
前言 这是kubernete官方文档中Ingress Resource的翻译，因为最近工作中用到，文章也不长，也很好理解，索性翻译一下，也便于自己加深理解，同时造福kubernetes中文社区。后续准备使用Traefik来做Ingress controller，文章末尾给出了几个相关链接，实际使用案例正在摸索中，届时相关安装文档和配置说明将同步更新到kubernetes-handbook中。
术语
在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。
 节点：Kubernetes集群中的一台物理机或者虚拟机。 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。
 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。
 集群网络：一组逻辑或物理链接，可根据Kubernetes网络模型实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的OVS。
 服务：使用标签选择器标识一组pod成为的Kubernetes服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。
  什么是Ingress？ 通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：
 internet | ------------ [ Services ]  Ingress是授权入站连接到达集群服务的规则集合。
 internet | [ Ingress ] --|-----|-- [ Services ]  你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。
先决条件 在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。
GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 运行多个ingress controller 和 关闭glbc.
确定你已经阅读了Ingress controller的beta版本限制。在非GCE/GKE的环境中，你需要在pod中部署一个controller。
Ingress Resource 最简化的Ingress配置：
1: apiVersion: extensions/v1beta1 2: kind: Ingress 3: metadata: 4: name: test-ingress 5: spec: 6: rules: 7: - http: 8: paths: 9: - path: /testpath 10: backend: 11: serviceName: test 12: servicePort: 80  如果你没有配置Ingress controller就将其POST到API server不会有任何用处</description>
    </item>
    
    <item>
      <title>Kubernetes Handbook发起</title>
      <link>http://rootsongjc.github.io/projects/kubernetes-handbook-startup/</link>
      <pubDate>Fri, 14 Apr 2017 19:33:27 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/kubernetes-handbook-startup/</guid>
      <description>（题图：地坛公园 Sep 27,2015）
玩转Kubernetes，我就看kubernetes handbook！
GitHub地址：https://github.com/rootsongjc/kubernetes-handbook
文章同步更新到gitbook，方便大家浏览和下载PDF。
说明 文中涉及的配置文件和代码链接在gitbook中会无法打开，请下载github源码后，在MarkDown编辑器中打开，点击链接将跳转到你的本地目录。
如何使用 方式一：在线浏览
访问gitbook：https://www.gitbook.com/book/rootsongjc/kubernetes-handbook/
方式二：本地查看
 将代码克隆到本地 安装gitbook：Setup and Installation of GitBook 执行gitbook serve 在浏览器中访问http://localhost:4000  P.S 本书中也将记录业界动态和经验分享，欢迎分享你的独到见解。
加入Kubernetes交流群，请添加我微信jimmysong。</description>
    </item>
    
    <item>
      <title>Kubernetes1.6集群部署完全指南 ——二进制文件部署开启TLS基于CentOS7发布</title>
      <link>http://rootsongjc.github.io/projects/kubernetes-installation-document/</link>
      <pubDate>Thu, 13 Apr 2017 14:00:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/kubernetes-installation-document/</guid>
      <description>（题图：清晨@首都机场 Aug 13,2016）
这可能是目前为止最详细的kubernetes安装文档了。
经过几天的安装、调试、整理，今天该文档终于发布了。
你可以在这里看到文档和配置文件和我一步步部署 kubernetes1.6 集群。
或者直接下载pdf版本（2.92M）。
Kubernetes的安装繁琐，步骤复杂，该文档能够帮助跳过很多坑，节约不少时间，我在本地环境上已经安装完成，有问题欢迎在GitHub上提issue。
安装的集群详情
 Kubernetes 1.6.0 Docker 1.12.5（使用yum安装） Etcd 3.1.5 Flanneld 0.7 vxlan 网络 TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node) RBAC 授权 kublet TLS BootStrapping kubedns、dashboard、heapster(influxdb、grafana)、EFK(elasticsearch、fluentd、kibana) 集群插件 私有docker镜像仓库harbor（请自行部署，harbor提供离线安装包，直接使用docker-compose启动即可）  该文档中包括以下几个步骤
 创建 TLS 通信所需的证书和秘钥 创建 kubeconfig 文件 创建三节点的高可用 etcd 集群 kubectl命令行工具 部署高可用 master 集群 部署 node 节点 kubedns 插件 Dashboard 插件 Heapster 插件 EFK 插件  </description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装EFK</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</link>
      <pubDate>Thu, 13 Apr 2017 12:28:10 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</guid>
      <description>（题图：簋街 Jun 17,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署EFK日志收集监控插件。
配置和安装 EFK 官方文件目录：cluster/addons/fluentd-elasticsearch
$ ls *.yaml es-controller.yaml es-service.yaml fluentd-es-ds.yaml kibana-controller.yaml kibana-service.yaml efk-rbac.yaml  同样EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。
已经修改好的 yaml 文件见：EFK
配置 es-controller.yaml $ diff es-controller.yaml.orig es-controller.yaml 24c24 &amp;lt; - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 --- &amp;gt; - image: sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2  配置 es-service.yaml 无需配置；
配置 fluentd-es-ds.yaml $ diff fluentd-es-ds.yaml.orig fluentd-es-ds.yaml 26c26 &amp;lt; image: gcr.io/google_containers/fluentd-elasticsearch:1.22 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22  配置 kibana-controller.yaml $ diff kibana-controller.yaml.orig kibana-controller.yaml 22c22 &amp;lt; image: gcr.io/google_containers/kibana:v4.6.1-1 --- &amp;gt; image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装heapster</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 20:20:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</guid>
      <description>（题图：大喵 Aug 8,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署heapster，包括influxdb、grafana等组件。
配置和安装Heapster 到 heapster release 页面 下载最新版本的 heapster。
$ wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip $ unzip v1.3.0.zip $ mv v1.3.0.zip heapster-1.3.0  文件目录： heapster-1.3.0/deploy/kube-config/influxdb
$ cd heapster-1.3.0/deploy/kube-config/influxdb $ ls *.yaml grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml heapster-rbac.yaml  我们自己创建了heapster的rbac配置heapster-rbac.yaml。
已经修改好的 yaml 文件见：heapster
配置 grafana-deployment $ diff grafana-deployment.yaml.orig grafana-deployment.yaml 16c16 &amp;lt; image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 40,41c40,41 &amp;lt; # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;lt; value: / --- &amp;gt; value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;gt; #value: /  如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为 /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://10.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装dashboard</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 15:53:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</guid>
      <description>（题图：东直门桥 Aug 20,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署dashboard。
感谢opsnull和ipchy的细心解答。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  配置和安装 dashboard 官方文件目录：kubernetes/cluster/addons/dashboard
我们使用的文件
$ ls *.yaml dashboard-controller.yaml dashboard-service.yaml dashboard-rbac.yaml  已经修改好的 yaml 文件见：dashboard
由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：
Forbidden (403) User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)  增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。
配置dashboard-service $ diff dashboard-service.yaml.orig dashboard-service.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之kubedns配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</link>
      <pubDate>Wed, 12 Apr 2017 13:04:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</guid>
      <description>（题图：雨过天晴@北京定福庄 Aug 27,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，使用yaml文件部署kubedns。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  安装和配置 kubedns 插件 官方的yaml文件目录：kubernetes/cluster/addons/dns。
该插件直接使用kubernetes部署，官方的配置文件中包含以下镜像：
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1  我clone了上述镜像，上传到我的私有镜像仓库：
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:1.14.1  同时上传了一份到时速云备份：
index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1  以下yaml配置文件中使用的是私有镜像仓库中的镜像。
kubedns-cm.yaml kubedns-sa.yaml kubedns-controller.yaml kubedns-svc.yaml  已经修改好的 yaml 文件见：github项目中的manifest/kubedns/目录。
系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；
$ kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes node节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</link>
      <pubDate>Tue, 11 Apr 2017 22:20:31 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</guid>
      <description>（题图：太阳宫桥@北京东北三环 Dec 11,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署node节点上的kube-proxy和kubelet，同时对之前部署的flannel改造。
安装环境配置信息
 CentOS7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  部署kubernetes node节点 kubernetes node 节点包含如下组件：
 Flanneld：参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。 Docker1.12.5：docker的安装很简单，这里也不说了。 kubelet kube-proxy  下面着重讲kubelet和kube-proxy的安装，同时还要将之前安装的flannel集成TLS验证。
目录和文件 我们再检查一下三个节点上，经过前几步操作生成的配置文件。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem $ ls /etc/kubernetes/ apiserver bootstrap.kubeconfig config controller-manager kubelet kube-proxy.kubeconfig proxy scheduler ssl token.csv  配置Flanneld 参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。
service配置文件/usr/lib/systemd/system/flanneld.service。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.</description>
    </item>
    
    <item>
      <title>Kubernetes高可用master节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</link>
      <pubDate>Tue, 11 Apr 2017 19:55:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</guid>
      <description>（题图：鬼见愁@北京西山 Sep 14,2015）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署master节点的kube-apiserver、kube-controller-manager和kube-scheduler的过程。
高可用kubernetes master节点安装 kubernetes master 节点包含的组件：
 kube-apiserver kube-scheduler kube-controller-manager  目前这三个组件需要部署在同一台机器上。
 kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关； 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；  本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）
TLS 证书文件 pem和token.csv证书文件我们在TLS证书和秘钥这一步中已经创建过了。我们再检查一下。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem  下载最新版本的二进制文件 有两种下载方式
方式一
从 github release 页面 下载发布版 tarball，解压后再执行下载脚本
$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz $ tar -xzvf kubernetes.tar.gz ... $ cd kubernetes $ .</description>
    </item>
    
    <item>
      <title>Kubernetes安装之etcd高可用配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</link>
      <pubDate>Tue, 11 Apr 2017 15:21:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</guid>
      <description>（题图：北京夜景@西山）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。
创建高可用 etcd 集群 kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为sz-pg-oam-docker-test-001.tendcloud.com、sz-pg-oam-docker-test-002.tendcloud.com、sz-pg-oam-docker-test-003.tendcloud.com：
 sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113 sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114 sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115  TLS 认证文件 需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl   kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；  下载二进制文件 到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件
$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz $ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz $ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin  创建 etcd 的 systemd unit 文件 注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；</description>
    </item>
    
    <item>
      <title>Kubernetes安装之创建kubeconfig文件</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</link>
      <pubDate>Tue, 11 Apr 2017 14:34:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</guid>
      <description>(题图：北海公园 May 8,2016)
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。 kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权； kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。
创建 TLS Bootstrapping Token Token auth file
Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;) cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot; EOF   后三行是一句，直接复制上面的脚本运行即可。
 将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。
$cp token.csv /etc/kubernetes/  创建 kubelet bootstrapping kubeconfig 文件 $ cd /etc/kubernetes $ export KUBE_APISERVER=&amp;quot;https://172.</description>
    </item>
    
    <item>
      <title>开源微服务管理平台fabric8简介</title>
      <link>http://rootsongjc.github.io/blogs/fabric8-introduction/</link>
      <pubDate>Mon, 10 Apr 2017 21:39:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/fabric8-introduction/</guid>
      <description>前言 无意中发现Fabric8这个对于Java友好的开源微服务管理平台。
其实这在这里发现的Achieving CI/CD with Kubernetes（by Ramit Surana,on February 17, 2017），其实是先在slideshare上看到的，pdf可以在此下载，大小2.04M。
大家可能以前听过一个叫做fabric的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把fabric8跟fabric搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，fabric8不是fabric的一个版本。Fabric是用python开发的，fabric8是java开发的。
如果你想了解简化Fabric可以看它的中文官方文档。
Fabric8简介 fabric8是一个开源集成开发平台，为基于Kubernetes和Jenkins的微服务提供持续发布。
使用fabric可以很方便的通过Continuous Delivery pipelines创建、编译、部署和测试微服务，然后通过Continuous Improvement和ChatOps运行和管理他们。
Fabric8微服务平台提供：
 Developer Console，是一个富web应用，提供一个单页面来创建、编辑、编译、部署和测试微服务。 Continuous Integration and Continous Delivery，使用 Jenkins with a Jenkins Workflow Library更快和更可靠的交付软件。 Management，集中式管理Logging、Metrics, ChatOps、Chaos Monkey，使用Hawtio和Jolokia管理Java Containers。 Integration Integration Platform As A Service with deep visualisation of your Apache Camel integration services, an API Registry to view of all your RESTful and SOAP APIs and Fabric8 MQ provides Messaging As A Service based on Apache ActiveMQ。 Java Tools 帮助Java应用使用Kubernetes:  Maven Plugin for working with Kubernetes ，这真是极好的 Integration and System Testing of Kubernetes resources easily inside JUnit with Arquillian Java Libraries and support for CDI extensions for working with Kubernetes.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之证书验证</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</link>
      <pubDate>Mon, 10 Apr 2017 17:28:41 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</guid>
      <description>（题图：铜牛@颐和园 Aug 25,2014）
前言 昨晚（Apr 9,2017）金山软件的opsnull发布了一个开源项目和我一步步部署kubernetes集群，下文是结合我之前部署kubernetes的过程打造的kubernetes环境和opsnull的文章创建 kubernetes 各组件 TLS 加密通信的证书和秘钥的实践。之前安装过程中一直使用的是非加密方式，一直到后来使用Fluentd和ElasticSearch收集Kubernetes集群日志时发现有权限验证问题，所以为了深入研究kubernentes。
Kubernentes中的身份验证 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
生成的 CA 证书和秘钥文件如下：
 ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem  使用证书的组件如下：
 etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem；  kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。
安装 CFSSL 方式一：直接使用二进制源码包安装
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 $ chmod +x cfssl_linux-amd64 $ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl $ wget https://pkg.</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践（第二版）》读后感</title>
      <link>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</link>
      <pubDate>Sat, 08 Apr 2017 12:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</guid>
      <description>（题图：长江三峡大坝@湖北宜昌 Apr 6,2015）
前言 最近（2017年3月）友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，点这里下载，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。这是该书的第二版，第一版2014年9月出版，第二版2016年9月出版，第二版的编者团队居然有50人之多😓
第二版分享了华为在云计算核心竞争力构建与价值转换方面的经验与建议，并补充了业界在公有云、私有云、行业云以及电信网络云化商用落地与技术应用方面的成功优秀实践。增加了对Docker容器与微服务敏捷迭代、大数据与数据库云化、行业建模与机器学习算法、混合云与管理自动化编排、云生态建设等方面的介绍。
第1章 云计算的商业动力与技术趋势 ​
​
​
​
​
​</description>
    </item>
    
    <item>
      <title>使用Fluentd和ElasticSearch收集Kubernetes集群日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</link>
      <pubDate>Fri, 07 Apr 2017 20:13:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</guid>
      <description>（题图：码头@古北水镇 Apr 30,2016）
前言 在安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard和配置Heapster监控插件后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。
官方文档
Kubernetes Logging and Monitoring Cluster Activity
Logging Using Elasticsearch and Kibana：不过这篇文章是在GCE上配置的，参考价值不大。
容器日志的存在形式 目前容器日志有两种输出形式：
stdout,stderr标准输出
这种形式的日志输出我们可以直接使用docker logs查看日志，kubernetes集群中同样可以使用kubectl logs类似的形式查看日志。
日志文件记录
这种日志输出我们无法从以上方法查看日志内容，只能tail日志文件查看。
Fluentd介绍 Fluentd是使用Ruby编写的，通过在后端系统之间提供统一的日志记录层来从后端系统中解耦数据源。 此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。 统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。 也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。
Fluentd收集的event由以下几个方面组成：
 Tag：字符串，中间用点隔开，如myapp.access Time：UNIX时间格式 Record：JSON格式  Fluentd特点  部署简单灵活 开源 经过验证的可靠性和性能 社区支持，插件较多 使用json格式事件格式 可拔插的架构设计 低资源要求 内置高可靠性  安装 查看cluster/addons/fluentd-elasticsearch插件目录，获取到需要用到的docker镜像名称。
$grep -rn &amp;quot;gcr.io&amp;quot; *.yaml es-controller.yaml:24: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 fluentd-es-ds.yaml:26: image: gcr.io/google_containers/fluentd-elasticsearch:1.22 kibana-controller.yaml:22: image: gcr.io/google_containers/kibana:v4.6.1-1  需要用到的镜像
 gcr.io/google_containers/kibana:v4.6.1-1 gcr.io/google_containers/elasticsearch:v2.4.1-2 gcr.</description>
    </item>
    
    <item>
      <title>Kubernetes的ConfigMap解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</link>
      <pubDate>Thu, 06 Apr 2017 21:24:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</guid>
      <description>（题图：龙形灯笼@古北水镇 Apr 30,2016）
前言 为什么要翻译这篇文章，是因为我在使用Fluentd和ElasticSearch收集Kubernetes集群日志的时候遇到了需要修改镜像中配置的问题，fluent-plugin-kubernetes_metadata里的需要的td-agent.conf文件。
其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
ConfigMap概览 ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。
kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3  data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：
 设置环境变量的值 在容器里设置命令行参数 在数据卷里面创建config文件  用户和系统组件两者都可以在ConfigMap里面存储配置数据。
其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。
Examples: # Create a new configmap named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.</description>
    </item>
    
    <item>
      <title>TensorFlow深度学习手写数字识别初体验</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</link>
      <pubDate>Wed, 05 Apr 2017 21:52:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</guid>
      <description>（题图：禾雀 @北京动物园 Apr 3,2017）
前言 TensorFlow学习曲线是陡峭的，不是所有的IT从业人员都很容易参与的，你需要有一定的数学专业知识，对于对深度学习没有经验的程序员，要想了解这门技术，最快捷的途径是先运行一个示例，我们认识事物都是先从感性、到理性的思辨过程。
下面我们来跟随Martin Gorner的TensorFlow and Deep Learing Without a PhD来编写我们的第一个TensorFlow程序——手写数字识别，这篇文章的中文版没有博士学位如何玩转TensorFlow和深度学习于2017年3月13日发表在发表在机器之心上。这篇文章也是根据3月8日-10日的Google Cloud NEXT&amp;rsquo;17大会上Martin Gorner做的讲解整理而成的，教程 | 没有博士学位，照样玩转TensorFlow深度学习这篇文章是对Martin Gorner的简易教程的原文翻译，我们暂时不要求了解TensorFlow背后复杂的理论，我们先跟随这篇简易教程玩一把TensorFlow的手写数字识别。
如果你想深入了解这本后的原理的话，可以查看哈尔滨工业大学社会计算与信息检索研究中心翻译的《神经网络与深度学习》这本书，该书翻译自Neural Networks and Deep Learning的中文翻译，原文作者 Michael Nielsen，而且这还是一本免费的电子书，该书中系统讲解了使用神经网络识别手写数字背后的原理。该书托管在GitBook上，你可以点击这里直接下载该书中文版的PDF。
准备 下载代码
这个代码仓库里包含了手写数字识别和下载依赖的训练数据的代码，我们将只用到mnist_1.0_softmax.py这一个代码文件。整个mnist_1.0_softmax.py代码并不复杂，不算注释的话只有36行。
git clone https://github.com/martin-gorner/tensorflow-mnist-tutorial.git  下载完后，可以看到有一个INSTALL.txt，这篇文章是运行代码所必需的环境要求说明。
安装TensorFlow
我之前写过详细的TensorFlow安装教程TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建，这篇文章中主要讲怎样在docker里安装TensorFlow。
我使用的Mac而且还是python2.7，所以我这样安装：
pip install --upgrade tensorflow --user -U pip install --upgrade matplotlib --user -U  运行示例 运行手写数字训练示例。
python mnist_1.0_softmax.py  运行过程中你会看到一大段输出：
Collecting matplotlib Downloading matplotlib-2.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (12.8MB) 100% |████████████████████████████████| 12.8MB 26kB/s Requirement already up-to-date: pyparsing!</description>
    </item>
    
    <item>
      <title>Kubernetes heapster监控插件安装文档</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</link>
      <pubDate>Wed, 05 Apr 2017 18:41:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</guid>
      <description>（题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）
前言 前面几篇文章中记录了我们安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard，但是还没法查看Pod的监控信息，虽然kubelet默认集成了cAdvisor（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。
安装 下载heapster的代码
直接现在Github上的最新代码。
git pull https://github.com/kubernetes/heapster.git  目前的最高版本是1.3.0。
在heapster/deploy/kube-config/influxdb目录下有几个yaml文件：
grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml  我们再看下用了哪些镜像：
grafana-deployment.yaml:16: image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 heapster-deployment.yaml:16: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 influxdb-deployment.yaml:16: image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1  下载镜像
我们下载好了这些images后，存储到私有镜像仓库里：
 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1  我已经将官方镜像克隆到了时速云上，镜像地址：
 index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1 index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2  需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。
docker login index.tendcloud.com  配置 参考Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI和Configuring Source，需要修改yaml文件中的几个配置。
 首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的 修改heapster-deployment.</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard/Web UI安装全记录</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</link>
      <pubDate>Wed, 05 Apr 2017 14:28:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</guid>
      <description>（题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）
前言 前几天在CentOS7.2上安装Kubernetes1.6和安装好flannel网络配置，今天我们来安装下kuberentnes的dashboard。
Dashboard是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟Docker Enterprise Edition的Docker Datacenter很像了。
安装Dashboard 官网的安装文档https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。
首先需要一个kubernetes-dashboard.yaml的配置文件，可以直接在Github的src/deploy/kubernetes-dashboard.yaml下载。
我们能看下这个文件的内容：
# Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</description>
    </item>
    
    <item>
      <title>容器技术在大数据场景下的应用——Yarn on Docker</title>
      <link>http://rootsongjc.github.io/projects/yarn-on-docker/</link>
      <pubDate>Tue, 04 Apr 2017 00:19:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/yarn-on-docker/</guid>
      <description>作者：宋净超 TalkingData云计算及大数据工程师
前言 我已就该话题已在2016年上海Qcon上发表过演讲，点此观看。
另外InfoQ网站上的文字版数据中心的Yarn on Docker集群方案，即本文。
项目代码开源在Github上：Magpie
当前数据中心存在的问题 数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源 数据中心中普遍存在如下问题：
1.主机资源利用率低
2.部署和扩展复杂
3.资源隔离无法动态调整
4.无法快速响应业务
为何使用Yarnon Docker
彻底隔离队列 
• 为了合理利用Hadoopyarn的资源，队列间会互相抢占计算资源，造成重要任务阻塞
• 根据部门申请的机器数量划分Yarn集群方便财务管理
• 更细粒度的资源分配 统一的资源分配
• 每个NodeManager和容器都可以限定CPU、内存资源
• Yarn资源划分精确到CPU核数和内存大小 弹性伸缩性服务
• 每个容器中运行一个NodeManager，增减yarn资源只需增减容器个数
• 可以指定每个NodeManager拥有的计算资源多少，按需申请资源 给我们带来什么好处？  Swarm统一集群资源调度
 • 统一资源
• 增加Docker虚拟化层，降低运维成本
增加Hadoop集群资源利用率
• Fordatacenter：避免了静态资源隔离
• Forcluster：加强集群内部资源隔离
 系统架构  比如数据中心中运行的Hadoop集群，我们将HDFS依然运行在物理机上，即DataNode依然部署在实体机器上，将Yarn计算层运行在Docker容器中，整个系统使用二层资源调度，Spark、Flinek、MapReduce等应用运行在Yarn上。
  Swarm调度最底层的主机硬件资源，CPU和内存封装为Docker容器，容器中运行NodeManager，提供给Yarn集群，一个Swarm集群中可以运行多个Yarn集群，形成圈地式的Yarn计算集群。
具体流程
1.swarm node向swarm master注册主机资源并加入到swarmcluster中
2.swarm master向cluster申请资源请求启动容器
3.swarm根据调度策略选择在某个node上启动dockercontainer
4.swarm node的docker deamon根据容器启动参数启动相应资源大小的NodeManager
5.NodeManager自动向YARN的ResourceManager注册资源一个NodeManager资源添加完成。
 Swarm为数据中心做容器即主机资源调度，每个swarmnode的节点结构如图：
一个Swarmnode就是一台物理机，每台主机上可以起多个同类型的dockercontainer，每个container的资源都有限制包括CPU、内存NodeManager容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是24核64G，我们可以分给一个容器5核12G，NodeManager占用4核10G的资源提供给Yarn。</description>
    </item>
    
    <item>
      <title>两款图片处理工具——Google Guetzli和基于AI的Deep Photo Style Transfer</title>
      <link>http://rootsongjc.github.io/talks/picture-process/</link>
      <pubDate>Sun, 02 Apr 2017 20:27:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/picture-process/</guid>
      <description>前言 如果你看过美剧「硅谷」会记得剧中主角们所在的创业公司PiedPipper，他们就是靠自己发明的视频压缩算法来跟大公司Hooli竞争的，这部剧现在已经发展到第4季，在腾讯视频上可以免费观看。
最近关注了两个图像处理的Open Source Projects。
 Google Guetzli 图像压缩工具 Luan Fujun&amp;rsquo;s Deep Photo Style Transfer 图像style转换工具  另外对于图像处理还处于Photoshop、Lightroom这种摄影后期和图像处理命令行工具ImageMagick的我来说，图像压缩，智能图像风格转换实乃上乘武功，不是我等凡夫俗子驾驭的了，但是乘兴而来，总不能败兴而归吧，下面我们来一探究竟。
Google Guetzli 聊聊架构微信公众号上有一篇介绍Google开源新算法，可将JPEG文件缩小35%文章。
我在Mac上试用了一下，安装很简单，只要一条命令：
brew install guetzli  但是当我拿一张22M大小的照片使用guetzli压缩的时候，我是绝望的，先后三次kill掉了进程。
因为实在是太慢了，也能是我软件对内存和CPU的利用率不高，效果你们自己看看。
原图是这个样子的，拍摄地点在景山上的，俯瞰紫禁城的绝佳位置。
guetzli --quality 84 --verbose 20160403052.jpg output.jpg  为什么quality要设置成84呢？因为只能设置为84+的quality，如果要设置的更低的话需要自己修改代码。
耗时了一个小时，后台进程信息。
这个是使用Squash压缩后的大小效果，压缩每张照片差不多只要3秒钟。
 Squash的logo就是个正在被剥皮的🍊，这是下载地址。
 压缩比分别为70%和30%。
压缩比70%后的细节放大图
压缩比30%的细节放大图
你看出什么区别了吗？反正我是没有。
下面再来看看耗时一个小时，千呼万唤始出来的guetzli压缩后的效果和使用squash压缩比为30%的效果对比。
左面是使用guetzli压缩后（4.1M），右面使用的squash压缩后（3.1M）的照片。
似乎还是没有什么区别啊？你看出来了吗？
Guetzli总结 可能是我使用Guetzli的方式不对，但是命令行里确实没有设置CPU和内存资源的选项啊，为啥压缩照片会这么慢呢？效果也并不出彩，不改代码的话照片质量只能设置成84以上，但是这个是Open Source的，使用的C++开发，可以研究下它的图像压缩算法。
Deep Photo Style Transfer 来自康奈尔大学的Luan Fujun开源的图像sytle转换工具，看了README的介绍，上面有很多图像风格转换的例子，真的很惊艳，市面上好像还没有这种能够在给定任意一张照片的情况下，自动将另一张照片转换成该照片的style。
这个工具使用Matlab和Lua开发，基于Torch运行的时候需要CUDA，cudnn，Matlab，环境实在太复杂，就没折腾，启动有人发布Docker镜像，已经有人提了issue。
如果它能够被商用，绝对是继Prisma后又一人工智能照片处理应用利器。
后记 是不是有了照片风格转换这个东西就不需要做照片后期了？只要选几张自己喜欢的风格照片，再鼠标点几下就可以完成照片处理了？摄影师要失业了？非也！照片风格东西本来就是很主观性的，每个人都有自己喜欢的风格，照相机发明后就有人说画家要失业了，其实不然，画画依然是创造性地劳动，只能说很多写实风格的画家要失业了。Deep Photo Style Transfer也许会成为Lightroom或者手机上一款app的功能，是一个不错的工具。也许还会成为像Prisma一样的现象级产品，who knows?🤷‍♂️</description>
    </item>
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>（题图：西安鼓楼 Oct 4,2014）
书接上文在CentOS中安装Kubernetes详细指南，这是一个系列文章，作为学习Kubernetes的心路历程吧。
本文主要讲解Kubernetes的网络配置，👆文中有一个安装Flannel的步骤，但是安装好后并没有相应的配置说明。
配置flannel 我们直接使用的yum安装的flannle，安装好后会生成/usr/lib/systemd/system/flanneld.service配置文件。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service  可以看到flannel环境变量配置文件在/etc/sysconfig/flanneld。
# Flanneld configuration options # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot; # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot; # Any additional options that you want to pass #FLANNEL_OPTIONS=&amp;quot;&amp;quot;   etcd的地址FLANNEL_ETCD_ENDPOINT etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX  在etcd中创建网络配置</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第三章TensorFlow入门</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</link>
      <pubDate>Thu, 30 Mar 2017 21:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</guid>
      <description>（题图：扬州东关 May 24,2015）
这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
这一章从三个角度带大家入门。
分别是TensorFlow的
 计算模型 数据模型 运行模型  3.1 TensorFlow的计算模型——图计算 计算图是TensorFlow中的一个最基本的概念，TensorFlow中的所有计算都会转化成计算图上的节点。
其实TensorFlow的名字已经暗示了它的实现方式了，Tensor表示的是数据结构——张量，Flow表示数据流——Tensor通过数据流相互转化。
常用的方法
 在python中导入tensorflow：import tensorflow as tf 获取当前默认的计算图：tf.get_default_graph() 生成新的计算图：tf.Graph()  书中这里都有例子讲解，可以从Github中下载代码，或者如果你使用才云提供的docker镜像的方式安装的话，在jupyter中可以看到各个章节的代码。
定义两个不同的图
import tensorflow as tf g1 = tf.Graph() with g1.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.zeros_initializer) # 设置初始值为0 g2 = tf.Graph() with g2.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.ones_initializer()) # 设置初始值为1 with tf.Session(graph = g1) as sess: tf.global_variables_initializer().run() with tf.variable_scope(&amp;quot;&amp;quot;, reuse=True): print(sess.</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>（题图：北京圆明园 Aug 25,2014）
作者：Jimmy Song，Peter Ma，2017年3月30日
最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是CNCF的成员之一。
这篇是根据官方安装文档实践整理的，操作系统是纯净的CentOS7.2。
另外还有一个Peter Ma写的在CentOS上手动安装kubernetes的文档可以参考。
角色分配
下面以在三台主机上安装Kubernetes为例。
172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel 172.20.0.114 node kubectl kube-proxy flannel 172.20.0.115 node kubectl kube-proxy flannel  第一台主机既作为master也作为node。
系统环境
 Centos 7.2.1511 docker 1.12.6 etcd 3.1.5 kubernetes 1.6.0 flannel 0.7.0-1  安装 下面给出两种安装方式：
 配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。 使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。  我们最终选择使用第二种方式安装。
本文的很多安装步骤和命令是参考的Kubernetes官网CentOS Manual Config文档。
第一种方式：CentOS系统中直接使用yum安装 给yum源增加一个Repo
[virt7-docker-common-release] name=virt7-docker-common-release baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/ gpgcheck=0  安装docker、kubernetes、etcd、flannel一步到位
yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel  安装好了之后需要修改一系列配置文件。</description>
    </item>
    
    <item>
      <title>Go语言中的并发编程总结</title>
      <link>http://rootsongjc.github.io/projects/golang-concurrency-summary/</link>
      <pubDate>Fri, 24 Mar 2017 08:36:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/golang-concurrency-summary/</guid>
      <description>Go语言并发编程总结  Golang :不要通过共享内存来通信，而应该通过通信来共享内存。这句风靡在Go社区的话,说的就是 goroutine中的 channel。他在go并发编程中充当着类型安全的管道作用。
 1、通过golang中的 goroutine 与sync.Mutex进行并发同步 import( &amp;quot;fmt&amp;quot; &amp;quot;sync&amp;quot; &amp;quot;runtime&amp;quot; ) var count int =0; func counter(lock * sync.Mutex){ lock.Lock() count++ fmt.Println(count) lock.Unlock() } func main(){ lock:=&amp;amp;sync.Mutex{} for i:=0;i&amp;lt;10;i++{ //传递指针是为了防止 函数内的锁和 调用锁不一致 go counter(lock) } for{ lock.Lock() c:=count lock.Unlock() ///把时间片给别的goroutine 未来某个时刻运行该routine runtime.Gosched() if c&amp;gt;=10{ fmt.Println(&amp;quot;goroutine end&amp;quot;) break } } }  2、goroutine之间通过 channel进行通信 channel是和类型相关的 可以理解为 是一种类型安全的管道。
简单的channel 使用
package main import &amp;quot;fmt&amp;quot; func Count(ch chan int) { ch &amp;lt;- 1 fmt.</description>
    </item>
    
    <item>
      <title>Pivotal Cloud foundry快速开始指南</title>
      <link>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</link>
      <pubDate>Thu, 23 Mar 2017 22:54:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</guid>
      <description>（题图：黄山日出后的云海 Oct 3,2013）
前言 最近研究了下Pivotal的Cloud foundry，CF本身是一款开源软件，很多PAAS厂商都加入了CF，我们用的是的PCF Dev（PCF Dev是一款可以在工作站上运行的轻量级PCF安装）来试用的，因为它可以部署在自己的环境里，而Pivotal Web Services只免费两个月，之后就要收费。这里有官方的详细教程。
开始 根据官网的示例，我们将运行一个Java程序示例。
安装命令行终端
下载后双击安装即可，然后执行cf help能够看到帮助。
安装PCF Dev
先下载，如果你没有Pivotal network账号的话，还需要注册个用户，然后用以下命令安装：
$./pcfdev-VERSION-osx &amp;amp;&amp;amp; \ cf dev start Less than 4096 MB of free memory detected, continue (y/N): &amp;gt; y Please sign in with your Pivotal Network account. Need an account? Join Pivotal Network: https://network.pivotal.io Email&amp;gt; 849122844@qq.com Password&amp;gt; Downloading VM... Progress: |+++++++++++++=======&amp;gt;| 100% VM downloaded. Allocating 4096 MB out of 16384 MB total system memory (3514 MB free).</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</link>
      <pubDate>Thu, 23 Mar 2017 19:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</guid>
      <description>（题图：广州海珠桥 Aug 10,2014）
 这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
 P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
睇完这一章后应该就可以自己搭建出一个TensorFlow的环境，我之前在docker里玩过，镜像比较大，下载慢一点，不过用起来很方便，如果你仅仅是想试用一下TensorFlow，看看它能干什么的话，可以直接在docker里试用一下。在Mac上安装的详细步骤，官方安装说明文档。
2.1 TensorFlow的主要依赖包 TensorFlow主要用到以下两个依赖：
 Protocol buffer：数据结构化工具。Google开源的结构化数据格式，用于网络传输数据时候的序列化和反序列化，使用的时候需要先定义schema，github地址https://github.com/google/protobuf。分布式TensorFlow使用到额gRPC也是使用Protocol Buffer来组织的， Bazel:自动化编译构建工具。Google开源的，github地址https://github.com/bazelbuild/bazel，它支持多语言、多平台、可重复编译和可伸缩，构建大型软件速度也是很快的。Bazel使用**项目空间**的形式管理编译的，每个项目空间需要包含[BUILD文件](https://github.com/tensorflow/tensorflow/blob/master/bower.BUILD)（定义编译目标）和[WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)文件（定义编译的依赖环境）。这两个文件都有点类似python语法。  2.2 TensorFlow安装 TensorFlow的安装方式包括docker镜像、pip安装、源码编译安装。
我选择最方便的docker镜像方式，其他方式对本地环境做很多配置，折腾起来比较麻烦。
我早就在docker中安装过TensorFlow0.9小试过牛刀。现在1.0.1版本已经released了。TensorFlow的所有版本都有对应的docker镜像发布在docker hub，可以直接docker pull安装。
为了和书中所用的镜像保持统一，我将使用caicloud提供的镜像，基于TensorFlow0.12.0（这个版本是2016年12月20日发布的），他们增加了一些其他机器学习工具包和TensorFlow可视化工具TensorBoard。
docker镜像方式安装
首先下载镜像，这个image比较大，下载下来比较费时间，我用了差不多15分钟吧。
docker pull cargo.caicloud.io/tensorflow/tensorflow:0.12.0  下载下来后我们再check下这个大小为1.41GB镜像的layers。
另外还有个nvidia版本的docker，可以将你电脑的GPU派山用场，我暂时没用到GPU，我电脑装的是docker17.03-ce，就不折腾GPU版本的TensorFlow了。
IMAGE CREATED CREATED BY SIZE COMMENT c8a8409297f2 5 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;/run_tf.sh&amp;quot;] 0 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY file:78332d36244852... 122 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY dir:8b6ab7d235e3975.</description>
    </item>
    
    <item>
      <title>容器的应用场景</title>
      <link>http://rootsongjc.github.io/talks/container-applications-scenarios/</link>
      <pubDate>Thu, 23 Mar 2017 15:26:11 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/container-applications-scenarios/</guid>
      <description>（题图：深圳大梅沙 Aug 1,2014）
 如果你对容器到底有什么用存在疑惑的话，推荐你看下我今天碰到的一篇阿里云的容器服务-产品简介-应用场景的文章，觉得比较好，把容器的典型应用场景都概括了，容器对于互联网的弹性扩展和微服务架构有很好的应用场景，P.S这里不是在帮阿里云做广告，这里的推荐搭配确实是很多常用的配置选项。
 DevOps 持续交付 最优化的持续交付流程
配合 Jenkins 帮您自动完成从代码提交到应用部署的 DevOps 完整流程，确保只有通过自动测试的代码才能交付和部署，高效替代业内部署复杂、迭代缓慢的传统方式。
能够实现：
 DevOps 自动化  实现从代码变更到代码构建，镜像构建和应用部署的全流程自动化。
 环境一致性  容器技术让您交付的不仅是代码，还有基于不可变架构的运行环境。
 持续反馈  每次集成或交付，都会第一时间将结果实时反馈。
推荐搭配使用：
云服务器 ECS + 容器服务
基于高性能计算的机器学习 专注机器学习本身，快速实现从 0 到 1
帮助数据工程师在 HPC 集群上轻松部署机器学习应用，跟踪试验和训练、发布模型，数据部署在分布式存储，无需关心繁琐部署运维，专注核心业务，快速从 0 到 1。
能够实现：
 快速弹性  一键部署机器学习应用，秒级启动和弹性伸缩。
 简单可控  一行配置轻松获取 GPU 计算能力，并且可以监控 GPU 的资源。
 深度整合  无缝接入阿里云存储、日志监控和安全基础架构能力。
推荐搭配使用：
高性能计算 (Alibaba Cloud HPC) + 容器服务 + 阿里云文件存储 NAS + 对象存储 OSS</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第一章深度学习简介</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</link>
      <pubDate>Mon, 20 Mar 2017 22:04:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</guid>
      <description>（题图：TensofFlow实战图书封面）
🙏电子工业出版社编辑赠书，能够这么快的拿到这本书，也🙏才云科技的郑泽宇大哥耐心的写了这本书，能够让我等小白一窥深度学习的真容。另外要强烈推荐下这本书，这是本TensorFlow深度学习很好的入门书。书中提供的代码下载地址，整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
1.1 人工智能、机器学习与深度学习 这一节是讲解三者之间的关系。
首先以垃圾邮件分类问题引入机器学习的逻辑回归算法。
逻辑回归算法的准确性取决于训练数据中的特征的提取，以及训练的数据数量。
文章中又提了一个从实体中提取特征的例子：通过笛卡尔坐标系活极角坐标系来表示不同颜色的点，看看能否用一条直线划分。这个例子用来说明一旦解决了数据表达和特征提取，很多人工智能的问题就能迎刃而解。
深度学习是机器学习的一个分支，除了能够学习特征和任务之间的关联之外，还能自动从简单特征中提取更加复杂的特征，这是其区别于机器学习的关键点。
总的来说，人工智能&amp;gt;机器学习&amp;gt;深度学习。
1.2深度学习的发展历程 本节介绍了深度网络历史的三个发展阶段。
2012年的ImageNet图像分类竞赛上，深度学习系统AlexNet赢得冠军，自此深度学习作为深层神经网络的代名词而被人熟知。
1.3深度学习的应用 这一节讲的是深度学习的应用，首先还是从ImageNet的图像识别开始，应用到了OCR（提到了卷积神经网络）、语音识别（提到了混合搞高斯模型）、自然语言处理（提到了语料库、单词向量、机器翻译、情感分析）、人机对弈（提到了AlphaGO）。
1.4 深度学习工具介绍与对比 TensorFlow的渊源是Google大脑团队在2011年开发，在内部使用的DistBelief，并赢得了ImageNet 2014年的比赛，TF是其开源版本，还发表了一篇论文TensorFlow: Large-Scale Machine Learning on Heteogeneous Distributed systems，这就跟当年的HDFS、MapReduce一个套路啊。
Google还把它用来做RankBrain和很多其他的产品线上使用。
当然，还有很多其他的深度学习工具，比如Caffe、Deeplearning4j、Torch等不一而足。从各种指标来看，TensorFlow都是目前最受关注的深度学习框架。</description>
    </item>
    
    <item>
      <title>Docker源码分析第一篇——代码结构</title>
      <link>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</link>
      <pubDate>Sun, 19 Mar 2017 23:00:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</guid>
      <description>(题图：北京八达岭长城 Oct 1,2015)
前言 之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下docker的源码，在网上找到一些相关资料，都比较过时了，发现*孙宏亮*大哥写过一本书叫《Docker源码分析》，而且之前也在InfoQ上陆续发过一些文章，虽然文章都比较老了，基于老的docker版本，但我认为依然有阅读的价值。起码能有这三方面收获：
 一是培养阅读源码的思维方式，为自己阅读docker源码提供借鉴。 二是可以了解docker版本的来龙去脉。 三还可以作为Go语言项目开发作为借鉴。  下载地址 鉴于这本书已经发行一年半了了，基于的docker版本还是1.2.0，而如今都到了1.13.0（docker17.03的老版本号），应该很少有人买了吧，可以说这本书的纸质版本的生命周期也差不多了吧。如果有人感兴趣可以下载pdf版本看看，Docker源码解析-机械工业出版社-孙宏亮著-2015年8月（完整文字版，大小25.86M），Docker源码解析-看云整理版（文字版，有缩略，大小7.62M）。
Out-of-date 有一点必须再次强调一下，这本书中的docker源码分析是基于docker1.2.0，而这个版本的docker源码在github上已经无法下载到了，github上available的最低版本的docker源码是1.4.1。
 顺便感叹一句，科技行业发展实在太快了，尤其是互联网，一本书能连续用上三年都不过时，如果这样的话那么这门技术恐怕都就要被淘汰了吧？
 总体架构 Docker总体上是用的是Client/Server模式，所有的命令都可以通过RESTful接口传递。
整个Docker软件的架构中可以分成三个角色：
 Daemon：常驻后台运行的进程，接收客户端请求，管理docker容器。 Client：命令行终端，包装命令发送API请求。 Engine：真正处理客户端请求的后端程序。  代码结构 Docker的代码结构比较清晰，分成的目录比较多，有以下这些：
 api：定义API，使用了Swagger2.0这个工具来生成API，配置文件在api/swagger.yaml builder：用来build docker镜像的包，看来历史比较悠久了 bundles：这个包是在进行docker源码编译和开发环境搭建的时候用到的，编译生成的二进制文件都在这里。 cli：使用cobra工具生成的docker客户端命令行解析器。 client：接收cli的请求，调用RESTful API中的接口，向server端发送http请求。 cmd：其中包括docker和dockerd两个包，他们分别包含了客户端和服务端的main函数入口。 container：容器的配置管理，对不同的platform适配。 contrib：这个目录包括一些有用的脚本、镜像和其他非docker core中的部分。 daemon：这个包中将docker deamon运行时状态expose出来。 distribution：负责docker镜像的pull、push和镜像仓库的维护。 dockerversion：编译的时候自动生成的。 docs：文档。这个目录已经不再维护，文档在另一个仓库里https://github.com/docker/docker.github.io/。 experimental：从docker1.13.0版本起开始增加了实验特性。 hack：创建docker开发环境和编译打包时用到的脚本和配置文件。 image：用于构建docker镜像的。 integration-cli：集成测试 layer：管理 union file system driver上的read-only和read-write mounts。 libcontainerd：访问内核中的容器系统调用。 man：生成man pages。 migrate：将老版本的graph目录转换成新的metadata。 oci：Open Container Interface库 opts：命令行的选项库。 pkg： plugin：docker插件后端实现包。 profiles：里面有apparmor和seccomp两个目录。用于内核访问控制。 project：项目管理的一些说明文档。 reference：处理docker store中镜像的reference。 registry：docker registry的实现。 restartmanager：处理重启后的动作。 runconfig：配置格式解码和校验。 vendor：各种依赖包。 volume：docker volume的实现。  下一篇将讲解docker的各个功能模块和原理。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://rootsongjc.github.io/about/</link>
      <pubDate>Sat, 18 Mar 2017 20:10:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/about/</guid>
      <description>About me  Jimmy Song Beijing, China rootsongjc@gmail.com Wechat: jimmysong  What I like Movie, Kubrick, Miyazaki, animation, Akira Kurosawa, photography, Daido Moriyama, potato, sony, apple, orange, Fellini, douban, google, Netease cloud music, Jay Chou, badminton, travelling, programming, golang, Java, Python, sea, blue, Henri Cartier-Bresson, Command &amp;amp; Conqure, Italy, Mac, pinao.
What I dislike Chattering, Chemical fiber clothes, noise.</description>
    </item>
    
    <item>
      <title>React入门</title>
      <link>http://rootsongjc.github.io/talks/react-tryout/</link>
      <pubDate>Sat, 18 Mar 2017 10:07:13 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/react-tryout/</guid>
      <description>前言 前端无疑是2016年最火热的技术，没有之一，2017年依然🔥。现在不会点前端技术都不好意思出去见人。
各种前端mvc框架层出不穷，angular js，vue，React，前端组件化开发概念已经深入人心。作为开发者，学习下前端设计也是有必要的，一来页面有些小的设计问题可以自己解决，同时还能提高自己的审美，提高网站的ui设计水平。
今天看到一本书《React Up and Running》的中文版本《React快速上手开发》出版了，英文版可以在这里下载。最近我在翻译的书Cloud Native Go中的实例也使用了React来构建Web应用程序，因此在网上找了一些React资料学习下。
必备基础技能 前端技能汇总这个项目详细记录 了前端工程师牵涉到的各方面知识。在具备基本技能之后可以在里面找到学习 的方向，完善技能和知识面。
frontend-dev-bookmarks是老外总结的前端开发资源。覆盖面非常广。包括各种知识点、工具、技术，非常全面。
以下是个人觉得入门阶段应该熟练掌握的基础技能：
 HTML4，HTML5语法、标签、语义 CSS2.1，CSS3规范，与HTML结合实现各种布局、效果 Ecma-262定义的javascript的语言核心，原生客户端javascript，DOM操作，HTML5新增功能 一个成熟的客户端javascript库，推荐jquery 一门服务器端语言：如果有服务器端开发经验，使用已经会的语言即可，如果没有服务器端开发经验，熟悉Java可以选择Servlet，不熟悉的可以选PHP，能实现简单登陆注册功能就足够支持前端开发了，后续可能需要继续学习，最基本要求是实现简单的功能模拟， HTTP  在掌握以上基础技能之后，工作中遇到需要的技术也能快速学习。
基本开发工具 恰当的工具能有效提高学习效率，将重点放在知识本身，在出现问题时能快速定位并 解决问题，以下是个人觉得必备的前端开发工具：
 文本编辑器：推荐Sublime Text，支持各种插件、主题、设置，使用方便 浏览器：推荐Google Chrome，更新快，对前端各种标准提供了非常好的支持 调试工具：推荐Chrome自带的Chrome develop tools，可以轻松查看DOM结构、样式，通过控制台输出调试信息，调试javascript，查看网络等 辅助工具：PhotoShop编辑图片、取色，fireworks量尺寸，AlloyDesigner对比尺寸，以及前面的到的Chrome develop tools， 翻墙工具：Shadowsocks、云梯VPN  学习方法和学习目标 方法：
 入门阶段反复阅读经典书籍的中文版，书籍中的每一个例子都动手实现并在浏览器中查看效果 在具备一定基础之后可以上网搜各种教程、demo，了解各种功能的实际用法和常见功能的实现方法 阅读HTML，CSS，Javascript标准全面完善知识点 阅读前端牛人的博客、文章提升对知识的理解 善用搜索引擎  目标：
 熟记前面知识点部分的重要概念，结合学习经历得到自己的理解 熟悉常见功能的实现方法，如常见CSS布局，Tab控件等。  入门之路 以下是入门阶段不错的书籍和资料
 HTML先看《HTML &amp;amp; CSS: Design and Build Websites》1-9章，然后《HTML5: The Missing Manual》1-4章。 CSS先看《CSS: The Missing Manual》，然后《CSS权威指南》 javascript先看《javascript高级程序设计》，然后《javascript权威指南》 HTTP看HTTP权威指南 在整个学习过程中HTML CSS JavaScript会有很多地方需要互相结合，实际工作中也是这样，一个简单的功能模块都需要三者结合才能实现。 动手是学习的重要组成部分，书籍重点讲解知识点，例子可能不是很充足，这就需要利用搜索引擎寻找一些简单教程，照着教程实现功能。以下是一些比较好的教程网址  可以搜索各大公司前端校招笔试面试题作为练习题或者他人总结的前端面试题还有个人总结的面试题（带参考答案） http://code.</description>
    </item>
    
    <item>
      <title>零基础使用Hugo和GitHub Pages创建自己的博客</title>
      <link>http://rootsongjc.github.io/talks/building-github-pages-with-hugo/</link>
      <pubDate>Fri, 17 Mar 2017 22:08:25 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/building-github-pages-with-hugo/</guid>
      <description>（题图：🦅 北京动物园 Oct 5,2015）
 亲，你还在为虚拟主机、域名、空间而发愁吗？你想拥有自己的网站吗？你想拥有一个分享知识、留住感动，为开源事业而奋斗终身吗？那么赶快拿起你手中的📱拨打16899168，不对，是看这篇文章吧，不用998，也不用168，这一切都是免费的，是的你没看错，真的不要钱！
 准备 当然还是需要你有一点电脑基础的，会不会编程不要紧，还要会一点英文，你需要先申请一下几个账号和安装一些软件环境：
 GitHub 这是必需的，因为你需要使用Github Pages来托管你的网站。而且你还需要安装git工具。创建一个以自己用户名命名的username.github.io的project。 七牛云存储 非必需，为了存储文件方便，建议申请一个，免费10G的存储空间，存储照片和一些小文件是足够的，可以用来做外链，方便存储和管理，这样你就不用把图片也托管到Github上了。流量也是不限的。我没有收七牛的一点好处，以为是我自己用的，所以推荐给大家，七牛还有命令行客户端，方便你上传和同步文件。如上的题图都是存储在七牛云中的。 百度统计 非必需，基本的网站数据分析，免费的，质量还行。还有微信公众号可以查看，这一点我发现腾讯分析居然都没有微信公众号，自家的产品咋都不推出微信客户端呢。顺便提一下，这个统计账号跟你的百度账号不是同一个东西，两者是两套体系，当然你可以和自己的百度账号关联。只需要在Web的Header中植入一段JS代码即可。 Hugo 必需的，静态网站生成工具，用来编译静态网站的。跟Hexo比起来我更喜欢这个工具。 Typro 非必需，但是强烈推荐，我最喜欢的免费的Markdown编辑器，hugo可以编译markdown格式为HTML，所以用它来写博客是最合适不过了。  好了注册好Github后你现在可以尽情的玩耍了！😄
Let&amp;rsquo;s rock&amp;amp;roll! 首先介绍下Hugo
Hugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。
静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。
采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。
静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo在速度方面做得非常好，Dan Hersam在他这个Hugo教程里提到，5000篇文章的博客，Hugo生成整个网站只花了6秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用Hugo生成整个网站只需要0.1秒。官方文档提供的数据是每篇页面的生成时间不到1ms。
认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo会将Markdown格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在Sublime Text里用Markdown写博客，再敲一行命令生成同步到服务器就OK了。整个体验是不是非常优雅简单还有点geek的味道呢？
了解Hugo 首先建立自己的网站，mysite是网站的路径
$ hugo new site mysite  然后进入该路径
$ cd mysite  在该目录下你可以看到以下几个目录和config.toml文件
 ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml  config.toml是网站的配置文件，包括baseurl, title, copyright等等网站参数。
这几个文件夹的作用分别是：
 archetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用markdown格式 layouts：包括了网站的模版，决定内容如何呈现 static：包括了css, js, fonts, media等，决定网站的外观  Hugo提供了一些完整的主题可以使用，下载这些主题：</description>
    </item>
    
    <item>
      <title>Contiv Ultimate-Docker17.03CE下思科docker网络插件contiv趟坑终极版</title>
      <link>http://rootsongjc.github.io/blogs/contiv-ultimate/</link>
      <pubDate>Fri, 17 Mar 2017 17:52:37 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-ultimate/</guid>
      <description>（题图：广州石牌桥 Aug 10,2014）
前几天写的几篇关于Contiv的文章已经把引入坑了😂
今天这篇文章将带领大家用正确的姿势编译和打包一个contiv netplugin。
 请一定要在Linux环境中编译。docker中编译也会报错，最好还是搞个虚拟🐔吧，最好还有VPN能翻墙。
 环境准备 我使用的是docker17.03-CE、安装了open vSwitch(这个包redhat的源里没有，需要自己的编译安装)，如果你懒得编译可以用我编译的rpm包，点这里下载。
编译 这一步是很容易失败的，有人提过issue-779
具体步骤
 创建一个link /go链接到你的GOPATH目录，下面编译的时候要用。 将源码的vender目录下的文件拷贝到$GOPATH/src目录。 执行编译  在netplugin目录下执行以下命令能够编译出二进制文件。
NET_CONTAINER_BUILD=1 make build  在你的/$GOPATH/bin目录下应该会有如下几个文件：
contivk8s github-release godep golint misspell modelgen netcontiv netctl netmaster netplugin  ⚠️编译过程中可能会遇到 有些包不存在或者需要翻墙下载。
打包 我们将其打包为docker plugin。
Makefile里用于创建plugin rootfs的命令是：
host-pluginfs-create: @echo dev: creating a docker v2plugin rootfs ... sh scripts/v2plugin_rootfs.sh  v2plugin_rootfs.sh这个脚本的内容：
#!/bin/bash # Script to create the docker v2 plugin # run this script from contiv/netplugin directory echo &amp;quot;Creating rootfs for v2plugin &amp;quot;, ${CONTIV_V2PLUGIN_NAME} cat install/v2plugin/config.</description>
    </item>
    
    <item>
      <title>Docker17.03-CE插件开发-举个🌰</title>
      <link>http://rootsongjc.github.io/blogs/docker-plugin-develop/</link>
      <pubDate>Wed, 15 Mar 2017 13:57:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-plugin-develop/</guid>
      <description>（题图：杭州吴山步道旁的墙壁 Oct 16,2016）
 当你看到这篇文章时，如果你也正在进行docker1.13+版本下的plugin开发，恭喜你也入坑了，如果你趟出坑，麻烦告诉你的方法，感恩不尽🙏
 看了文章后你可能会觉得，官网上的可能是个假🌰。虽然官网上的文档写的有点不对，不过你使用docker-ssh-volume的开源代码自己去构建plugin的还是可以成功的！
Docker plugin开发文档 首先docker官方给出了一个docker legacy plugin文档，这篇文章基本就是告诉你docker目前支持哪些插件，罗列了一系列连接，不过对不起，这些不是docker官方插件，有问题去找它们的开发者去吧😂
Docker plugin貌似开始使用了新的v2 plugin了，legacy版本的plugin可以能在后期被废弃。
从docker的源码plugin/store.go中可以看到：
/* allowV1PluginsFallback determines daemon&#39;s support for V1 plugins. * When the time comes to remove support for V1 plugins, flipping * this bool is all that will be needed. */ const allowV1PluginsFallback bool = true /* defaultAPIVersion is the version of the plugin API for volume, network, IPAM and authz. This is a very stable API.</description>
    </item>
    
    <item>
      <title>Docker 17.03-CE create plugin源码解析</title>
      <link>http://rootsongjc.github.io/blogs/docker-create-plugin/</link>
      <pubDate>Wed, 15 Mar 2017 12:09:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-create-plugin/</guid>
      <description>（题图：故宫 Apr 3,2016）
继续上一篇Docker17.03-CE插件开发的🌰，今天来看下docker create plugin的源码。
cli/command/plugin/create.go
Docker命令行docker plugin create调用的，使用的是cobra，这个命令行工具开发包很好用，推荐下。
执行这两个函数
func newCreateCommand(dockerCli *command.DockerCli) *cobra.Command //调用下面的函数，拼装成URL调用RESTful API接口 func runCreate(dockerCli *command.DockerCli, options pluginCreateOptions) error { ... if err = dockerCli.Client().PluginCreate(ctx, createCtx, createOptions); err != nil { return err } ... }  在api/server/router/plugin/plugin_routes.go中
func (pr *pluginRouter) createPlugin(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { ... if err := pr.backend.CreateFromContext(ctx, r.Body, options); err != nil { return err } ... }  createPlugin这个方法定义在api/server/route/plugin/backen.</description>
    </item>
    
    <item>
      <title>微服务设计读书笔记</title>
      <link>http://rootsongjc.github.io/talks/microservice-reading-notes/</link>
      <pubDate>Sat, 11 Mar 2017 15:45:27 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/microservice-reading-notes/</guid>
      <description>(题图：青海湖畔 Jun 25,2016)
最近在看《微服务设计（Sam Newman著）》这本书，下载本书PDF(扫描版，高清49.17M)。作者是ThoughtWorks的Sam Newman。这本书中包括很多业界是用案例，比如Netflix和亚马逊。有兴趣的话大家一起看看讨论一下。😄
本书读者交流微信群二维码，扫码入群（3月18日前有效），如果二维码失效，请移步这里加我微信，拉你入群。
P.S 这本书比较偏理论，另外还有一本中国人写的书，《微服务架构与实践，王磊著，电子工业出版社》，下载本书的pdf，文字版，大小28.08M。这个人同样也是ThoughtWorks的，两个人的观点不谋而合，依然是便理论的东西。
Cloud Native Go - 基于Go和React的web云服务构建指南
这本书是我最近在翻译的，将由电子工业出版社出版，本书根据实际案例教你如何构建一个web微服务，是实践为服务架构的很好的参考。查看本书介绍。
1.微服务初探 什么是微服务？ 微服务（Microservices）这个词比较新颖，但是其实这种架构设计理念早就有了。微服务是一种分布式架构设计理念，为了推动细粒度服务的使用，这些服务要能协同工作，每个服务都有自己的生命周期。一个为服务就是一个独立的实体，可以独立的部署在PAAS平台上，也可以作为一个独立的进程在主机中运行。服务之间通过API访问，修改一个服务不会影响其它服务。
微服务的好处 微服务的好处有很多，包括:
 帮助你更快的采用新技术 解决技术异构的问题，因为是用API网络通信，可以使用不同的语言和技术开发不同的服务 增强系统弹性，服务的边界比较清晰，便于故障处理 方便扩展，比如使用容器技术，可以很方便的一次性启动很多个微服务 方便部署，因为微服务之间彼此独立，所以能够独立的部署单个服务而不影响其它服务，如果部署失败的话还可以回滚 别忘了康为定律，微服务可以很好契合解决组织架构问题 可重用，可随意组合 便于维护，可以随时重写服务，不必担心历史遗留问题  与面向服务架构SOA的关系 可以说微服务架构师SOA的一种，但是目前的大多数SOA做的都不好，在通信协议的选择、第三方中间件的选择、服务力度如何划分方面做的都不够好。
微服务与SOA的共同点
 都使用共享库，比如可重用的代码库 模块化，比如Java中的OSGI(Open Source Gateway Initiative)、Erlang中的模块化  2.架构师的职责 架构师应该关心是什么 架构师（Architect）在英文中和建筑师是同一个词，他们之间也有很多相同之处，架构师构建的是软件，而建筑师构建的是建筑。
终于看到了我翻译的*Cloud Native Go*第14章中引用的这本书的原话了。
软件的需求变更是来的那么快来的那么直接，不像建筑那样可以在设计好后按照设计图纸一步步的去建设。
架构师应该关心的是什么呢？
 保证系统适合开发人员在上面工作 关注服务之间的交互，不需要过于关注各个服务内部发生的事情，比如服务之间互相调用的接口，是使用protocol buffer呢，还是使用RESTful API，还是使用Java RMI，这个才是架构师需要关注的问题，至于服务内部究竟使用什么，那就看开发人员自己了，架构师更需要关注系统的边界和分区。 架构师应该与团队在一起，结对编程 🤓🤓 了解普通工作，知道普通的工作是什么样子，做一个代码架构师 😂  架构师应该做什么  提供原则指导实践，比如Heroku的12因素法则用来指导SAAS应用架构一样，微服务架构设计也要有一套原则。 提供要求标准，通过日志功能和监控对服务进行集中式管理，明确接口标准，提供安全性建议。 代码治理。为开发人员提供范例和服务代码模板。 解决技术债务。 集中治理和领导。维持良好的团队关系，当团队跑偏的时候及时纠正。  3.服务建模 以MusicCorp这家公司的服务为例子讲解。</description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part2</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</link>
      <pubDate>Fri, 10 Mar 2017 22:06:32 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</guid>
      <description> （题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)
本文是Docker v.s Kubernetes第二篇，续接上文Docker v.s Kuberntes Part1。
Kubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。
Kubernetes核心架构 master节点  apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类：  endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。   node节点  kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。  Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。
APIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。
Scheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。
工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。
工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。
工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。
Controller Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。
与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。
kubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。
kube-proxy kube-proxy提供两种功能:
 提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。  </description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part1</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</link>
      <pubDate>Fri, 10 Mar 2017 21:09:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</guid>
      <description> （题图：杭州西湖 Oct 16,2016）
前言 这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看Kunernetes权威指南（第二版）这本书（P.S感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。
此系列文章中所说的docker指的是*17.03-ce*版本。
概念性的差别 Kubernetes
了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：
 Pod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job  Docker
Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：
 Service Node Stack Docker  就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。
功能性差别  Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。 Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。 Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。 Kubernetes对象选择机制继续通过label selector，用于对象调度。 Kubernetes中有一个比较特别的镜像，叫做google_containers/pause，这个镜像是用来实现Pod概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。 Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。 持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用volume plugin。 多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。  总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。
 以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。
 </description>
    </item>
    
    <item>
      <title>Contiv入坑指南-v2plugin</title>
      <link>http://rootsongjc.github.io/blogs/contiv-v2plugin/</link>
      <pubDate>Fri, 10 Mar 2017 11:51:09 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-v2plugin/</guid>
      <description>(题图：上海交通大学 Oct 22,2016)
继续趟昨天挖的坑。
昨天的issue-776已经得到@gkvijay的回复，原来是因为没有安装contiv/v2plugin的缘故，所以create contiv network失败，我需要自己build一个docker plugin。
查看下这个commit里面有build v2plugin的脚本更改，所以直接调用以下命令就可以build自己的v2plugin。
前提你需要先build出netctl、netmaster、netplugin三个二进制文件并保存到bin目录下，如果你没自己build直接下载release里面的文件保存进去也行。
编译v2plugin插件 修改config.json插件配置文件
{ &amp;quot;manifestVersion&amp;quot;: &amp;quot;v0&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Contiv network plugin for Docker&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;https://contiv.github.io&amp;quot;, &amp;quot;entrypoint&amp;quot;: [&amp;quot;/startcontiv.sh&amp;quot;], &amp;quot;network&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;host&amp;quot; }, &amp;quot;env&amp;quot;: [ { &amp;quot;Description&amp;quot;: &amp;quot;To enable debug mode, set to &#39;-debug&#39;&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;dbg_flag&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;-debug&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;VLAN uplink interface used by OVS&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;iflist&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;Etcd or Consul cluster store url&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;cluster_store&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;etcd://172.</description>
    </item>
    
    <item>
      <title>Contiv入坑指南-试用全记录</title>
      <link>http://rootsongjc.github.io/blogs/contiv-tryout/</link>
      <pubDate>Thu, 09 Mar 2017 14:23:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-tryout/</guid>
      <description>(题图：山东荣成滨海风力发电场 Jan 31,2017）
关于contiv的介绍请看我的上一篇文章Contiv Intro。
开发环境使用Vagrant搭建，昨天试用了下，真不知道它们是怎么想的，即然是docker插件为啥不直接在docker中开发呢，我有篇文章介绍如何搭建docker开发环境，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像dockercore/docker:latest即可，不过有点大2.31G，使用阿里云的mirror下载倒是也划算，总比你自己部署一个开发环境节省时间。
Contiv概念解析 Contiv用于给容器创建和分配网路，可以创建策略管理容器的安全、带宽、优先级等，相当于一个SDN。
Group 按容器或Pod的功能给容器分配策略组，通常是按照容器/Pod的label来分组，应用组跟contiv的network不是一一对应的，可以很多应用组属于同一个network或IP subnet。
Polices 用来限定group的行为，contiv支持两种类型的policy：
 Bandwidth 限定应用组的资源使用上限 Isolation 资源组的访问权限  Group可以同时应用一个或多个policy，当有容器调度到该group里就会适用该group的policy。
Network IPv4或IPv6网络，可以配置subnet和gateway。
Contiv中的网络
在contiv中可以配置两种类型的网络
 application network：容器使用的网络 infrastructure network：host namespace的虚拟网络，比如基础设施监控网络  网络封装
Contiv中有两种类型的网络封装
 Routed：overlay topology和L3-routed BGP topology Bridged：layer2 VLAN  Tenant Tenant提供contiv中的namespace隔离。一个tenant可以有很多个network，每个network都有个subnet。该tenant中的用户可以使用它的任意network和subnet的IP。
物理网络中的tenant称作虚拟路由转发(VRF)。Contiv使用VLAN和VXLAN ID来实现外部网络访问，这取决你使用的是layer2、layer3还是Cisco ACI。
Contiv下载 Contiv的编译安装比较复杂，我们直接下载github上的release-1.0.0-beta.3-03-08-2017.18-51-20.UTC文件解压获得二进制文件安装。
 https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。
 如果试用可以的话，我会后续写contiv开发环境搭建的文章。
这个release是2017年3月8日发布的，就在我写这篇文章的前一天。有个最重要的更新是支持docker1.13 swarm mode。
官方安装文档
下载解压后会得到如下几个文件：
 contivk8s k8s专用的 contrib 文件夹，里面有个netctl的bash脚本 netcontiv 这个命令就一个-version选项用来查看contiv的版本😓 netctl contiv命令行工具，用来配置网络、策略、服务负载均衡，使用说明 netmaster contiv的主节点服务 netplugin  下面的安装中用到的只有netctl、netmaster和netplugin这三个二进制文件。</description>
    </item>
    
    <item>
      <title>Contiv Intro</title>
      <link>http://rootsongjc.github.io/blogs/contiv-guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-guide/</guid>
      <description>(题图：北京蓝色港湾夜景 Feb 11,2017 元宵节)
Contiv是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。
容器网络插件 Calico 与 Contiv Netplugin深入比较
还有篇文章讲解了docker网络方案的改进
Contiv Netplugin 简介 Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。

 Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址 Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。 集群管理依赖 etcd/serf</description>
    </item>
    
    <item>
      <title>Packer Intro</title>
      <link>http://rootsongjc.github.io/blogs/packer-intro/</link>
      <pubDate>Thu, 09 Mar 2017 10:58:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/packer-intro/</guid>
      <description>昨天研究了下Vagrant，感觉它的虚拟机ruby格式定义很麻烦，经人指点还有一个叫做packer的东西，也是Hashicorp这家公司出品的，今天看了下。
Packer是一款开源轻量级的镜像定义工具，可以根据一份定义文件生成多个平台的镜像，支持的平台有：
 Amazon EC2 (AMI). Both EBS-backed and instance-store AMIs Azure DigitalOcean Docker Google Compute Engine OpenStack Parallels QEMU. Both KVM and Xen images. VirtualBox VMware  Packer创造的镜像也能转换成Vagrant boxes。
Packer的镜像创建需要一个json格式的定义文件，例如quick-start.json
{ &amp;quot;variables&amp;quot;: { &amp;quot;access_key&amp;quot;: &amp;quot;{{env `AWS_ACCESS_KEY_ID`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{env `AWS_SECRET_ACCESS_KEY`}}&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-af22d9b9&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;packer-example {{timestamp}}&amp;quot; }] }  使用packer build quick-start.json可以在AWS上build一个AIM镜像。
Packer的详细文档：https://www.packer.io/docs/</description>
    </item>
    
    <item>
      <title>Vagrant介绍-从使用到放弃完全指南</title>
      <link>http://rootsongjc.github.io/blogs/vagrant-intro/</link>
      <pubDate>Wed, 08 Mar 2017 20:40:08 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/vagrant-intro/</guid>
      <description>（题图：北京地铁13号线光熙家园夜景 Mar 5,2017）
起源 久闻Vagrant大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。
因为今天在看contiv正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的Vagrantfile文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。
➜ netplugin git:(master) ✗ vagrant status Current machine states: netplugin-node1 running (virtualbox) netplugin-node2 running (virtualbox) netplugin-node3 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`.  Vagrant是hashicorp这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform。他们的产品都是基于Open Source的Github地址。
用途 Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。
Vagrant提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。
跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个Ubuntu 12.04的镜像。
用法 你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。
Vagrantfile
Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。
看个例子，选自https://github.com/fenbox/Vagrantfile
# -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below.</description>
    </item>
    
    <item>
      <title>Docker技术选型</title>
      <link>http://rootsongjc.github.io/projects/docker-tech-selection/</link>
      <pubDate>Wed, 08 Mar 2017 10:37:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/docker-tech-selection/</guid>
      <description>回顾历史  多少次我回过头看看走过的路，你还在小村旁。
 去年基于docker1.11对Hadoop yarn进行了docker化改造，详情请看大数据集群虚拟化-Yarn on docker始末，我将这个事件命名为magpie，因为它就像是喜鹊一样收集着各种各样的资源搭建自己的小窝。magpie还是有很多事情可以做的，大数据集群的虚拟化也不会止步，它仅仅是对其做了初步的探索，对于资源利用率和管理方面的优化还有很长的路要走，Yarn本身就是做为大数据集群的资源管理调度角色出现的，一开始是为调度MapReduce，后来的spark、hive、tensrflow、HAWQ、slide等等不一而足陆续出现。但是用它来管理docker似乎还是有点过重，还不如用kubernetes、marathon、nomad、swarm等。
但是在微服务方面docker1.11的很多弊端或者说缺点就暴露了出来，首先docker1.11原生并不带cluster管理，需要配合·docker swarm、kubernetes、marathon等才能管理docker集群。之前的对于docker的使用方式基本就是按照虚拟机的方式使用的，固定IP有悖于微服务的原则。
我们基于docker1.11和shrike二层网络模式，还有shipyard来做集群管理，shipyard只是一个简单的docker集群管理的WebUI，基本都是调用docker API，唯一做了一点docker原生没有的功能就是scale容器，而且只支持到docker1.11，早已停止开发。我抛弃了shipyard，它的页面功能基本可有可无，我自己开发的magpie一样可以管理yarn on docker集群。
Docker Swarm有如下几个缺点
 对于大规模集群的管理效率太低，当管理上百个node的时候经常出现有节点状态不同步的问题，比如主机重启后容器已经Exited了，但是master让然认为是Running状态，必须重启所有master节点才行。 没有中心化Node管理功能，必须登录到每台node上手动启停swarm-agent。 集群管理功能实在太太太简陋，查看所有node状态只能用docker info而且那个格式就不提了，shipyard里有处理这个格式的代码，我copy到了magpie里，彻底抛弃shipyard了。 Docker swarm的集群管理概念缺失，因为docker一开始设计的时候就不是用来管理集群的，所以出现了swarm，但是只能使用docker-compose来编排服务，但是无法在swarm集群中使用我们自定义的mynet网络，compose issue-4233，compose也已经被docker官方废弃（最近一年docker发展的太快了，原来用python写的compose已经被用go重构为libcompose直接集成到swarm mode里了），而且docker1.11里也没有像kubernetes那样service的单位，在docker1.11所有的管理都是基于docker容器的。  Docker Swarm的问题也是shipyard的问题，谁让shipyard直接调用docker的API呢。当然，在后续版本的docker里以上问题都已经不是问题，docker已经越来越像kubernetes，不论是在设计理念上还是在功能上，甚至还发行了企业版，以后每个月发布一个版本。
技术选型 主要对比Docker1.11和Docker17.03-ce版本。
首先有一点需要了解的是，docker1.12+带来的swarm mode，你可以使用一个命令直接启动一个复杂的stack，其中包括了服务编排和所有的服务配置，这是一个投票应用的例子。
下表对比了docker1.11和docker17.03-ce
   版本 docker1.11 docker17.03-ce     基本单位 docker容器 docker容器、service、stack   服务编排 compose，不支持docker swarm的mynet网络 改造后的compose，支持stack中完整的服务编排   网络模型 Host、bridge、overlay、mynet 默认支持跨主机的overlay网络，创建单个容器时也可以attach到已有的overla网络中   插件 没有插件管理命令，但是可以手动创建和管理 有插件管理命令，可以手动创建和从docker hub中下载，上传插件到自己的私有镜像仓库   升级 不支持平滑升级，重启docker原来的容器也会停掉 可以停止docker engine但不影响已启动的容器   弹性伸缩 不支持 service内置功能   服务发现 监听docker event增删DNS 内置服务发现，根据DNS负载均衡   节点管理 手动启停 中心化管理node节点   服务升级 手动升级 service内置功能   负载均衡 本身不支持 Swarm mode内部DNS轮寻    基于以上对比，使用docker17.</description>
    </item>
    
    <item>
      <title>Docker源码编译和开发环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/docker-dev-env/</link>
      <pubDate>Mon, 06 Mar 2017 17:03:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-dev-env/</guid>
      <description>看了下网上其他人写的docker开发环境搭建，要么是在ubuntu下搭建，要么就是使用官方说明的build docker-dev镜像的方式一步步搭建的，甚是繁琐，docker hub上有一个docker官方推出的dockercore/docker镜像，其实这就是官网上所说的docker-dev镜像，不过以前的那个deprecated了，使用目前这个镜像搭建docker开发环境是最快捷的了。
想要修改docker源码和做docker定制开发的同学可以参考下。
官方指导文档：https://docs.docker.com/opensource/code/
设置docker开发环境：https://docs.docker.com/opensource/project/set-up-dev-env/
docker的编译实质上是在docker容器中运行docker。
因此在本地编译docker的前提是需要安装了docker，还需要用git把代码pull下来。
创建分支 为了方便以后给docker提交更改，我们从docker官方fork一个分支。
git clone https://github.com/rootsongjc/docker.git git config --local user.name &amp;quot;Jimmy Song&amp;quot; git config --local user.email &amp;quot;rootsongjc@gmail.com&amp;quot; git remote add upstream https://github.com/docker/docker.git git config --local -l git remote -v git checkout -b dry-run-test touch TEST.md vim TEST.md git status git add TEST.md git commit -am &amp;quot;Making a dry run test.&amp;quot; git push --set-upstream origin dry-run-test  然后就可以在dry-run-test这个分支下工作了。
配置docker开发环境 官网上说需要先清空自己电脑上已有的容器和镜像。
docker开发环境本质上是创建一个docker镜像，镜像里包含了docker的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。
在dry-run-test分支下执行
make BIND_DIR=.</description>
    </item>
    
    <item>
      <title>Cloud Native Go - 基于Go和React的web云服务构建指南</title>
      <link>http://rootsongjc.github.io/talks/cloud-native-go/</link>
      <pubDate>Fri, 03 Mar 2017 17:29:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/cloud-native-go/</guid>
      <description>(题图：北京植物园桃花 Mar 26,2016)
更新于Apr 3,2017
最近在翻译Kevin Hoffman和Dan Nemeth的书《Cloud Native Go - 基于Go和React的web云服务构建指南》。目前已经完成图书的翻译，已交给编辑校对。本书将由电子工业出版社出版。
简介 Cloud Native Go向开发人员展示如何构建大规模云应用程序，在满足当今客户的强大需求的同时还可以动态扩展来处理几乎任何规模的数据量、流量或用户。
Kevin Hoffman和Dan Nemeth详细描述了现代云原生应用程序，阐明了与快速、可靠的云原生开发相关的因素、规则和习惯。他们还介绍了Go这种“简单优雅”的高性能语言，它特别适合于云开发。
在本书中你将使用Go语言创建微服务，使用ReactJS和Flux添加前端Web组件，并掌握基于Go的高级云原生技术。Hoffman和Nemeth展示了如何使用Wercker、Docker和Dockerhub等工具构建持续交付管道; 自动推送应用程序到平台上; 并系统地监控生产中的应用程序性能。
 学习“云之道”：为什么开发好的云软件基本上是关于心态和规则 了解为什么使用Go语言是云本地微服务开发的理想选择 规划支持持续交付和部署的云应用程序 设计服务生态系统，然后以test-first的方式构建它们
 将正在进行的工作推送到云
 使用事件源和CQRS模式来响应大规模和高吞吐量
 安全的基于云的Web应用程序：做与不做的选择
 使用第三方消息传递供应商创建响应式云应用程序
 使用React和Flux构建大规模，云友好的GUI
 监控云中的动态扩展，故障转移和容错
  下面先罗列下目录，以飨读者。
目录 Cloud Native Go. 1
构建基于Go和React的云原生Web应用&amp;hellip; 1
云服务构建完全指南&amp;hellip; 1
目录&amp;hellip; 4
前言&amp;hellip; 8
关于作者&amp;hellip; 9
致谢&amp;hellip; 9
第1章 云之道&amp;hellip; 10 云的优势&amp;hellip; 10
崇尚简洁&amp;hellip; 11
测试优先，测试一切&amp;hellip; 11
尽早发布, 频繁发布&amp;hellip; 12</description>
    </item>
    
    <item>
      <title>Jimmy Song&#39;s Resume</title>
      <link>http://rootsongjc.github.io/resume/</link>
      <pubDate>Wed, 01 Mar 2017 22:21:21 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/resume/</guid>
      <description>$whoami 宋净超 Jimmy Song
男 未婚
出生：1990年
现居：北京
籍贯：山东威海
学历：大学本科
毕业：武汉理工大学
专业：软件工程
微信：jimmysong
手机：18514468566
Github: github.com/rootsonjc
Blog：rootsongjc.github.io
自我介绍 Geek，热爱分享和写作，早年混迹Hadoop生态圈（2013-2015），后来（2015年末至今）沉浸于容器生态圈不能自拔，现在正研究kubernetes和CNCF组件，期待容器技术能为企业带来管理的变革、效率的提高、成本的改善。电子工业出版社Cloud Native Go图书译者，先后在Qcon上海2016大数据应用优化与实践、上海全球微服务架构大会做大数据集群和微服务相关主题分享，参与了2016年云栖大会大规模容器集群的管理与调度的圆桌论坛。
技能树 工作经历 TalkingData 大数据及云计算工程师
2015.07至今
 负责TalkingData大数据平台数十PB数据的管理。 主导完成了TakingData大数据集群Docker虚拟化项目yarn on docker，目前正在进行微服务化的推广。  科大讯飞（sz.002230） 软件开发工程师
2013.07-2015.07
 个性化数据同步系统开发。 讯飞大数据管理平台maple开发。 讯飞语音云Hadoop集群建设  项目经验 2016.08至2016.12：微服务和Paas平台建设
项目描述：构建基于Docker的微服务平台。
 Docker版本选型：docker1.11 Docker网络选型：Shrike自研网络插件。 Docker镜像制作：制定镜像制作流程，优化镜像制作过程，探究镜像使用技巧。 私有仓库搭建：harbor。  2016.01至2016.07：Hadoop计算资源虚拟化
项目描述：为了实现Hadoop的计算资源隔离与弹性调度，使用Docker虚拟化技术。
 Docker集群管理工具：Magpie Yarn on docker——大数据集群的计算资源虚拟化  2015.08至2015.12：Hadoop管理与优化
项目描述：负责TalkingData几个Hadoop管理、迁移、升级与优化。
 Hadoop集群监控管理平台开发：使用技术Java、Play Framework、High-chart、JavaScript、Bootstrap、Cassandra。 Hadoop集群的迁移与升级：升级到版本CDH5.5.2，使用Cloudera Manager管理。  2015.01至2015.07：Hadoop集群升级
项目描述：升级公司原来的Hadoop集群到CDH5并增加安全性配置。
 CDH5新特性调研 Hadoop新集群搭建 Hadoop安全性配置 Hadoop集群参数优化  2014.</description>
    </item>
    
    <item>
      <title>12因素法则</title>
      <link>http://rootsongjc.github.io/blogs/12-factor-app/</link>
      <pubDate>Mon, 27 Feb 2017 22:32:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/12-factor-app/</guid>
      <description>Twelve-factor App 简介 如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建如下的 SaaS 应用提供了方法论：
 使用标准化流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。 和操作系统之间尽可能的划清界限，在各个系统中提供最大的可移植性。 适合部署在现代的云计算平台，从而在服务器和系统管理方面节省资源。 将开发环境和生产环境的差异降至最低，并使用持续交付实施敏捷开发。 可以在工具、架构和开发流程不发生明显变化的前提下实现扩展。  这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。
背景 本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过 Heroku 平台间接见证了数十万应用程序的开发，运作以及扩展的过程。
本文综合了我们关于 SaaS 应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何 避免软件污染 。
我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于 Martin Fowler 的书籍： *Patterns of Enterprise Application Architecture* ， *Refactoring* 。
12-factors I. 基准代码 一份基准代码，多份部署 II. 依赖 显式声明依赖关系 III. 配置 在环境中存储配置 IV. 后端服务 把后端服务当作附加资源 V. 构建，发布，运行 严格分离构建和运行 VI. 进程 以一个或多个无状态进程运行应用 VII. 端口绑定 通过端口绑定提供服务 VIII. 并发 通过进程模型进行扩展 IX. 易处理 快速启动和优雅终止可最大化健壮性 X. 开发环境与线上环境等价 尽可能的保持开发，预发布，线上环境相同 XI. 日志 把日志当作事件流 XII.</description>
    </item>
    
    <item>
      <title>Docker Service Discovery</title>
      <link>http://rootsongjc.github.io/blogs/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-service-discovery/</guid>
      <description>Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of service discovery backend. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.

Thanks to Docker 1.12 Swarm Mode, we don’t have to depend upon these external tools and complex configurations. Docker Engine 1.12 runs it’s own internal DNS service to route services by name.</description>
    </item>
    
    <item>
      <title>Docker内置DNS</title>
      <link>http://rootsongjc.github.io/blogs/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-embedded-dns/</guid>
      <description>本文主要介绍了Docker容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。
Configure container DNS DNS in default bridge network    Options Description     -h HOSTNAME or –hostname=HOSTNAME 在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。   –link=CONTAINER_NAME or ID:ALIAS 在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？   –dns=IP_ADDRESS… 在该容器启动时，将nameserver IP_ADDRESS添加到容器内的/etc/resolv.conf中。可以配置多个。   –dns-search=DOMAIN… 在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。   –dns-opt=OPTION… 在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。     说明：
 如果docker run时不含--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。 如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：  如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.</description>
    </item>
    
    <item>
      <title>Raft一致性算法</title>
      <link>http://rootsongjc.github.io/blogs/raft/</link>
      <pubDate>Mon, 27 Feb 2017 10:47:14 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/raft/</guid>
      <description>这是一个动画演示版的Raft一致性算法的说明，很直观，推荐观看。 http://thesecretlivesofdata.com/raft/
P.S Raft一致性算法在很多软件中都有应用，如Docker（Swarm Mode）、Ectd等，Hadoop生态圈里的Zookeeper用的是艰深的Paxos算法。</description>
    </item>
    
    <item>
      <title>TalkingData Annual Meeting</title>
      <link>http://rootsongjc.github.io/talks/td-annual-meeting/</link>
      <pubDate>Sun, 26 Feb 2017 20:18:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/td-annual-meeting/</guid>
      <description>TalkingData Annual Meeting 2017 Dayin Theater, Beijing Friday, Feb 24, 2017
Photo by Jimmy Song</description>
    </item>
    
    <item>
      <title>My github pages</title>
      <link>http://rootsongjc.github.io/projects/my-github-pages/</link>
      <pubDate>Wed, 22 Feb 2017 16:56:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/my-github-pages/</guid>
      <description> My Open Source Project  Magpie - Magpie is a command line tool for deploying and managing Yarn on Docker cluster. Docker IPAM plugin - Docker network plugin to make a L2 flat network. Docker practice - Docker in practice Go practice - Go in practice Linux practice - Linux in practice :) Team management - About team management  </description>
    </item>
    
  </channel>
</rss>