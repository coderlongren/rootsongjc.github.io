<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy&#39;s blog</title>
    <link>http://rootsongjc.github.io/index.xml</link>
    <description>Recent content on Jimmy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Mar 2017 20:40:08 +0800</lastBuildDate>
    <atom:link href="http://rootsongjc.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>vagrant介绍</title>
      <link>http://rootsongjc.github.io/post/vagrant_introduce/</link>
      <pubDate>Wed, 08 Mar 2017 20:40:08 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/vagrant_introduce/</guid>
      <description>

&lt;h2 id=&#34;起源&#34;&gt;起源&lt;/h2&gt;

&lt;p&gt;久闻&lt;strong&gt;Vagrant&lt;/strong&gt;大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。&lt;/p&gt;

&lt;p&gt;因为今天在看&lt;a href=&#34;https://github.com/contiv/netplugin&#34;&gt;contiv&lt;/a&gt;正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的&lt;a href=&#34;https://github.com/contiv/netplugin/blob/master/Vagrantfile&#34;&gt;Vagrantfile&lt;/a&gt;文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  netplugin git:(master) ✗ vagrant status
Current machine states:

netplugin-node1           running (virtualbox)
netplugin-node2           running (virtualbox)
netplugin-node3           running (virtualbox)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vagrant是&lt;a href=&#34;https://www.hashicorp.com/&#34;&gt;hashicorp&lt;/a&gt;这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有&lt;code&gt;Consul&lt;/code&gt;、&lt;code&gt;Vault&lt;/code&gt;、&lt;code&gt;Nomad&lt;/code&gt;、&lt;code&gt;Terraform&lt;/code&gt;。他们的产品都是基于&lt;strong&gt;Open Source&lt;/strong&gt;的&lt;a href=&#34;https://github.com/hashicorp&#34;&gt;Github地址&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;用途&#34;&gt;用途&lt;/h2&gt;

&lt;p&gt;Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是&lt;strong&gt;virtualbox&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Vagrant提供一个命令行工具&lt;code&gt;vagrant&lt;/code&gt;，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。&lt;/p&gt;

&lt;p&gt;跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用&lt;code&gt;vagrant init hashicorp/precise64&lt;/code&gt;就可以初始化一个Ubuntu 12.04的镜像。&lt;/p&gt;

&lt;h2 id=&#34;用法&#34;&gt;用法&lt;/h2&gt;

&lt;p&gt;你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vagrantfile&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。&lt;/p&gt;

&lt;p&gt;看个例子，选自&lt;a href=&#34;https://github.com/fenbox/Vagrantfile&#34;&gt;https://github.com/fenbox/Vagrantfile&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;# -*- mode: ruby -*-
# vi: set ft=ruby :

# All Vagrant configuration is done below. The &amp;quot;2&amp;quot; in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don&#39;t change it unless you know what
# you&#39;re doing.
Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Every Vagrant development environment requires a box. You can search for
  # boxes at https://atlas.hashicorp.com/search.
  config.vm.box = &amp;quot;ubuntu/trusty64&amp;quot;

  # Disable automatic box update checking. If you disable this, then
  # boxes will only be checked for updates when the user runs
  # `vagrant box outdated`. This is not recommended.
  # config.vm.box_check_update = false

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine. In the example below,
  # accessing &amp;quot;localhost:8080&amp;quot; will access port 80 on the guest machine.
  # config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 80, host: 8080

  # Create a private network, which allows host-only access to the machine
  # using a specific IP.
  config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.10&amp;quot;

  # Create a public network, which generally matched to bridged network.
  # Bridged networks make the machine appear as another physical device on
  # your network.
  # config.vm.network &amp;quot;public_network&amp;quot;

  # Share an additional folder to the guest VM. The first argument is
  # the path on the host to the actual folder. The second argument is
  # the path on the guest to mount the folder. And the optional third
  # argument is a set of non-required options.
  # config.vm.synced_folder &amp;quot;../data&amp;quot;, &amp;quot;/vagrant_data&amp;quot;

  # Provider-specific configuration so you can fine-tune various
  # backing providers for Vagrant. These expose provider-specific options.
  # Example for VirtualBox:
  #
  # config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb|
  #   # Display the VirtualBox GUI when booting the machine
  #   vb.gui = true
  #
  #   # Customize the amount of memory on the VM:
  #   vb.memory = &amp;quot;1024&amp;quot;
  # end
  #
  # View the documentation for the provider you are using for more
  # information on available options.

  # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
  # such as FTP and Heroku are also available. See the documentation at
  # https://docs.vagrantup.com/v2/push/atlas.html for more information.
  # config.push.define &amp;quot;atlas&amp;quot; do |push|
  #   push.app = &amp;quot;YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME&amp;quot;
  # end

  # Enable provisioning with a shell script. Additional provisioners such as
  # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the
  # documentation for more information about their specific syntax and use.
  # config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;lt;&amp;lt;-SHELL
  #   apt-get update
  #   apt-get install -y apache2
  # SHELL
  config.vm.provision :shell, path: &amp;quot;bootstrap.sh&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Boxes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vagrant的基础镜像，相当于docker images。可以在这些基础镜像的基础上制作自己的虚拟机镜像。&lt;/p&gt;

&lt;p&gt;添加一个box&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ vagrant box add hashicorp/precise64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Vagrantfile中指定box&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.box_version = &amp;quot;1.1.0&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;使用ssh进入vagrant&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vagrant up&lt;/code&gt;后就可以用&lt;code&gt;vagrant ssh $name&lt;/code&gt;进入虚拟机内，如果主机上就一个vagrant可以不指定名字。默认进入的用户是vagrant。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;文件同步&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vagrant up&lt;/code&gt;后在虚拟机中会有一个&lt;code&gt;/vagrant&lt;/code&gt;目录，这跟你定义&lt;code&gt;Vagrantfile&lt;/code&gt;是同一级目录。&lt;/p&gt;

&lt;p&gt;这个目录跟你宿主机上的目录文件是同步的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;软件安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在Vagrantfile中定义要安装的软件和操作。&lt;/p&gt;

&lt;p&gt;例如安装apache&lt;/p&gt;

&lt;p&gt;在与Vagrantfile同级的目录下创建一个&lt;code&gt;bootstrap.sh&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/usr/bin/env bash

apt-get update
apt-get install -y apache2
if ! [ -L /var/www ]; then
  rm -rf /var/www
  ln -fs /vagrant /var/www
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在Vagrantfile中使用它。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.box_version = &amp;quot;1.1.0&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;端口转发&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.provision :shell, path: &amp;quot;bootstrap.sh&amp;quot;
  config.vm.network :forwarded_port, guest: 80, host: 4567
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行&lt;code&gt;vagrant reload&lt;/code&gt;或者&lt;code&gt;vagrant up&lt;/code&gt;可以生效。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;分享&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;你自己做的vagrant是可以分享给别人的用的，只要你有一个hashicorp账号，&lt;code&gt;vagrant login&lt;/code&gt;后就可以执行&lt;code&gt;vagrant share&lt;/code&gt;分享，会生成一个URL，其它人也可以访问到你的vagrant里的服务。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中止&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vagrant suspend&lt;/li&gt;
&lt;li&gt;Vagrant halt&lt;/li&gt;
&lt;li&gt;Vagrant destroy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;重构&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;再次执行&lt;code&gt;vagrant up&lt;/code&gt;即可。&lt;/p&gt;

&lt;h2 id=&#34;分布式环境&#34;&gt;分布式环境&lt;/h2&gt;

&lt;p&gt;开发分布式环境下的应用时往往需要多个虚拟机用于测试，这时候才是vagrant显威力的时候。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义多个主机&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;quot;echo Hello&amp;quot;

  config.vm.define &amp;quot;web&amp;quot; do |web|
    web.vm.box = &amp;quot;apache&amp;quot;
  end

  config.vm.define &amp;quot;db&amp;quot; do |db|
    db.vm.box = &amp;quot;mysql&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个比较复杂，详见&lt;a href=&#34;https://www.vagrantup.com/docs/multi-machine/&#34;&gt;https://www.vagrantup.com/docs/multi-machine/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有一些其它功能，如push、plugins、providers按下不表。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;总的来说说Vagrant没有Docker好用，但是对于协同开发，用它来定义分布式开发环境还可以，ruby的语法看着有点不习惯，好在也不复杂，如果是团队几个人开发，弄几个虚拟机大家互相拷贝一下也没那么复杂吧？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker技术选型</title>
      <link>http://rootsongjc.github.io/project/docker_tech_selection/</link>
      <pubDate>Wed, 08 Mar 2017 10:37:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/project/docker_tech_selection/</guid>
      <description>

&lt;h2 id=&#34;回顾历史&#34;&gt;回顾历史&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;多少次我回过头看看走过的路，你还在小村旁。&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;去年基于docker1.11对Hadoop yarn进行了docker化改造，详情请看&lt;a href=&#34;https://rootsongjc.github.io/docker-practice/docs/td_yarn_on_docker.html&#34;&gt;大数据集群虚拟化-Yarn on docker始末&lt;/a&gt;，我将这个事件命名为&lt;a href=&#34;https://github.com/rootsongjc/magpie&#34;&gt;magpie&lt;/a&gt;，因为它就像是喜鹊一样收集着各种各样的资源搭建自己的小窝。&lt;strong&gt;magpie&lt;/strong&gt;还是有很多事情可以做的，大数据集群的虚拟化也不会止步，它仅仅是对其做了初步的探索，对于资源利用率和管理方面的优化还有很长的路要走，&lt;strong&gt;Yarn&lt;/strong&gt;本身就是做为大数据集群的资源管理调度角色出现的，一开始是为调度&lt;strong&gt;MapReduce&lt;/strong&gt;，后来的&lt;code&gt;spark&lt;/code&gt;、&lt;code&gt;hive&lt;/code&gt;、&lt;code&gt;tensrflow&lt;/code&gt;、&lt;code&gt;HAWQ&lt;/code&gt;、&lt;code&gt;slide&lt;/code&gt;等等不一而足陆续出现。但是用它来管理docker似乎还是有点过重，还不如用kubernetes、marathon、nomad、swarm等。&lt;/p&gt;

&lt;p&gt;但是在微服务方面docker1.11的很多弊端或者说缺点就暴露了出来，首先docker1.11原生并不带cluster管理，需要配合·&lt;code&gt;docker swarm&lt;/code&gt;、&lt;code&gt;kubernetes&lt;/code&gt;、&lt;code&gt;marathon&lt;/code&gt;等才能管理docker集群。&lt;u&gt;之前的对于docker的使用方式基本就是按照虚拟机的方式使用的，固定IP有悖于微服务的原则。&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;我们基于docker1.11和&lt;a href=&#34;github.com/talkingdata/shrike&#34;&gt;shrike&lt;/a&gt;二层网络模式，还有&lt;a href=&#34;https://github.com/shipyard/shipyard&#34;&gt;shipyard&lt;/a&gt;来做集群管理，shipyard只是一个简单的docker集群管理的WebUI，基本都是调用docker API，唯一做了一点docker原生没有的功能就是&lt;strong&gt;scale&lt;/strong&gt;容器，而且只支持到docker1.11，早已停止开发。我抛弃了&lt;strong&gt;shipyard&lt;/strong&gt;，它的页面功能基本可有可无，我自己开发的&lt;a href=&#34;https://github.com/rootsongjc/magpie&#34;&gt;magpie&lt;/a&gt;一样可以管理&lt;code&gt;yarn on docker&lt;/code&gt;集群。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker Swarm有如下几个缺点&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对于大规模集群的管理效率太低，当管理上百个node的时候经常出现有节点状态不同步的问题，比如主机重启后容器已经&lt;strong&gt;Exited&lt;/strong&gt;了，但是master让然认为是&lt;strong&gt;Running&lt;/strong&gt;状态，必须重启所有master节点才行。&lt;/li&gt;
&lt;li&gt;没有中心化Node管理功能，必须登录到每台node上手动启停&lt;code&gt;swarm-agent&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;集群管理功能实在&lt;strong&gt;太太太&lt;/strong&gt;简陋，查看所有node状态只能用&lt;code&gt;docker info&lt;/code&gt;而且那个格式就不提了，shipyard里有处理这个格式的代码，我copy到了magpie里，彻底抛弃shipyard了。&lt;/li&gt;
&lt;li&gt;Docker swarm的集群管理概念缺失，因为docker一开始设计的时候就不是用来管理集群的，所以出现了swarm，但是只能使用&lt;strong&gt;docker-compose&lt;/strong&gt;来编排服务，但是无法在swarm集群中使用我们自定义的&lt;strong&gt;mynet&lt;/strong&gt;网络，&lt;a href=&#34;https://github.com/docker/compose/issues/4233&#34;&gt;compose issue-4233&lt;/a&gt;，&lt;strong&gt;compose&lt;/strong&gt;也已经被docker官方废弃（最近一年docker发展的太快了，原来用python写的compose已经被用go重构为&lt;strong&gt;libcompose&lt;/strong&gt;直接集成到swarm mode里了），而且docker1.11里也没有像kubernetes那样&lt;code&gt;service&lt;/code&gt;的单位，在docker1.11所有的管理都是基于docker容器的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Docker Swarm的问题也是shipyard的问题，谁让shipyard直接调用docker的API呢。当然，在后续版本的docker里以上问题都已经不是问题，docker已经越来越像kubernetes，不论是在设计理念上还是在功能上，甚至还发行了企业版，以后每个月发布一个版本。&lt;/p&gt;

&lt;h2 id=&#34;技术选型&#34;&gt;技术选型&lt;/h2&gt;

&lt;p&gt;主要对比&lt;code&gt;Docker1.11&lt;/code&gt;和&lt;code&gt;Docker17.03-ce&lt;/code&gt;版本。&lt;/p&gt;

&lt;p&gt;首先有一点需要了解的是，docker1.12+带来的&lt;a href=&#34;https://rootsongjc.github.io/docker-practice/docs/swarm_mode.html&#34;&gt;swarm mode&lt;/a&gt;，你可以使用一个命令直接启动一个复杂的&lt;strong&gt;stack&lt;/strong&gt;，其中包括了服务编排和所有的服务配置，这是一个&lt;a href=&#34;https://rootsongjc.github.io/docker-practice/docs/create_swarm_app.html&#34;&gt;投票应用的例子&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;下表对比了docker1.11和docker17.03-ce&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;docker1.11&lt;/th&gt;
&lt;th&gt;docker17.03-ce&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;基本单位&lt;/td&gt;
&lt;td&gt;docker容器&lt;/td&gt;
&lt;td&gt;docker容器、service、stack&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;服务编排&lt;/td&gt;
&lt;td&gt;compose，不支持docker swarm的mynet网络&lt;/td&gt;
&lt;td&gt;改造后的compose，支持stack中完整的服务编排&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;网络模型&lt;/td&gt;
&lt;td&gt;Host、bridge、overlay、mynet&lt;/td&gt;
&lt;td&gt;默认支持跨主机的overlay网络，创建单个容器时也可以attach到已有的overla网络中&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;插件&lt;/td&gt;
&lt;td&gt;没有插件管理命令，但是可以手动创建和管理&lt;/td&gt;
&lt;td&gt;有插件管理命令，可以手动创建和从docker hub中下载，上传插件到自己的私有镜像仓库&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;升级&lt;/td&gt;
&lt;td&gt;不支持平滑升级，重启docker原来的容器也会停掉&lt;/td&gt;
&lt;td&gt;可以停止docker engine但不影响已启动的容器&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;弹性伸缩&lt;/td&gt;
&lt;td&gt;不支持&lt;/td&gt;
&lt;td&gt;service内置功能&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;服务发现&lt;/td&gt;
&lt;td&gt;监听docker event增删DNS&lt;/td&gt;
&lt;td&gt;内置服务发现，根据DNS负载均衡&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;节点管理&lt;/td&gt;
&lt;td&gt;手动启停&lt;/td&gt;
&lt;td&gt;中心化管理node节点&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;服务升级&lt;/td&gt;
&lt;td&gt;手动升级&lt;/td&gt;
&lt;td&gt;service内置功能&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;负载均衡&lt;/td&gt;
&lt;td&gt;本身不支持&lt;/td&gt;
&lt;td&gt;Swarm mode内部DNS轮寻&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;基于以上对比，使用docker17.03-ce不仅可以兼容以前的mynet网络模式，只需要重构以前的shrike为&lt;strong&gt;docker plugin&lt;/strong&gt;，在创建service的时候指定为mynet即可。也可以同时使用docker mode的overlay网络，而且还可以安装其它docker plugin首先更高级网络和volume功能。&lt;/p&gt;

&lt;p&gt;Docker17.03-ce借鉴了很多kubernetes的设计理念，docker发力企业级市场，相信新版的才符合微服务的方向，既能兼容以前的&lt;strong&gt;虚拟机式&lt;/strong&gt;的使用模式，也能兼容&lt;strong&gt;微服务&lt;/strong&gt;架构。&lt;/p&gt;

&lt;h2 id=&#34;下一步&#34;&gt;下一步&lt;/h2&gt;

&lt;p&gt;之前考虑过使用docker1.11 + compose + shipyard + eureka + nginx等做微服务架构，但是考虑到最新版docker的重大升级，从长远的眼光来看，不能一直限定于之前的那一套，我更倾向于新版本。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调研Docker17.03-ce的新特性，尤其是服务治理方面&lt;/li&gt;
&lt;li&gt;结合具体业务试用&lt;/li&gt;
&lt;li&gt;重构shrike为docker plugin&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Don&amp;rsquo;t speak, I&amp;rsquo;ll try to save us from ourselves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If were going down, we&amp;rsquo;re going down in flames&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>docker源码编译和开发环境搭建</title>
      <link>http://rootsongjc.github.io/post/docker-dev-env/</link>
      <pubDate>Mon, 06 Mar 2017 17:03:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-dev-env/</guid>
      <description>

&lt;p&gt;看了下网上其他人写的docker开发环境搭建，要么是在ubuntu下搭建，要么就是使用官方说明的build docker-dev镜像的方式一步步搭建的，甚是繁琐，docker hub上有一个docker官方推出的&lt;strong&gt;dockercore/docker&lt;/strong&gt;镜像，其实这就是官网上所说的docker-dev镜像，不过以前的那个deprecated了，使用目前这个镜像搭建docker开发环境是最快捷的了。&lt;/p&gt;

&lt;p&gt;想要修改docker源码和做docker定制开发的同学可以参考下。&lt;/p&gt;

&lt;p&gt;官方指导文档：&lt;a href=&#34;https://docs.docker.com/opensource/code/&#34;&gt;https://docs.docker.com/opensource/code/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;设置docker开发环境：&lt;a href=&#34;https://docs.docker.com/opensource/project/set-up-dev-env/&#34;&gt;https://docs.docker.com/opensource/project/set-up-dev-env/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;docker的编译实质上是在docker容器中运行docker。&lt;/p&gt;

&lt;p&gt;因此在本地编译docker的前提是需要安装了docker，还需要用git把代码pull下来。&lt;/p&gt;

&lt;h3 id=&#34;创建分支&#34;&gt;创建分支&lt;/h3&gt;

&lt;p&gt;为了方便以后给docker提交更改，我们从docker官方fork一个分支。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/rootsongjc/docker.git
git config --local user.name &amp;quot;Jimmy Song&amp;quot;
git config --local user.email &amp;quot;rootsongjc@gmail.com&amp;quot;
git remote add upstream https://github.com/docker/docker.git
git config --local -l
git remote -v
git checkout -b dry-run-test
touch TEST.md
vim TEST.md
git status
git add TEST.md
git commit -am &amp;quot;Making a dry run test.&amp;quot;
git push --set-upstream origin dry-run-test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以在&lt;code&gt;dry-run-test&lt;/code&gt;这个分支下工作了。&lt;/p&gt;

&lt;h3 id=&#34;配置docker开发环境&#34;&gt;配置docker开发环境&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/opensource/project/set-up-dev-env/&#34;&gt;官网&lt;/a&gt;上说需要先清空自己电脑上已有的容器和镜像。&lt;/p&gt;

&lt;p&gt;docker开发环境本质上是创建一个docker镜像，镜像里包含了docker的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;dry-run-test&lt;/code&gt;分支下执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;make BIND_DIR=. shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令会自动编译一个docker镜像，From debian:jessie。这一步会上网下载很多依赖包，速度比较慢。如果翻不了墙的话肯定都会失败。因为需要下载的软件和安装包都是在国外服务器上，不翻墙根本就下载不下来，为了不用这么麻烦，推荐直接使用docker官方的dockercore/docker镜像，也不用以前的docker-dev镜像，那个造就废弃了。这个镜像大小有2.31G。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull dockercore/docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用方法见这里：&lt;a href=&#34;https://hub.docker.com/r/dockercore/docker/&#34;&gt;https://hub.docker.com/r/dockercore/docker/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后就可以进入到容器里&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run --rm -i --privileged -e BUILDFLAGS -e KEEPBUNDLE -e DOCKER_BUILD_GOGC -e DOCKER_BUILD_PKGS -e DOCKER_CLIENTONLY -e DOCKER_DEBUG -e DOCKER_EXPERIMENTAL -e DOCKER_GITCOMMIT -e DOCKER_GRAPHDRIVER=devicemapper -e DOCKER_INCREMENTAL_BINARY -e DOCKER_REMAP_ROOT -e DOCKER_STORAGE_OPTS -e DOCKER_USERLANDPROXY -e TESTDIRS -e TESTFLAGS -e TIMEOUT -v &amp;quot;/Users/jimmy/Workspace/github/rootsongjc/docker/bundles:/go/src/github.com/docker/docker/bundles&amp;quot; -t &amp;quot;dockercore/docker:latest&amp;quot; bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照官网的说明make会报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker# ./hack/make.sh binary                          

error: .git directory missing and DOCKER_GITCOMMIT not specified
  Please either build with the .git directory accessible, or specify the
  exact (--short) commit hash you are building using DOCKER_GITCOMMIT for
  future accountability in diagnosing build issues.  Thanks!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个&lt;a href=&#34;https://github.com/docker/docker/issues/27581&#34;&gt;issue-27581&lt;/a&gt;，解决方式就是在make的时候手动指定&lt;code&gt;DOCKER_GITCOMMIT&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker# DOCKER_GITCOMMIT=3385658 ./hack/make.sh binary

---&amp;gt; Making bundle: binary (in bundles/17.04.0-dev/binary)
Building: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev
Created binary: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev
Building: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev
Created binary: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev
Copying nested executables into bundles/17.04.0-dev/binary-daemon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bundles目录下会生成如下文件结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── 17.04.0-dev
│   ├── binary-client
│   │   ├── docker -&amp;gt; docker-17.04.0-dev
│   │   ├── docker-17.04.0-dev
│   │   ├── docker-17.04.0-dev.md5
│   │   └── docker-17.04.0-dev.sha256
│   └── binary-daemon
│       ├── docker-containerd
│       ├── docker-containerd-ctr
│       ├── docker-containerd-ctr.md5
│       ├── docker-containerd-ctr.sha256
│       ├── docker-containerd-shim
│       ├── docker-containerd-shim.md5
│       ├── docker-containerd-shim.sha256
│       ├── docker-containerd.md5
│       ├── docker-containerd.sha256
│       ├── docker-init
│       ├── docker-init.md5
│       ├── docker-init.sha256
│       ├── docker-proxy
│       ├── docker-proxy.md5
│       ├── docker-proxy.sha256
│       ├── docker-runc
│       ├── docker-runc.md5
│       ├── docker-runc.sha256
│       ├── dockerd -&amp;gt; dockerd-17.04.0-dev
│       ├── dockerd-17.04.0-dev
│       ├── dockerd-17.04.0-dev.md5
│       └── dockerd-17.04.0-dev.sha256
└── latest -&amp;gt; 17.04.0-dev

4 directories, 26 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以将docker-daemon和docker-client目录下的docker可以执行文件复制到容器的/usr/bin/目录下了。&lt;/p&gt;

&lt;p&gt;启动docker deamon&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker daemon -D&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查下docker是否可用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker/bundles/17.04.0-dev# docker version
DEBU[0048] Calling GET /_ping                           
DEBU[0048] Calling GET /v1.27/version                   
Client:
 Version:      17.04.0-dev
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   3385658
 Built:        Mon Mar  6 08:39:06 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.04.0-dev
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   3385658
 Built:        Mon Mar  6 08:39:06 2017
 OS/Arch:      linux/amd64
 Experimental: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此docker源码编译和开发环境都已经搭建好了。&lt;/p&gt;

&lt;p&gt;如果想要修改docker源码，只要在你的IDE、容器里或者你本机上修改docker代码后，再执行上面的hack/make.sh binary命令就可以生成新的docker二进制文件，再替换原来的/usr/bin/目录下的docker二进制文件即可。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>cloud native go</title>
      <link>http://rootsongjc.github.io/presentation/cloud-native-go/</link>
      <pubDate>Fri, 03 Mar 2017 17:29:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/presentation/cloud-native-go/</guid>
      <description>&lt;p&gt;最近在翻译Kevin Hoffman和Dan Nemeth的书《Cloud Native Go》。预计本月将完成，将由&lt;strong&gt;电子工业出版社&lt;/strong&gt;出版。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践》读后感</title>
      <link>http://rootsongjc.github.io/post/cloud-computing-architecture-practice/</link>
      <pubDate>Wed, 01 Mar 2017 18:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/cloud-computing-architecture-practice/</guid>
      <description>&lt;p&gt;最近友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，&lt;a href=&#34;http://olz1di9xf.bkt.clouddn.com/docs/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E8%B7%B5%E7%AC%AC2%E7%89%88.pdf&#34;&gt;点这里下载&lt;/a&gt;，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。&lt;/p&gt;

&lt;p&gt;后续我将会陆续分享这本书的读后感。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>12 factor app</title>
      <link>http://rootsongjc.github.io/post/12-factor-app/</link>
      <pubDate>Mon, 27 Feb 2017 22:32:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/12-factor-app/</guid>
      <description>

&lt;h1 id=&#34;twelve-factor-app&#34;&gt;Twelve-factor App&lt;/h1&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建如下的 SaaS 应用提供了方法论：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用&lt;strong&gt;标准化&lt;/strong&gt;流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。&lt;/li&gt;
&lt;li&gt;和操作系统之间尽可能的&lt;strong&gt;划清界限&lt;/strong&gt;，在各个系统中提供&lt;strong&gt;最大的可移植性&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;适合&lt;strong&gt;部署&lt;/strong&gt;在现代的&lt;strong&gt;云计算平台&lt;/strong&gt;，从而在服务器和系统管理方面节省资源。&lt;/li&gt;
&lt;li&gt;将开发环境和生产环境的&lt;strong&gt;差异降至最低&lt;/strong&gt;，并使用&lt;strong&gt;持续交付&lt;/strong&gt;实施敏捷开发。&lt;/li&gt;
&lt;li&gt;可以在工具、架构和开发流程不发生明显变化的前提下实现&lt;strong&gt;扩展&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过 &lt;a href=&#34;http://www.heroku.com/&#34;&gt;Heroku&lt;/a&gt; 平台间接见证了数十万应用程序的开发，运作以及扩展的过程。&lt;/p&gt;

&lt;p&gt;本文综合了我们关于 SaaS 应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何 &lt;a href=&#34;http://blog.heroku.com/archives/2011/6/28/the_new_heroku_4_erosion_resistance_explicit_contracts/&#34;&gt;避免软件污染&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于 Martin Fowler 的书籍： *Patterns of Enterprise Application Architecture* ， *Refactoring* 。&lt;/p&gt;

&lt;h1 id=&#34;12-factors&#34;&gt;12-factors&lt;/h1&gt;

&lt;h2 id=&#34;i-基准代码-https-12factor-net-zh-cn-codebase&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/codebase&#34;&gt;I. 基准代码&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;一份基准代码-多份部署&#34;&gt;一份基准代码，多份部署&lt;/h3&gt;

&lt;h2 id=&#34;ii-依赖-https-12factor-net-zh-cn-dependencies&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/dependencies&#34;&gt;II. 依赖&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;显式声明依赖关系&#34;&gt;显式声明依赖关系&lt;/h3&gt;

&lt;h2 id=&#34;iii-配置-https-12factor-net-zh-cn-config&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/config&#34;&gt;III. 配置&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;在环境中存储配置&#34;&gt;在环境中存储配置&lt;/h3&gt;

&lt;h2 id=&#34;iv-后端服务-https-12factor-net-zh-cn-backing-services&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/backing-services&#34;&gt;IV. 后端服务&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;把后端服务当作附加资源&#34;&gt;把后端服务当作附加资源&lt;/h3&gt;

&lt;h2 id=&#34;v-构建-发布-运行-https-12factor-net-zh-cn-build-release-run&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/build-release-run&#34;&gt;V. 构建，发布，运行&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;严格分离构建和运行&#34;&gt;严格分离构建和运行&lt;/h3&gt;

&lt;h2 id=&#34;vi-进程-https-12factor-net-zh-cn-processes&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/processes&#34;&gt;VI. 进程&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;以一个或多个无状态进程运行应用&#34;&gt;以一个或多个无状态进程运行应用&lt;/h3&gt;

&lt;h2 id=&#34;vii-端口绑定-https-12factor-net-zh-cn-port-binding&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/port-binding&#34;&gt;VII. 端口绑定&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;通过端口绑定提供服务&#34;&gt;通过端口绑定提供服务&lt;/h3&gt;

&lt;h2 id=&#34;viii-并发-https-12factor-net-zh-cn-concurrency&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/concurrency&#34;&gt;VIII. 并发&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;通过进程模型进行扩展&#34;&gt;通过进程模型进行扩展&lt;/h3&gt;

&lt;h2 id=&#34;ix-易处理-https-12factor-net-zh-cn-disposability&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/disposability&#34;&gt;IX. 易处理&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;快速启动和优雅终止可最大化健壮性&#34;&gt;快速启动和优雅终止可最大化健壮性&lt;/h3&gt;

&lt;h2 id=&#34;x-开发环境与线上环境等价-https-12factor-net-zh-cn-dev-prod-parity&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/dev-prod-parity&#34;&gt;X. 开发环境与线上环境等价&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;尽可能的保持开发-预发布-线上环境相同&#34;&gt;尽可能的保持开发，预发布，线上环境相同&lt;/h3&gt;

&lt;h2 id=&#34;xi-日志-https-12factor-net-zh-cn-logs&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/logs&#34;&gt;XI. 日志&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;把日志当作事件流&#34;&gt;把日志当作事件流&lt;/h3&gt;

&lt;h2 id=&#34;xii-管理进程-https-12factor-net-zh-cn-admin-processes&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/admin-processes&#34;&gt;XII. 管理进程&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;后台管理任务当作一次性进程运行&#34;&gt;后台管理任务当作一次性进程运行&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>docker service discovery</title>
      <link>http://rootsongjc.github.io/post/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-service-discovery/</guid>
      <description>&lt;p&gt;Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of &lt;a href=&#34;https://docs.docker.com/v1.11/swarm/discovery/&#34;&gt;service discovery backend&lt;/a&gt;. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34; alt=&#34;pic-intro&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to Docker 1.12 Swarm Mode&lt;/strong&gt;, we don’t have to depend upon these external tools and complex configurations. &lt;a href=&#34;https://github.com/docker/docker/releases/tag/v1.12.0-rc5&#34;&gt;Docker Engine 1.12&lt;/a&gt; runs it’s own internal DNS service to route services by name.Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it help?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you create a service and provide a name for it, you can use just that name as a target hostname, and it’s going to be automatically resolved to the proper container IP of the service. In short, within the swarm, containers can simply reference other services via their names and the built-in DNS will be used to find the appropriate IP and port automatically. It is important to note that if the service has multiple replicas, &lt;strong&gt;the requests would be round-robin load-balanced&lt;/strong&gt;. This would still work if you didn’t forward any ports when you created your docker services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34; alt=&#34;Pic10&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Embedded DNS is not a new concept. It was first included under Docker 1.10 release. Please note that DNS lookup for containers connected to user-defined networks works differently compared to the containers connected to &lt;code&gt;default bridge&lt;/code&gt; network. As of Docker 1.10, the docker daemon implements an embedded DNS server which provides built-in service discovery for any container created with a valid &lt;code&gt;name&lt;/code&gt; or &lt;code&gt;net-alias&lt;/code&gt; or aliased by &lt;code&gt;link&lt;/code&gt;. Moreover,container name configured using &lt;code&gt;--name&lt;/code&gt; is used to discover a container within an user-defined docker network. The embedded DNS server maintains the mapping between the container name and its IP address (on the network the container is connected to).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does Embedded DNS resolve unqualified names?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34; alt=&#34;Pic22&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With Docker 1.12 release, a new API called “service” is being included which clearly talks about the functionality of service discovery.  It is important to note that Service discovery is scoped within the network. What it really means is –  If you have redis application and web client as two separate services , you combine into single application and put them into same network.If you try build your application in such a way that you are trying to reach to redis through name “redis”,it will always resolve to name “redis”. Reason – both of these services are part of the same network. You don’t need to be inside the application trying to resolve this service using FQDN. Reason – FQDN name is not going to be portable which in turn, makes your application non-portable.&lt;/p&gt;

&lt;p&gt;Internally, there is a listener opened inside the container itself. If we try to enter into the container which is providing a service discovery and look at /etc/resolv.conf, we will find that the nameserver entry holds something really different like 127.0.0.11.This is nothing but a loopback address. So, whenever resolver tried to resolve, it will resolve to 127.0.0.11 and this request is rightly trapped.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34; alt=&#34;Pic-12&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this request is trapped, it is sent to particular random UDP / TCP port currently being listened under the docker daemon. Consequently, the socket is to be created inside the namespace. When DNS server and daemon gets the request, it knows that this is coming from which specific network, hence gets aware of  the context of from where it is coming from.Once it knows the context, it can generate the appropriate DNS response.&lt;/p&gt;

&lt;p&gt;To demonstrate Service Discovery  under Docker 1.12, I have upgraded Docker 1.12.rc5 to 1.12.0 GA version. The swarm cluster look like:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34; alt=&#34;Pico01&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have created a network called “collabnet” for the new services as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34; alt=&#34;Pic-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create a service called “wordpressdb” under collabnet network :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34; alt=&#34;pico-mysql&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can list the running tasks(containers) and the node on which these containers are running on:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34; alt=&#34;Pic-4&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create another service called “wordpressapp” under the same network:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34; alt=&#34;pico-app&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, we can list out the number of services running on our swarm cluster as shown below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34; alt=&#34;pico-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have scaled out the number of wordpressapp and wordpressdb just for demonstration purpose.&lt;/p&gt;

&lt;p&gt;Let’s consider my master node where I have two of the containers running as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34; alt=&#34;Pico-1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can reach out one service(wordpressapp) from another service(wordpressapp) through just service-name as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34; alt=&#34;pico-last&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, I can reach out to particular container by its name from other container running different service but on the same network. As shown below, I can reach out to wordpressapp.3.6f8bthp container via wordpressdb.7.e62jl57qqu running wordpressdb.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34; alt=&#34;pico-tasktoo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The below picture depicts the Service Discovery in a nutshell:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic23.png&#34; alt=&#34;Pic23&#34; /&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every service has Virtual IP(VIP) associated which can be derived as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34; alt=&#34;pic-list&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As shown above, each service has an IP address and this IP address maps to multiple container IP address associated with that service. It is important to note that service IP associated with a service does not change even though containers associated with the service dies/ restarts.&lt;/p&gt;

&lt;p&gt;Few important points to remember:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VIP based services use Linux IPVS load balancing to route to the backend containers. This works only for TCP/UDP protocols. When you use DNS-RR mode services don’t have a VIP allocated. Instead service names resolves to one of the backend container IPs randomly.&lt;/li&gt;
&lt;li&gt;Ping not working for VIP is as designed. Technically, IPVS is a TCP/UDP load-balancer, while ping uses ICMP and hence IPVS is not going to load-balance the ping request.&lt;/li&gt;
&lt;li&gt;For VIP based services the reason ping works on the local node is because the VIP is added a 2nd IP address on the overlay network interface&lt;/li&gt;
&lt;li&gt;You can any of the tools like  dig, nslookup or wget -O- &lt;service name&gt; to demonstrate the service discovery functionality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below picture depicts that the network is the scope of service discoverability which means that when you have a service running on one network , it is scoped to that network and won’t be able to reach out to different service running on different network(unless it is part of that network).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34; alt=&#34;SD&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s dig little further introducing Load-balancing aspect too. To see what is basically enabling the load-balancing functionality, we can go into sandbox of each containers and see how it has been resolved.&lt;/p&gt;

&lt;p&gt;Let’s pick up the two containers running on the master node. We can see the sandbox running through the following command:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34; alt=&#34;pico-namespace&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Under /var/run/docker/netns, you will find various namespaces. The namespaces marked with x-{id} represents network namespace managed by the overlay network driver for its operation (such as creating a bridge, terminating vxlan tunnel, etc…). They don’t represent the container network namespace. Since it is managed by the driver, it is not recommended to manipulate anything within this namespace. But if you are curious on the deep dive, then you can use the “nsenter” tool to understand more about this internal namespace.&lt;/p&gt;

&lt;p&gt;We can enter into sandbox through the nsenter utility:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34; alt=&#34;pico-mangle&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case you faced an error stating “nsenter: reassociate to namespace ‘ns/net’ failed: Invalid argument”, I suggest to look at &lt;a href=&#34;http://tinyurl.com/gu5rsw9&#34;&gt;this&lt;/a&gt; workaround.&lt;/p&gt;

&lt;p&gt;10.0.3.4 service IP is marked 0x108 using iptables OUTPUT chain. ipvs uses this marking and load balances it to containers 10.0.3.5 and 10.0.3.6 as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34; alt=&#34;ipvs&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are key takeaways from this entire post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34; alt=&#34;Pic34&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&#34;http://collabnix.com/archives/1504&#34;&gt;http://collabnix.com/archives/1504&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker embedded dns</title>
      <link>http://rootsongjc.github.io/post/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-embedded-dns/</guid>
      <description>

&lt;p&gt;本文主要介绍了&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Docker&lt;/a&gt;容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。&lt;/p&gt;

&lt;h2 id=&#34;configure-container-dns&#34;&gt;Configure container DNS&lt;/h2&gt;

&lt;h3 id=&#34;dns-in-default-bridge-network&#34;&gt;DNS in default bridge network&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-h HOSTNAME or –hostname=HOSTNAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME or ID:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=IP_ADDRESS…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;添加到容器内的/etc/resolv.conf中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：

&lt;ul&gt;
&lt;li&gt;如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.conf内容更新容器内的/etc/resolv.conf.&lt;/li&gt;
&lt;li&gt;如果容器状态为running，则容器内的/etc/resolv.conf将不会改变，直到该容器状态变为stopped.&lt;/li&gt;
&lt;li&gt;如果容器启动后修改过容器内的/etc/resolv.conf，则不会对该容器进行处理，否则可能会丢失已经完成的修改，无论该容器为什么状态。 
如果容器启动时，用了–dns, –dns-search, or –dns-opt选项，其启动时已经修改了宿主机的/etc/resolv.conf过滤后的内容，因此docker daemon永远不会更新这种容器的/etc/resolv.conf。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;: docker daemon监控宿主机/etc/resolv.conf的这个file change notifier的实现是依赖linux内核的inotify特性，而inotfy特性不兼容overlay fs，因此使用overlay fs driver的docker deamon将无法使用该/etc/resolv.conf自动更新的功能。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;embedded-dns-in-user-defined-networks&#34;&gt;Embedded DNS in user-defined networks&lt;/h3&gt;

&lt;p&gt;在docker 1.10版本中，docker daemon实现了一个叫做&lt;code&gt;embedded DNS server&lt;/code&gt;的东西，用来当你创建的容器满足以下条件时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用自定义网络；&lt;/li&gt;
&lt;li&gt;容器创建时候通过&lt;code&gt;--name&lt;/code&gt;,&lt;code&gt;--network-alias&lt;/code&gt; or &lt;code&gt;--link&lt;/code&gt;提供了一个name；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;docker daemon就会利用embedded DNS server对整个自定义网络中所有容器进行名字解析（你可以理解为一个网络中的一种服务发现）。&lt;/p&gt;

&lt;p&gt;因此当你启动容器时候满足以上条件时，该容器的域名解析就不应该去考虑容器内的/etc/hosts, /etc/resolv.conf，应该保持其不变，甚至为空，将需要解析的域名都配置到对应embedded DNS server中。具体配置参数及说明如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;–name=CONTAINER-NAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将CONTAINER-NAME和该容器的IP配置到该容器连接到的自定义网络中的embedded DNS server中，由它提供该自定义网络范围内的域名解析&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–network-alias=ALIAS&lt;/td&gt;
&lt;td&gt;将容器的name-ip map配置到容器连接到的其他网络的embedded DNS server中。PS：一个容器可能连接到多个网络中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP配置到该容器连接到的自定义网络中的embedded DNS server中，但仅限于配置了该link的容器能解析这条rule。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=[IP_ADDRESS…]&lt;/td&gt;
&lt;td&gt;当embedded DNS server无法解析该容器的某个dns query时，会将请求foward到这些–dns配置的IP_ADDRESS DNS Server，由它们进一步进行域名解析。注意，这些–dns配置到&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;全部由对应的embedded DNS server管理，并不会更新到容器内的/etc/resolv.conf.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-search配置的DOMAIN们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-opt配置的OPTION们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;注意容器内/etc/resolv.conf中配置的DNS server，只有当the embedded DNS server无法解析某个name时，才会用到。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;embedded-dns-server源码分析&#34;&gt;embedded DNS server源码分析&lt;/h2&gt;

&lt;p&gt;所有embedded DNS server相关的代码都在libcontainer项目中，几个最主要的文件分别是&lt;code&gt;/libnetwork/resolver.Go&lt;/code&gt;,&lt;code&gt;/libnetwork/resolver_unix.go&lt;/code&gt;,&lt;code&gt;sandbox_dns_unix.go&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;OK, 先来看看embedded DNS server对象在docker中的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go

// resolver implements the Resolver interface
type resolver struct {
    sb         *sandbox
    extDNSList [maxExtDNS]extDNSEntry
    server     *dns.Server
    conn       *net.UDPConn
    tcpServer  *dns.Server
    tcpListen  *net.TCPListener
    err        error
    count      int32
    tStamp     time.Time
    queryLock  sync.Mutex
}

// Resolver represents the embedded DNS server in Docker. It operates
// by listening on container&#39;s loopback interface for DNS queries.
type Resolver interface {
    // Start starts the name server for the container
    Start() error
    // Stop stops the name server for the container. Stopped resolver
    // can be reused after running the SetupFunc again.
    Stop()
    // SetupFunc() provides the setup function that should be run
    // in the container&#39;s network namespace.
    SetupFunc() func()
    // NameServer() returns the IP of the DNS resolver for the
    // containers.
    NameServer() string
    // SetExtServers configures the external nameservers the resolver
    // should use to forward queries
    SetExtServers([]string)
    // ResolverOptions returns resolv.conf options that should be set
    ResolverOptions() []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，resolver就是embedded DNS server，每个resolver都bind一个sandbox，并定义了一个对应的dns.Server，还定义了外部DNS对象列表，但embedded DNS server无法解析某个name时，就会forward到那些外部DNS。&lt;/p&gt;

&lt;p&gt;Resolver Interface定义了embedded DNS server必须实现的接口，这里会重点关注SetupFunc()和Start()，见下文分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;dns.Server的实现，全部交给github.com/miekg/dns，限于篇幅，这里我将不会跟进去分析。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从整个&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Container&lt;/a&gt; create的流程上来看，docker daemon对embedded DNS server的处理是从endpoint Join a sandbox开始的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libnetwork/endpoint.go


func (ep *endpoint) Join(sbox Sandbox, options ...EndpointOption) error {
    ...

    return ep.sbJoin(sb, options...)
}


func (ep *endpoint) sbJoin(sb *sandbox, options ...EndpointOption) error {
    ...

    if err = sb.populateNetworkResources(ep); err != nil {
        return err
    }

    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox join a sandbox的流程中，会调用sandbox. populateNetworkResources做网络资源的设置，这其中就包括了embedded DNS server的启动。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox.go
func (sb *sandbox) populateNetworkResources(ep *endpoint) error {
    ...
    if ep.needResolver() {
        sb.startResolver(false)
    }
    ...
}


libnetwork/sandbox_dns_unix.go
func (sb *sandbox) startResolver(restore bool) {
    sb.resolverOnce.Do(func() {
        var err error
        sb.resolver = NewResolver(sb)
        defer func() {
            if err != nil {
                sb.resolver = nil
            }
        }()

        // In the case of live restore container is already running with
        // right resolv.conf contents created before. Just update the
        // external DNS servers from the restored sandbox for embedded
        // server to use.
        if !restore {
            err = sb.rebuildDNS()
            if err != nil {
                log.Errorf(&amp;quot;Updating resolv.conf failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
                return
            }
        }
        sb.resolver.SetExtServers(sb.extDNS)

        sb.osSbox.InvokeFunc(sb.resolver.SetupFunc())
        if err = sb.resolver.Start(); err != nil {
            log.Errorf(&amp;quot;Resolver Setup/Start failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox.startResolver是流程关键:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过sanbdox.rebuildDNS生成了container内的/etc/resolv.conf&lt;/li&gt;
&lt;li&gt;通过resolver.SetExtServers(sb.extDNS)设置embedded DNS server的forward DNS list&lt;/li&gt;
&lt;li&gt;通过resolver.SetupFunc()启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener&lt;/li&gt;
&lt;li&gt;通过resolver.Start()对容器内的iptable进行设置(见下)，并通过miekg/dns启动一个nameserver在53端口提供服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我将逐一介绍上面的各个步骤。&lt;/p&gt;

&lt;h3 id=&#34;sanbdox-rebuilddns&#34;&gt;sanbdox.rebuildDNS&lt;/h3&gt;

&lt;p&gt;sanbdox.rebuildDNS负责构建容器内的resolv.conf，构建规则就是第一节江参数配置时候提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save the external name servers in resolv.conf in the sandbox&lt;/li&gt;
&lt;li&gt;Add only the embedded server’s IP to container’s resolv.conf&lt;/li&gt;
&lt;li&gt;If the embedded server needs any resolv.conf options add it to the current list&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox_dns_unix.go

func (sb *sandbox) rebuildDNS() error {
    currRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)
    if err != nil {
        return err
    }

    // localhost entries have already been filtered out from the list
    // retain only the v4 servers in sb for forwarding the DNS queries
    sb.extDNS = resolvconf.GetNameservers(currRC.Content, types.IPv4)

    var (
        dnsList        = []string{sb.resolver.NameServer()}
        dnsOptionsList = resolvconf.GetOptions(currRC.Content)
        dnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)
    )

    dnsList = append(dnsList, resolvconf.GetNameservers(currRC.Content, types.IPv6)...)

    resOptions := sb.resolver.ResolverOptions()

dnsOpt:
    ...
    dnsOptionsList = append(dnsOptionsList, resOptions...)

    _, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setextservers&#34;&gt;resolver.SetExtServers&lt;/h3&gt;

&lt;p&gt;设置embedded DNS server的forward DNS list, 当embedded DNS server不能解析某name时，就会将请求forward到ExtServers。代码很简单，不多废话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go
func (r *resolver) SetExtServers(dns []string) {
    l := len(dns)
    if l &amp;gt; maxExtDNS {
        l = maxExtDNS
    }
    for i := 0; i &amp;lt; l; i++ {
        r.extDNSList[i].ipStr = dns[i]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setupfunc&#34;&gt;resolver.SetupFunc&lt;/h3&gt;

&lt;p&gt;启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;libnetwork/resolver.go

func (r *resolver) SetupFunc() func() {
    return (func() {
        var err error

        // DNS operates primarily on UDP
        addr := &amp;amp;net.UDPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.conn, err = net.ListenUDP(&amp;quot;udp&amp;quot;, addr)
        ...

        // Listen on a TCP as well
        tcpaddr := &amp;amp;net.TCPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.tcpListen, err = net.ListenTCP(&amp;quot;tcp&amp;quot;, tcpaddr)
        ...
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-start&#34;&gt;resolver.Start&lt;/h3&gt;

&lt;p&gt;resolver.Start中两个重要步骤，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;setupIPTable设置容器内的iptables&lt;/li&gt;
&lt;li&gt;启动dns nameserver在53端口开始提供域名解析服务&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;func (r *resolver) Start() error {
    ...
    if err := r.setupIPTable(); err != nil {
        return fmt.Errorf(&amp;quot;setting up IP table rules failed: %v&amp;quot;, err)
    }
    ...
    tcpServer := &amp;amp;dns.Server{Handler: r, Listener: r.tcpListen}
    r.tcpServer = tcpServer
    go func() {
        tcpServer.ActivateAndServe()
    }()
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先来看看怎么设置容器内的iptables的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (r *resolver) setupIPTable() error {
    ...
    // 获取setupFunc()时的两个本地随机监听端口
    laddr := r.conn.LocalAddr().String()
    ltcpaddr := r.tcpListen.Addr().String()

    cmd := &amp;amp;exec.Cmd{
        Path:   reexec.Self(),
        // 将这两个端口传给setup-resolver命令并启动执行
        Args:   append([]string{&amp;quot;setup-resolver&amp;quot;}, r.sb.Key(), laddr, ltcpaddr),
        Stdout: os.Stdout,
        Stderr: os.Stderr,
    }
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(&amp;quot;reexec failed: %v&amp;quot;, err)
    }
    return nil
}

// init时就注册setup-resolver对应的handler
func init() {
    reexec.Register(&amp;quot;setup-resolver&amp;quot;, reexecSetupResolver)
}

// setup-resolver对应的handler定义
func reexecSetupResolver() {
    ...
    // 封装iptables数据
    _, ipPort, _ := net.SplitHostPort(os.Args[2])
    _, tcpPort, _ := net.SplitHostPort(os.Args[3])
    rules := [][]string{
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[2]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--sport&amp;quot;, ipPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[3]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--sport&amp;quot;, tcpPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
    }
    ...

    // insert outputChain and postroutingchain
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在reexecSetupResolver()中清楚的定义了iptables添加outputChain 和postroutingchain，将到容器内的dns query请求重定向到embedded DNS server(127.0.0.11)上的udp/tcp两个随机可用端口，embedded DNS server(127.0.0.11)的返回数据则重定向到容器内的53端口，这样完成了整个dns query请求。&lt;/p&gt;

&lt;p&gt;模型图如下： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215440792?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;贴一张实例图： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215310369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20170105215322635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到这里，关于embedded DNS server的源码分析就结束了。当然，其中还有很多细节，就留给读者自己走读代码了。&lt;/p&gt;

&lt;h2 id=&#34;福利&#34;&gt;福利&lt;/h2&gt;

&lt;p&gt;从该时序图中看看embedded DNS server的操作在整个容器create流程中的位置。
&lt;img src=&#34;http://img.blog.csdn.net/20170105215401307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>raft consensus algorithm</title>
      <link>http://rootsongjc.github.io/post/raft/</link>
      <pubDate>Mon, 27 Feb 2017 10:47:14 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/raft/</guid>
      <description>&lt;p&gt;A wonderful raft consensus algorithm illustation
&lt;a href=&#34;http://thesecretlivesofdata.com/raft/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TalkingData Annual Meeting</title>
      <link>http://rootsongjc.github.io/presentation/td-annual-meeting/</link>
      <pubDate>Sun, 26 Feb 2017 20:18:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/presentation/td-annual-meeting/</guid>
      <description>

&lt;h1 id=&#34;talkingdata-annual-meeting-2017&#34;&gt;TalkingData Annual Meeting 2017&lt;/h1&gt;

&lt;p&gt;Dayin Theater, Beijing Friday, Feb 24, 2017&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017022401.jpeg&#34; alt=&#34;pic1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017022402.jpeg&#34; alt=&#34;pics2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Photo by &lt;a href=&#34;http://rootsongjc.github.io&#34;&gt;Jimmy Song&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My github pages</title>
      <link>http://rootsongjc.github.io/project/my_github_pages/</link>
      <pubDate>Wed, 22 Feb 2017 16:56:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/project/my_github_pages/</guid>
      <description>

&lt;h1 id=&#34;my-open-source-project&#34;&gt;My Open Source Project&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://github.com/rootsongjc/magpie&#34;&gt;Magpie&lt;/a&gt; - Magpie is a command line tool for deploying and managing Yarn on Docker cluster.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://github.com/rootsongjc/docker-ipam-plugin&#34;&gt;Docker IPAM plugin&lt;/a&gt; - Docker network plugin to make a L2 flat network.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rootsongjc.github.io/docker-practice/&#34;&gt;Docker practice&lt;/a&gt; - Docker in practice&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rootsongjc.github.io/go-practice/&#34;&gt;Go practice&lt;/a&gt; - Go in practice&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rootsongjc.github.io/linux-practice&#34;&gt;Linux practice&lt;/a&gt; - Linux in practice :)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rootsongjc.github.com/team-management&#34;&gt;Team management&lt;/a&gt; - About team management&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>hello world</title>
      <link>http://rootsongjc.github.io/post/helloworld/</link>
      <pubDate>Mon, 20 Feb 2017 22:28:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/helloworld/</guid>
      <description>

&lt;h1 id=&#34;hello-world&#34;&gt;Hello world&lt;/h1&gt;

&lt;p&gt;This is my first post!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://rootsongjc.github.io/about/</link>
      <pubDate>Mon, 20 Feb 2017 20:10:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/about/</guid>
      <description>

&lt;h1 id=&#34;about-me&#34;&gt;About me&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Jimmy Song&lt;/li&gt;
&lt;li&gt;rootsongjc@gmail.com&lt;/li&gt;
&lt;li&gt;wechat: jimmysong&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-i-like&#34;&gt;What I like&lt;/h2&gt;

&lt;p&gt;Movie, Kubrick, Miyazaki, animation, Akira Kurosawa, photography, Daido Moriyama, potato, sony, apple, orange, Fellini, douban, google, Netease cloud music, Jay Chou, badminton, travelling, programming, golang, Java, Python, sea, blue, Henri Cartier-Bresson, Command &amp;amp; Conqure, Italy.&lt;/p&gt;

&lt;h2 id=&#34;what-i-dislike&#34;&gt;What I dislike&lt;/h2&gt;

&lt;p&gt;Chattering, Chemical fiber clothes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;imgs/wechat_qrcode.jpg&#34; alt=&#34;wechat_qrcode&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>