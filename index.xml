<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/index.xml</link>
    <description>Recent content on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Apr 2017 20:07:24 +0800</lastBuildDate>
    <atom:link href="http://rootsongjc.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>使用Fluentd和ElasticSearch收集Kubernetes集群日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</link>
      <pubDate>Fri, 07 Apr 2017 20:07:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160430080.jpg&#34; alt=&#34;古北水镇&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：码头@古北水镇 Apr 30,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;安装好了Kubernetes集群&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;配置好了flannel网络&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/&#34;&gt;安装了Kubernetes Dashboard&lt;/a&gt;和&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/&#34;&gt;配置Heapster监控插件&lt;/a&gt;后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;官方文档&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34;&gt;Kubernetes Logging and Monitoring Cluster Activity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/&#34;&gt;Logging Using Elasticsearch and Kibana&lt;/a&gt;：不过这篇文章是在GCE上配置的，参考价值不大。&lt;/p&gt;

&lt;h2 id=&#34;容器日志的存在形式&#34;&gt;容器日志的存在形式&lt;/h2&gt;

&lt;p&gt;目前容器日志有两种输出形式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;stdout,stderr标准输出&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种形式的日志输出我们可以直接使用&lt;code&gt;docker logs&lt;/code&gt;查看日志，kubernetes集群中同样可以使用&lt;code&gt;kubectl logs&lt;/code&gt;类似的形式查看日志。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;日志文件记录&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种日志输出我们无法从以上方法查看日志内容，只能&lt;code&gt;tail&lt;/code&gt;日志文件查看。&lt;/p&gt;

&lt;h2 id=&#34;fluentd介绍&#34;&gt;Fluentd介绍&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/fluent/fluentd&#34;&gt;Fluentd&lt;/a&gt;是使用Ruby编写的，通过在后端系统之间提供&lt;strong&gt;统一的日志记录层&lt;/strong&gt;来从后端系统中解耦数据源。
此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。
统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。
也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/c4abfe337c0b54b36f81bce78481f8965acbc7a9/687474703a2f2f646f63732e666c75656e74642e6f72672f696d616765732f666c75656e74642d6172636869746563747572652e706e67&#34; alt=&#34;arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fluentd收集的&lt;strong&gt;event&lt;/strong&gt;由以下几个方面组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tag&lt;/strong&gt;：字符串，中间用点隔开，如myapp.access&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;：UNIX时间格式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Record&lt;/strong&gt;：JSON格式&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fluentd特点&#34;&gt;Fluentd特点&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;部署简单灵活&lt;/li&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;经过验证的可靠性和性能&lt;/li&gt;
&lt;li&gt;社区支持，插件较多&lt;/li&gt;
&lt;li&gt;使用json格式事件格式&lt;/li&gt;
&lt;li&gt;可拔插的架构设计&lt;/li&gt;
&lt;li&gt;低资源要求&lt;/li&gt;
&lt;li&gt;内置高可靠性&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;查看&lt;code&gt;cluster/addons/fluentd-elasticsearch&lt;/code&gt;插件目录，获取到需要用到的docker镜像名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$grep -rn &amp;quot;gcr.io&amp;quot; *.yaml
es-controller.yaml:24:      - image: gcr.io/google_containers/elasticsearch:v2.4.1-2
fluentd-es-ds.yaml:26:        image: gcr.io/google_containers/fluentd-elasticsearch:1.22
kibana-controller.yaml:22:        image: gcr.io/google_containers/kibana:v4.6.1-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;需要用到的镜像&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcr.io/google_containers/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为这些镜像在墙外，所以我特意备份了一份在本地还有时速云上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;测试环境镜像名称&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;备份到时速云上的镜像名称&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改上面的那三个yaml文件，将其中的镜像名称改成我们测试环境中的。&lt;/p&gt;

&lt;h3 id=&#34;启动集群&#34;&gt;启动集群&lt;/h3&gt;

&lt;p&gt;使用刚修改好yaml文件的那个目录启动fluentd-elasticsearch。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f flucentd-elasticsearch
$kubectl get -f fluentd-elasticsearch/
NAME                          DESIRED   CURRENT   READY     AGE
rc/elasticsearch-logging-v1   2         2         2         13m

NAME                        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/elasticsearch-logging   10.254.107.114   &amp;lt;none&amp;gt;        9200/TCP   13m

NAME                  DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
ds/fluentd-es-v1.22   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   13m

NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kibana-logging   1         1         1            1           13m

NAME                 CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/kibana-logging   10.254.104.215   &amp;lt;none&amp;gt;        5601/TCP   13m

$kubectl cluster-info
Kubernetes master is running at http://sz-pg-oam-docker-test-001:8080
Elasticsearch is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging
monitoring-grafana is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动完成，但是查看pod的日志后会发现出错了。&lt;/p&gt;

&lt;p&gt;如何保证每个节点启动一个Fluentd呢？答案是使用DaemonSet。&lt;/p&gt;

&lt;h3 id=&#34;排错&#34;&gt;排错&lt;/h3&gt;

&lt;p&gt;查看启动的pod。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl --namespace=kube-system get all
NAME                                       READY     STATUS    RESTARTS   AGE
po/elasticsearch-logging-v1-nshz2          1/1       Running   0          16m
po/elasticsearch-logging-v1-q515j          1/1       Running   0          16m
po/heapster-3669180046-06n3d               1/1       Running   0          23h
po/kibana-logging-4247188994-h8jxx         1/1       Running   0          16m
po/kubernetes-dashboard-1074266307-hsgxx   1/1       Running   0          1d
po/monitoring-grafana-127711743-xl9v1      1/1       Running   0          23h
po/monitoring-influxdb-1411048194-cvxmm    1/1       Running   0          23h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;应该在个node节点上启动的&lt;strong&gt;fluentd&lt;/strong&gt;没有看到。查看logging pod的日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl -n kube-system logs po/elasticsearch-logging-v1-nshz2
F0406 08:30:05.488197       7 elasticsearch_logging_discovery.go:49] Failed to make client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
goroutine 1 [running]:
...
[2017-04-06 08:30:23,450][WARN ][discovery.zen.ping.unicast] [elasticsearch-logging-v1-nshz2] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[][127.0.0.1:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:945)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:360)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$4400(ZenDiscovery.java:96)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1296)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected]
	at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1141)
	at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:830)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:329)
	... 12 more
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到报错中有这样的描述：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discovery.zen.ping.unicast failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[ Node not connected]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面有两个错误：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无法访问到API Server&lt;/li&gt;
&lt;li&gt;elasticsearch两个节点间互ping失败&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个是镜像中的配置问题，配置文件在&lt;code&gt;fluentd-es-image/td-agent.conf&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;参考&lt;a href=&#34;http://tonybai.com/2017/03/03/implement-kubernetes-cluster-level-logging-with-fluentd-and-elasticsearch-stack/&#34;&gt;使用Fluentd和ElasticSearch Stack实现Kubernetes的集群Logging&lt;/a&gt;，Tony Bai也遇到了这个问题，我们了解下&lt;a href=&#34;https://kubernetes.io/docs/user-guide/configmap/&#34;&gt;ConfigMap&lt;/a&gt;还有&lt;a href=&#34;https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter&#34;&gt;fluent-plugin-kubernetes_metadata_filter&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;参考我的另一片译文&lt;a href=&#34;rootsongjc.github.io/blogs/kubernetes-configmap-introduction&#34;&gt;Kubernetes中ConfigMap解析&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;问题排查&#34;&gt;问题排查&lt;/h2&gt;

&lt;p&gt;前面写的是直接使用&lt;code&gt;kubectl create -f flucentd-elasticsearch&lt;/code&gt;命令启动整个fluentd+elasticsearch集群，这样启动看似很简单，但是对于问题排查的时候不便于我们分析出错原因，因为你根本不知道服务之间的依赖关系和启动顺序，所以现在我们依次启动每个服务，看看背后都做了什么。&lt;/p&gt;

&lt;h3 id=&#34;启动fluentd&#34;&gt;启动fluentd&lt;/h3&gt;

&lt;p&gt;首先启动fluentd收集日志的服务，从&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;的配置中可以看到fluentd是以&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt;方式来运行的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DaemonSet简介&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DaemonSet能够让所有（或者一些特定）的Node节点运行同一个pod。当节点加入到kubernetes集群中，pod会被（DaemonSet）调度到该节点上运行，当节点从kubernetes集群中被移除，被（DaemonSet）调度的pod会被移除，如果删除DaemonSet，所有跟这个DaemonSet相关的pods都会被删除。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dockerinfo.net/1139.html&#34;&gt;DaemonSet详细介绍&lt;/a&gt;，这是官方文档的中文翻译，其中还有示例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动fluentd&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f fluentd-es-ds.yaml
daemonset &amp;quot;fluentd-es-v1.22&amp;quot; created
$kubectl get -f fluentd-es-ds.yaml 
NAME               DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
fluentd-es-v1.22   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在没有修改&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;的情况下直接启动fluentd，实际上一个Pod也没有启动起来，这是为什么呢？因为&lt;strong&gt;NODE-SELECTOR&lt;/strong&gt;选择的label是&lt;code&gt;beta.kubernetes.io/fluentd-ds-ready=true&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;我们再来看下&lt;strong&gt;node&lt;/strong&gt;的&lt;strong&gt;label&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl describe node sz-pg-oam-docker-test-001.tendcloud.com
Name:			sz-pg-oam-docker-test-001.tendcloud.com
Role:			
Labels:			beta.kubernetes.io/arch=amd64
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=sz-pg-oam-docker-test-001.tendcloud.com
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们没有给node设置&lt;code&gt;beta.kubernetes.io/fluentd-ds-ready=true&lt;/code&gt;的label，所以DaemonSet没有调度上去。&lt;/p&gt;

&lt;p&gt;我们需要手动给kubernetes集群的三个node添加label。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl label node sz-pg-oam-docker-test-001.tendcloud.com beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;sz-pg-oam-docker-test-001.tendcloud.com&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给另外两个node执行同样的操作。&lt;/p&gt;

&lt;p&gt;现在再查看下DaemonSet的状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get -f fluentd-es-ds.yaml 
NAME               DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
fluentd-es-v1.22   3         3         0         3            0           beta.kubernetes.io/fluentd-ds-ready=true   31m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以看到三个DeamonSet都启动起来了。&lt;/p&gt;

&lt;p&gt;查看下fluentd的日志&lt;code&gt;/var/log/fluentd.log&lt;/code&gt;，日志是mount到本地的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-04-07 03:53:42 +0000 [info]: adding match pattern=&amp;quot;fluent.**&amp;quot; type=&amp;quot;null&amp;quot;
2017-04-07 03:53:42 +0000 [info]: adding filter pattern=&amp;quot;kubernetes.**&amp;quot; type=&amp;quot;kubernetes_metadata&amp;quot;
2017-04-07 03:53:42 +0000 [error]: config error file=&amp;quot;/etc/td-agent/td-agent.conf&amp;quot; error=&amp;quot;Invalid Kubernetes API v1 endpoint https://10.254.0.1:443/api: SSL_connect returned=1 errno=0 state=error: certificate verify failed&amp;quot;
2017-04-07 03:53:42 +0000 [info]: process finished code=256
2017-04-07 03:53:42 +0000 [warn]: process died within 1 second. exit.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从日志的最后几行中可以看到，&lt;code&gt;Invalid Kubernetes API v1 endpoint https://10.254.0.1:443/api: SSL_connect returned=1 errno=0 state=error: certificate verify failed&lt;/code&gt;这样的错误，这些需要在&lt;code&gt;/etc/td-agent/td-agent.conf&lt;/code&gt;文件中配置的。&lt;/p&gt;

&lt;p&gt;但是这些配置已经在创建&lt;code&gt;gcr.io/google_containers/fluentd-elasticsearch:1.22&lt;/code&gt;镜像（该镜像是运行带有elasticsearch插件的fluentd的）的时候就已经copy进去了，从&lt;code&gt;fluentd-elasticsearch/fluentd-es-image/Dockerfile&lt;/code&gt;文件中就可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Copy the Fluentd configuration file.
COPY td-agent.conf /etc/td-agent/td-agent.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/&#34;&gt;ConfigMap&lt;/a&gt;，不用重新再build镜像，通过文件挂载的形式替换镜像中已有的td-agent.conf文件。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;tonybai.com&#34;&gt;Tony Bai&lt;/a&gt;给出的两点建议：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在基于td-agent.conf创建configmap资源之前，需要将td-agent.conf中的注释行都删掉，否则生成的configmap的内容可能不正确；&lt;/li&gt;
&lt;li&gt;fluentd pod将创建在kube-system下，因此ConfigMap资源也需要创建在kube-system namespace下面，否则kubectl create无法找到对应的ConfigMap。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在td-agent.conf的配置文件的&lt;filter kubernetes.**&gt;中增加两条配置配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter kubernetes.**&amp;gt;
  type kubernetes_metadata
  kubernetes_url sz-pg-oam-docker-test-001.tendcloud.com:8080
  verify_ssl false
&amp;lt;/filter&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建ConfigMap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap td-agent-config --from-file=fluentd-elasticsearch/fluentd-es-image/td-agent.conf -n kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看刚创建的ConfigMap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl -n kube-system get configmaps td-agent-config -o yaml
apiVersion: v1
data:
  td-agent.conf: |
    &amp;lt;match fluent.**&amp;gt;
      type null
    &amp;lt;/match&amp;gt;
...
&amp;lt;filter kubernetes.**&amp;gt;
  type kubernetes_metadata
  kubernetes_url http://sz-pg-oam-docker-test-001.tendcloud.com:8080
  verify_ssl false
&amp;lt;/filter&amp;gt;
...

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;⚠️ kubernetes_url地址要加上&lt;strong&gt;http&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;修改&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;文件，在其中增加&lt;code&gt;td-agent.conf&lt;/code&gt;文件的volume。&lt;/p&gt;

&lt;p&gt;该文件的部分内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
...
    spec:
     ...
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: td-agent-config
          mountPath: /etc/td-agent
...
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: td-agent-config
        configMap:
          name: td-agent-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动日志收集服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./fluentd-elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在再查看&lt;code&gt;/var/log/fluentd.log&lt;/code&gt;日志里面就没有错误了。&lt;/p&gt;

&lt;p&gt;查看下elasticsearch pod日志，发现里面还有错误，跟以前的一样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2017-04-07 10:54:57,858][WARN ][discovery.zen.ping.unicast] [elasticsearch-logging-v1-wxd5f] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[][127.0.0.1:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:945)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:360)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$4400(ZenDiscovery.java:96)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1296)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected]
	at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1141)
	at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:830)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:329)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看下elasticsearch:v2.4.1-2镜像的代码，在&lt;code&gt;fluentd-elasticsearch/es-image&lt;/code&gt;目录下，该目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config
Dockerfile
elasticsearch_logging_discovery.go
Makefile
run.sh
template-k8s-logstash.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从&lt;strong&gt;Dockerfile&lt;/strong&gt;中可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;RUN mkdir -p /elasticsearch/config/templates
COPY template-k8s-logstash.json /elasticsearch/config/templates/template-k8s-logstash.json

COPY config /elasticsearch/config

COPY run.sh /
COPY elasticsearch_logging_discovery /

RUN useradd --no-create-home --user-group elasticsearch \
    &amp;amp;&amp;amp; mkdir /data \
    &amp;amp;&amp;amp; chown -R elasticsearch:elasticsearch /elasticsearch

VOLUME [&amp;quot;/data&amp;quot;]
EXPOSE 9200 9300

CMD [&amp;quot;/run.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将本地的&lt;code&gt;config&lt;/code&gt;目录作为配置文件拷贝到了镜像里，&lt;code&gt;run.sh&lt;/code&gt;启动脚本中有三行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/elasticsearch_logging_discovery &amp;gt;&amp;gt; /elasticsearch/config/elasticsearch.yml

chown -R elasticsearch:elasticsearch /data

exec gosu elasticsearch /elasticsearch/bin/elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再进入到镜像里查看下&lt;code&gt;/elasticsearch/config/elasticsearch.yml&lt;/code&gt;文件的内容。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;cluster.name: kubernetes-logging

node.name: ${NODE_NAME}
node.master: ${NODE_MASTER}
node.data: ${NODE_DATA}

transport.tcp.port: ${TRANSPORT_PORT}
http.port: ${HTTP_PORT}

path.data: /data

network.host: 0.0.0.0

discovery.zen.minimum_master_nodes: ${MINIMUM_MASTER_NODES}
discovery.zen.ping.multicast.enabled: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes中的DNS没有配置。&lt;/li&gt;
&lt;li&gt;ElasticSearch的配置有问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;To be continued…&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes的ConfigMap解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</link>
      <pubDate>Thu, 06 Apr 2017 21:24:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160430004.jpg&#34; alt=&#34;古北水镇&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：龙形灯笼@古北水镇 Apr 30,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;为什么要翻译这篇文章，是因为我在&lt;a href=&#34;rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation&#34;&gt;使用Fluentd和ElasticSearch收集Kubernetes集群日志&lt;/a&gt;的时候遇到了需要修改镜像中配置的问题，&lt;a href=&#34;https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter&#34;&gt;fluent-plugin-kubernetes_metadata&lt;/a&gt;里的需要的&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image/td-agent.conf&#34;&gt;td-agent.conf&lt;/a&gt;文件。&lt;/p&gt;

&lt;p&gt;其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。&lt;/p&gt;

&lt;h2 id=&#34;configmap概览&#34;&gt;ConfigMap概览&lt;/h2&gt;

&lt;p&gt;The ConfigMap API resource holds key-value pairs of configuration data that can be consumed in pods or used to store configuration data for system components such as controllers. ConfigMap is similar to [Secrets](), but designed to more conveniently support working with strings that do not contain sensitive information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ConfigMap API&lt;/strong&gt;资源用来保存&lt;strong&gt;key-value pair&lt;/strong&gt;配置数据，这个数据可以在&lt;strong&gt;pods&lt;/strong&gt;里使用，或者被用来为像&lt;strong&gt;controller&lt;/strong&gt;一样的系统组件存储配置数据。虽然ConfigMap跟&lt;a href=&#34;https://kubernetes.io/docs/user-guide/secrets/&#34;&gt;Secrets&lt;/a&gt;类似，但是ConfigMap更方便的处理不含敏感信息的字符串。
注意：&lt;u&gt;ConfigMaps不是属性配置文件的替代品。&lt;/u&gt;ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的&lt;code&gt;/etc&lt;/code&gt;目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。&lt;/p&gt;

&lt;p&gt;Note: ConfigMaps are not intended to act as a replacement for a properties file. ConfigMaps are intended to act as a reference to multiple properties files. You can think of them as way to represent something similar to the /etc directory, and the files within, on a Linux computer. One example of this model is creating Kubernetes Volumes from ConfigMaps, where each data item in the ConfigMap becomes a new file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: ConfigMap
apiVersion: v1
metadata:
  creationTimestamp: 2016-02-18T19:14:38Z
  name: example-config
  namespace: default
data:
  example.property.1: hello
  example.property.2: world
  example.property.file: |-
    property.1=value-1
    property.2=value-2
    property.3=value-3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;data&lt;/code&gt;一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。
配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;设置环境变量的值&lt;/li&gt;
&lt;li&gt;在容器里设置命令行参数&lt;/li&gt;
&lt;li&gt;在数据卷里面创建config文件&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;用户和系统组件两者都可以在ConfigMap里面存储配置数据。&lt;/p&gt;

&lt;p&gt;其实不用看下面的文章，直接从&lt;code&gt;kubectl create configmap -h&lt;/code&gt;的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Examples:
  # Create a new configmap named my-config based on folder bar
  kubectl create configmap my-config --from-file=path/to/bar
  
  # Create a new configmap named my-config with specified keys instead of file basenames on disk
  kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt
  
  # Create a new configmap named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建configmaps&#34;&gt;创建ConfigMaps&lt;/h2&gt;

&lt;p&gt;可以使用该命令，用给定值、文件或目录来创建ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;使用目录创建&#34;&gt;使用目录创建&lt;/h3&gt;

&lt;p&gt;比如我们已经有个了包含一些配置文件，其中包含了我们想要设置的ConfigMap的值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls docs/user-guide/configmap/kubectl/
game.properties
ui.properties

$ cat docs/user-guide/configmap/kubectl/game.properties
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30

$ cat docs/user-guide/configmap/kubectl/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用下面的命令可以创建一个包含目录中所有文件的ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;—from-file&lt;/code&gt;指定在目录下的所有文件都会被用在ConfigMap里面创建一个键值对，键的名字就是文件名，值就是文件的内容。&lt;/p&gt;

&lt;p&gt;让我们来看一下这个命令创建的ConfigMap：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe configmaps game-config
Name:           game-config
Namespace:      default
Labels:         &amp;lt;none&amp;gt;
Annotations:    &amp;lt;none&amp;gt;

Data
====
game.properties:        158 bytes
ui.properties:          83 bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到那两个key是从kubectl指定的目录中的文件名。这些key的内容可能会很大，所以在kubectl describe的输出中，只能够看到键的名字和他们的大小。
如果想要看到键的值的话，可以使用&lt;code&gt;kubectl get&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get configmaps game-config -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们以&lt;code&gt;yaml&lt;/code&gt;格式输出配置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
  game.properties: |
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T18:34:05Z
  name: game-config
  namespace: default
  resourceVersion: &amp;quot;407&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/game-config
  uid: 30944725-d66e-11e5-8cd0-68f728db1985
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;使用文件创建&#34;&gt;使用文件创建&lt;/h3&gt;

&lt;p&gt;刚才&lt;strong&gt;使用目录创建&lt;/strong&gt;的时候我们&lt;code&gt;—from-file&lt;/code&gt;指定的是一个目录，只要指定为一个文件就可以从单个文件中创建ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties 

$ kubectl get configmaps game-config-2 -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
data:
  game-special-key: |
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T18:54:22Z
  name: game-config-3
  namespace: default
  resourceVersion: &amp;quot;530&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/game-config-3
  uid: 05f8da22-d671-11e5-8cd0-68f728db1985
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;—from-file&lt;/code&gt;这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。&lt;/p&gt;

&lt;h3 id=&#34;使用literal值创建&#34;&gt;使用literal值创建&lt;/h3&gt;

&lt;p&gt;使用文字值创建，利用&lt;code&gt;—from-literal&lt;/code&gt;参数传递配置信息，该参数可以使用多次，格式如下；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm

$ kubectl get configmaps special-config -o yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
data:
  special.how: very
  special.type: charm
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T19:14:38Z
  name: special-config
  namespace: default
  resourceVersion: &amp;quot;651&amp;quot;
  selfLink: /api/v1/namespaces/default/configmaps/special-config
  uid: dadce046-d673-11e5-8cd0-68f728db1985
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pod中使用configmap&#34;&gt;Pod中使用ConfigMap&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;使用ConfigMap来替代环境变量&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ConfigMap可以被用来填入环境变量。看下下面的ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: env-config
  namespace: default
data:
  log_level: INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以在Pod中这样使用ConfigMap：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;env&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个Pod运行后会输出如下几行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;用ConfigMap设置命令行参数&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ConfigMap也可以被使用来设置容器中的命令或者参数值。它使用的是Kubernetes的$(VAR_NAME)替换语法。我们看下下面这个ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了将ConfigMap中的值注入到命令行的参数里面，我们还要像前面那个例子一样使用环境变量替换语法&lt;code&gt;${VAR_NAME)&lt;/code&gt;。（其实这个东西就是给Docker容器设置环境变量，以前我创建镜像的时候经常这么玩，通过docker run的时候指定-e参数修改镜像里的环境变量，然后docker的CMD命令再利用该$(VAR_NAME)通过sed来来修改配置文件或者作为命令行启动参数。）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行这个Pod后会输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;通过数据卷插件使用ConfigMap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ConfigMap也可以在数据卷里面被使用。还是这个ConfigMap。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在数据卷里面使用这个ConfigMap，有不同的选项。最基本的就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;cat /etc/config/special.how&amp;quot; ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行这个Pod的输出是&lt;code&gt;very&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;我们也可以在ConfigMap值被映射的数据卷里控制路径。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;,&amp;quot;-c&amp;quot;,&amp;quot;cat /etc/config/path/to/special-key&amp;quot; ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: special.how
          path: path/to/special-key
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行这个Pod后的结果是&lt;code&gt;very&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;还是那句话，如果你了解docker的ENV机制就会明白ConfigMap背后究竟发生了什么，so easy~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow深度学习手写数字识别初体验</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</link>
      <pubDate>Wed, 05 Apr 2017 21:52:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/201703085.jpg&#34; alt=&#34;禾雀&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：禾雀 @北京动物园 Apr 3,2017）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;TensorFlow学习曲线是陡峭的，不是所有的IT从业人员都很容易参与的，你需要有一定的数学专业知识，对于对深度学习没有经验的程序员，要想了解这门技术，最快捷的途径是先运行一个示例，我们认识事物都是先从感性、到理性的思辨过程。&lt;/p&gt;

&lt;p&gt;下面我们来跟随&lt;strong&gt;Martin Gorner&lt;/strong&gt;的&lt;a href=&#34;https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0&#34;&gt;TensorFlow and Deep Learing Without a PhD&lt;/a&gt;来编写我们的第一个TensorFlow程序——手写数字识别，这篇文章的中文版&lt;a href=&#34;http://www.jiqizhixin.com/article/2458&#34;&gt;没有博士学位如何玩转TensorFlow和深度学习&lt;/a&gt;于2017年3月13日发表在发表在&lt;a href=&#34;www.jiqizhixin.com&#34;&gt;机器之心&lt;/a&gt;上。这篇文章也是根据3月8日-10日的&lt;strong&gt;Google Cloud NEXT&amp;rsquo;17&lt;/strong&gt;大会上Martin Gorner做的讲解整理而成的，&lt;a href=&#34;http://it.sohu.com/20170124/n479480999.shtml&#34;&gt;教程 | 没有博士学位，照样玩转TensorFlow深度学习&lt;/a&gt;这篇文章是对Martin Gorner的简易教程的原文翻译，我们暂时不要求了解TensorFlow背后复杂的理论，我们先跟随这篇简易教程玩一把TensorFlow的手写数字识别。&lt;/p&gt;

&lt;p&gt;如果你想深入了解这本后的原理的话，可以查看哈尔滨工业大学社会计算与信息检索研究中心翻译的&lt;a href=&#34;https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details&#34;&gt;《神经网络与深度学习》&lt;/a&gt;这本书，该书翻译自&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt;的中文翻译，原文作者 Michael Nielsen，而且这还是一本免费的电子书，该书中系统讲解了&lt;a href=&#34;https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap1/c1s0.html&#34;&gt;使用神经网络识别手写数字&lt;/a&gt;背后的原理。该书托管在GitBook上，你可以点击&lt;a href=&#34;http://olz1di9xf.bkt.clouddn.com/neural-networks-and-deep-learning-zh_cn.pdf&#34;&gt;这里&lt;/a&gt;直接下载该书中文版的PDF。&lt;/p&gt;

&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;下载代码&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个代码仓库里包含了手写数字识别和下载依赖的训练数据的代码，我们将只用到&lt;code&gt;mnist_1.0_softmax.py&lt;/code&gt;这一个代码文件。整个&lt;code&gt;mnist_1.0_softmax.py&lt;/code&gt;代码并不复杂，不算注释的话只有36行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/martin-gorner/tensorflow-mnist-tutorial.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载完后，可以看到有一个&lt;strong&gt;INSTALL.txt&lt;/strong&gt;，这篇文章是运行代码所必需的环境要求说明。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装TensorFlow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我之前写过详细的TensorFlow安装教程&lt;a href=&#34;http://rootsongjc.github.io/blogs/tensorflow-practice-02/&#34;&gt;TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建&lt;/a&gt;，这篇文章中主要讲怎样在docker里安装TensorFlow。&lt;/p&gt;

&lt;p&gt;我使用的Mac而且还是python2.7，所以我这样安装：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install --upgrade tensorflow --user -U
pip install --upgrade matplotlib --user -U
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;运行示例&#34;&gt;运行示例&lt;/h2&gt;

&lt;p&gt;运行手写数字训练示例。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python mnist_1.0_softmax.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行过程中你会看到一大段输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Collecting matplotlib
  Downloading matplotlib-2.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (12.8MB)
    100% |████████████████████████████████| 12.8MB 26kB/s 
Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,&amp;gt;=1.5.6 in /Users/jimmy/Library/Python/2.7/lib/python/site-packages (from matplotlib)
Requirement already up-to-date: numpy&amp;gt;=1.7.1 in /Users/jimmy/Library/Python/2.7/lib/python/site-packages (from matplotlib)
Collecting functools32 (from matplotlib)
  Downloading functools32-3.2.3-2.zip
Collecting pytz (from matplotlib)
  Downloading pytz-2017.2-py2.py3-none-any.whl (484kB)
    100% |████████████████████████████████| 491kB 33kB/s 
Requirement already up-to-date: six&amp;gt;=1.10 in /Users/jimmy/Library/Python/2.7/lib/python/site-packages (from matplotlib)
Collecting cycler&amp;gt;=0.10 (from matplotlib)
  Downloading cycler-0.10.0-py2.py3-none-any.whl
Collecting subprocess32 (from matplotlib)
  Downloading subprocess32-3.2.7.tar.gz (54kB)
    100% |████████████████████████████████| 61kB 26kB/s 
Collecting python-dateutil (from matplotlib)
  Downloading python_dateutil-2.6.0-py2.py3-none-any.whl (194kB)
    100% |████████████████████████████████| 194kB 45kB/s 
Building wheels for collected packages: functools32, subprocess32
  Running setup.py bdist_wheel for functools32 ... done
  Stored in directory: /Users/jimmy/Library/Caches/pip/wheels/3c/d0/09/cd78d0ff4d6cfecfbd730782a7815a4571cd2cd4d2ed6e69d9
  Running setup.py bdist_wheel for subprocess32 ... done
  Stored in directory: /Users/jimmy/Library/Caches/pip/wheels/7d/4c/a4/ce9ceb463dae01f4b95e670abd9afc8d65a45f38012f8030cc
Successfully built functools32 subprocess32
Installing collected packages: functools32, pytz, cycler, subprocess32, python-dateutil, matplotlib
Successfully installed cycler-0.10.0 functools32-3.2.3.post2 matplotlib-2.0.0 python-dateutil-2.6.0 pytz-2017.2 subprocess32-3.2.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看到这个过程中下载了一些python依赖库如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;matplotlib&lt;/li&gt;
&lt;li&gt;pytz&lt;/li&gt;
&lt;li&gt;subprocess32&lt;/li&gt;
&lt;li&gt;cycler&lt;/li&gt;
&lt;li&gt;python_dateutil&lt;/li&gt;
&lt;li&gt;functools32&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有的依赖都下载完成之后，就会弹出一个窗口，同时后台也会在不断滚动显示训练的阶段，直到2001步，如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-01.jpg&#34; alt=&#34;tensorflow-mnist-01&#34; /&gt;&lt;/p&gt;

&lt;p&gt;正规的TensorFlow项目会使用&lt;strong&gt;TensorBoard&lt;/strong&gt;作可视化，我们用&lt;strong&gt;matplotlib&lt;/strong&gt;作为替代。&lt;/p&gt;

&lt;p&gt;至此整个训练过程结束了，但是我们还不明白这个窗口里的6个图分别表示的含义，下面将依次作出解释。&lt;/p&gt;

&lt;h2 id=&#34;窗口中的图片说明&#34;&gt;窗口中的图片说明&lt;/h2&gt;

&lt;p&gt;我们分别来看下MNIST窗口中的6个Panel。&lt;/p&gt;

&lt;h3 id=&#34;training-digits&#34;&gt;Training Digits&lt;/h3&gt;

&lt;p&gt;此&lt;strong&gt;DataSet&lt;/strong&gt;中一共有50000个&lt;u&gt;训练数字&lt;/u&gt;，每次&lt;strong&gt;Iteration&lt;/strong&gt;送入100个数字作为循环，500次迭代后可以将所有数字训练一次，叫做一个&lt;strong&gt;Epoch&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;上图中白色背景的数字表示识别正确的，红色背景的部分表示识别错误的，每个数字左边下标表示应该被识别成的正确结果，数字右边的下标是识别错误的结果。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-01.jpg&#34; alt=&#34;training digits&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;test-digits&#34;&gt;Test Digits&lt;/h3&gt;

&lt;p&gt;此外也不能光有训练数字吧，MNIST数据集中还有10000个测试数字，此处你能看到每个数字对应的大约 1000 种书写形式，其中所有错误识别的数字列在顶部（有红色背景）。左边的刻度会给你一个粗略的分辨率精确度（正确识别的百分比）。如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-02.jpg&#34; alt=&#34;test digits&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到经过2000轮的训练后，已经对手写数字的识别率达到了92%。&lt;/p&gt;

&lt;h3 id=&#34;cross-entropy-loss&#34;&gt;Cross Entropy Loss&lt;/h3&gt;

&lt;p&gt;为了驱动训练，我们来定义&lt;strong&gt;损失函数&lt;/strong&gt;，即&lt;u&gt;一个展示出系统数字识别能力有多糟的值&lt;/u&gt;，并且系统会尽力将其最小化。损失函数（loss function，此处为&lt;a href=&#34;https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s1.html&#34;&gt;交叉熵&lt;/a&gt;）的选择稍后会做出解释。你会看到，随着训练的进行，训练和测试数据的损失会减少，而这个现象是好的，意味着神经网络正在学习。X 轴表示了学习过程中的迭代。如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-03.jpg&#34; alt=&#34;cross entropy loss&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;accuracy&#34;&gt;Accuracy&lt;/h3&gt;

&lt;p&gt;这个准确度只是正确识别的数字的百分比，是在训练和测试集上计算出的。如果训练顺利，它便会上升。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-04.jpg&#34; alt=&#34;accuratcy&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;weight-biases&#34;&gt;Weight &amp;amp; Biases&lt;/h3&gt;

&lt;p&gt;最后的两幅图表说明了内部变量所取的所有值的扩展，即随训练进行而变化的权重和偏置。比如偏置从 0 开始，且最终得到的值大致均匀地分布在-1.5 和 1.5 之间。如果系统不能很好地收敛，那么这些图可能有用。倘若你发现权重和偏差扩展到上百或上千，那么就可能有问题了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-05.jpg&#34; alt=&#34;weight&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-mnist-panel-06.jpg&#34; alt=&#34;biases&#34; /&gt;&lt;/p&gt;

&lt;p&gt;剩下的部分就是理论讲解了，先复制过来，期待有朝一日我能看懂吧😄&lt;/p&gt;

&lt;h3 id=&#34;理论-单层神经网络&#34;&gt;理论 : 单层神经网络&lt;/h3&gt;

&lt;p&gt;MNIST 数据集中，手写数字是 28x28 像素的灰度图像。将它们进行分类的最简单的方法就是使用 28x28=784 个像素作为单层神经网络的输入。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/5070df2db57143eabd8549e038c95735_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;神经网络中的每个「神经元」对其所有的输入进行加权求和，并添加一个被称为「偏置（bias）」的常数，然后通过一些非线性激活函数来反馈结果。&lt;/p&gt;

&lt;p&gt;为了将数字分为 10 类（0 到 9），我们设计了一个具有 10 个输出神经元的单层神经网络。对于分类问题，softmax 是一个不错的激活函数。通过取每个元素的指数，然后归一化向量（使用任意的范数（norm），比如向量的普通欧几里得距离）从而将 softmax 应用于向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/7648d231901a468a99cb9a849e8630a5_th.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;那么为什么「softmax」会被称为 softmax 呢？指数是一种骤增的函数。这将加大向量中每个元素的差异。它也会迅速地产生一个巨大的值。然后，当进行向量的标准化时，支配范数（norm）的最大的元素将会被标准化为一个接近 1 的数字，其他的元素将会被一个较大的值分割并被标准化为一个接近 0 的数字。所得到的向量清楚地显示出了哪个是其最大的值，即「max」，但是却又保留了其值的原始的相对排列顺序，因此即为「soft」。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/c85a5d9f37f045ceb8b7b0b457159782_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们现在将使用矩阵乘法将这个单层的神经元的行为总结进一个简单的公式当中。让我们直接这样做：100 个图像的「mini-batch」作为输入，产生 100 个预测（10 元素向量）作为输出。&lt;/p&gt;

&lt;p&gt;使用加权矩阵 W 的第一列权重，我们计算第一个图像所有像素的加权和。该和对应于第一神经元。使用第二列权重，我们对第二个神经元进行同样的操作，直到第 10 个神经元。然后，我们可以对剩余的 99 个图像重复操作。如果我们把一个包含 100 个图像的矩阵称为 X，那么我们的 10 个神经元在这 100 张图像上的加权和就是简单的 X.W（矩阵乘法）。&lt;/p&gt;

&lt;p&gt;每一个神经元都必须添加其偏置（一个常数）。因为我们有 10 个神经元，我们同样拥有 10 个偏置常数。我们将这个 10 个值的向量称为 b。它必须被添加到先前计算的矩阵中的每一行当中。使用一个称为 &amp;ldquo;broadcasting&amp;rdquo; 的魔法，我们将会用一个简单的加号写出它。&lt;/p&gt;

&lt;p&gt;「Broadcasting」是 Python 和 numpy（Python 的科学计算库）的一个标准技巧。它扩展了对不兼容维度的矩阵进行正常操作的方式。「Broadcasting add」意味着「如果你因为两个矩阵维度不同的原因而不能将其相加，那么你可以根据需要尝试复制一个小的矩阵使其工作。」&lt;/p&gt;

&lt;p&gt;我们最终应用 softmax 激活函数并且得到一个描述单层神经网络的公式，并将其应用于 100 张图像：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/cd007da75f714c96ade9a4e4c0187bfa_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;顺便说一下，什么是「tensor（张量）」？&lt;/p&gt;

&lt;p&gt;「张量（tensor）」像一个矩阵，但是却有着任意数量的维度。一个 1 维的张量是一个向量。一个二维的张量是一个矩阵。然后你可以有 3, 4, 5 或者更多维的张量。&lt;/p&gt;

&lt;h3 id=&#34;理论-梯度下降&#34;&gt;理论：梯度下降&lt;/h3&gt;

&lt;p&gt;现在我们的神经网络从输入图像中产生预测，我们需要知道它们可以做到什么样的程度，即在我们知道的事实和网络的预测之间到底有多大的距离。请记住，我们对于这个数据集中的所有图像都有一个真实的标签。&lt;/p&gt;

&lt;p&gt;任何一种定义的距离都可以进行这样的操作，普通欧几里得距离是可以的，但是对于分类问题，被称为「交叉熵（cross-entropy）」的距离更加有效。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/9fe5ab61209c490297769b729a471c81_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;「one-hot」编码意味着你使用一个 10 个值的向量，其中除了第 6 个值为 1 以外的所有值都是 0。这非常方便，因为这样的格式和我们神经网络预测输出的格式非常相似，同时它也作为一个 10 值的向量。&lt;/p&gt;

&lt;p&gt;「训练」一个神经网络实际上意味着使用训练图像和标签来调整权重和偏置，以便最小化交叉熵损失函数。它是这样工作的。&lt;/p&gt;

&lt;p&gt;交叉熵是一个关于权重、偏置、训练图像的像素和其已知标签的函数。&lt;/p&gt;

&lt;p&gt;如果我们相对于所有的权重和所有的偏置计算交叉熵的偏导数，我们就得到一个对于给定图像、标签和当前权重和偏置的「梯度」。请记住，我们有 7850 个权重和偏置，所以计算梯度需要大量的工作。幸运的是，TensorFlow 可以来帮我们做这项工作。&lt;/p&gt;

&lt;p&gt;梯度的数学意义在于它指向「上（up）」。因为我们想要到达一个交叉熵低的地方，那么我们就去向相反的方向。我们用一小部分的梯度更新权重和偏置并且使用下一批训练图像再次做同样的事情。我们希望的是，这可以使我们到达交叉熵最小的凹点的低部。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/b90ff98a395b46e392c21c6ebca0c7d6_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在这副图片当中，交叉熵被表示为一个具有两个权重的函数。事实上，还有更多。梯度下降算法遵循着一个最陡的坡度下降到局部最小值的路径。训练图像在每一次迭代中同样会被改变，这使得我们向着一个适用于所有图像的局部最小值收敛。&lt;/p&gt;

&lt;p&gt;「学习率（learning rate）」： 在整个梯度的长度上，你不能在每一次迭代的时候都对权重和偏置进行更新。这就会像是你穿着七里靴却试图到达一个山谷的底部。你会直接从山谷的一边到达另一边。为了到达底部，你需要一些更小的步伐，即只使用梯度的一部分，通常在 &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1000&lt;/sub&gt; 区域中。我们称这个部分为「学习率（Learning rate）」。&lt;/p&gt;

&lt;p&gt;总结一下，以下是训练过程的步骤：&lt;/p&gt;

&lt;p&gt;Training digits and labels =&amp;gt; loss function =&amp;gt; gradient (partial derivatives) =&amp;gt; steepest descent =&amp;gt; update weights and biases =&amp;gt; repeat with next mini-batch of training images and labels&lt;/p&gt;

&lt;p&gt;训练数字和标签 =&amp;gt; 损失函数 =&amp;gt; 梯度（部分偏导数）=&amp;gt; 最陡的梯度 =&amp;gt; 更新权重和偏置 =&amp;gt; 使用下一个 mini-batch 的图像和标签重复这一过程&lt;/p&gt;

&lt;p&gt;为什么使用 100 个图像和标签的 mini-batch？&lt;/p&gt;

&lt;p&gt;你当然也可以只在一个示例图像中计算你的梯度并且立即更新权重和偏置（这在科学文献中被称为「随机梯度下降（stochastic gradient descent）」）。在 100 个样本上都这样做可以得到一个更好地表示由不同样本图像施加约束的梯度并且可能更快地朝着解决方案收敛。mini-batch 的大小是可调整的参数。还有一个更加技术化的原因：使用批处理也意味着使用较大的矩阵，而这些通常更容易在 GPU 上优化。&lt;/p&gt;

&lt;p&gt;常见问题&lt;/p&gt;

&lt;p&gt;为什么交叉熵是在分类问题中合适的定义距离？&lt;/p&gt;

&lt;p&gt;　　解答链接：&lt;a href=&#34;https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/&#34;&gt;https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验-让我们来看看代码&#34;&gt;实验：让我们来看看代码&lt;/h3&gt;

&lt;p&gt;单层神经网络的代码已经写好了。请打开 mnist_1.0_softmax.py 文件并按说明进行操作。&lt;/p&gt;

&lt;p&gt;你在本节的任务是理解开始代码，以便稍后对其改进。&lt;/p&gt;

&lt;p&gt;你应该看到，在文档中的说明和启动代码只有微小的差别。它们对应于可视化的函数，并且在注释中被标记。此处可忽略。&lt;/p&gt;

&lt;p&gt;　　mnist_1.0_softmax.py：&lt;/p&gt;

&lt;p&gt;　　&lt;a href=&#34;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&#34;&gt;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/b8be8ddc1a9e41d19ddbffe3ed1ddc05_th.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;　　我们首先定义 TensorFlow 的变量和占位符。变量是你希望训练算法为你确定的所有的参数。在我们的例子中参数是权重和偏差。&lt;/p&gt;

&lt;p&gt;占位符是在训练期间填充实际数据的参数，通常是训练图像。持有训练图像的张量的形式是 [None, 28, 28, 1]，其中的参数代表：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;28, 28, 1: 图像是 28x28 每像素 x 1（灰度）。最后一个数字对于彩色图像是 3 但在这里并非是必须的。&lt;/li&gt;
&lt;li&gt;None: 这是代表图像在小批量（mini-batch）中的数量。在训练时可以得到。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　　mnist_1.0_softmax.py：&lt;/p&gt;

&lt;p&gt;　　&lt;a href=&#34;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&#34;&gt;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/71fef16bc66b4939975a236b58dac8ba_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;　　第一行是我们单层神经网络的模型。公式是我们在前面的理论部分建立的。tf.reshape 命令将我们的 28×28 的图像转化成 784 个像素的单向量。在 reshape 中的「-1」意味着「计算机，计算出来，这只有一种可能」。在实际当中，这会是图像在小批次（mini-batch）中的数量。&lt;/p&gt;

&lt;p&gt;然后，我们需要一个额外的占位符用于训练标签，这些标签与训练图像一起被提供。&lt;/p&gt;

&lt;p&gt;现在我们有模型预测和正确的标签，所以我们计算交叉熵。tf.reduce_sum 是对向量的所有元素求和。&lt;/p&gt;

&lt;p&gt;最后两行计算了正确识别数字的百分比。这是留给读者的理解练习，使用 TensorFlow API 参考。你也可以跳过它们。&lt;/p&gt;

&lt;p&gt;　　mnist_1.0_softmax.py：&lt;/p&gt;

&lt;p&gt;　　&lt;a href=&#34;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py)&#34;&gt;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;　　optimizer = tf.train.GradientDescentOptimizer(0.003)&lt;/p&gt;

&lt;p&gt;　　train_step = optimizer.minimize(cross_entropy)&lt;/p&gt;

&lt;p&gt;　　才是 TensorFlow 发挥它力量的地方。你选择一个适应器（optimiser，有许多可供选择）并且用它最小化交叉熵损失。在这一步中，TensorFlow 计算相对于所有权重和所有偏置（梯度）的损失函数的偏导数。这是一个形式衍生（ formal derivation），并非是一个耗时的数值型衍生。&lt;/p&gt;

&lt;p&gt;梯度然后被用来更新权重和偏置。学习率为 0.003。&lt;/p&gt;

&lt;p&gt;最后，是时候来运行训练循环了。到目前为止，所有的 TensorFlow 指令都在内存中准备了一个计算图，但是还未进行计算。&lt;/p&gt;

&lt;p&gt;TensorFlow 的 “延迟执行（deferred execution）” 模型：TensorFlow 是为分布式计算构建的。它必须知道你要计算的是什么、你的执行图（execution graph），然后才开始发送计算任务到各种计算机。这就是为什么它有一个延迟执行模型，你首先使用 TensorFlow 函数在内存中创造一个计算图，然后启动一个执行 Session 并且使用 Session.run 执行实际计算任务。在此时，图形无法被更改。&lt;/p&gt;

&lt;p&gt;由于这个模型，TensorFlow 接管了分布式运算的大量运筹。例如，假如你指示它在计算机 1 上运行计算的一部分 ，而在计算机 2 上运行另一部分，它可以自动进行必要的数据传输。&lt;/p&gt;

&lt;p&gt;计算需要将实际数据反馈进你在 TensorFlow 代码中定义的占位符。这是以 Python 的 dictionary 的形式给出的，其中的键是占位符的名称。&lt;/p&gt;

&lt;p&gt;　　mnist_1.0_softmax.py：&lt;/p&gt;

&lt;p&gt;　　&lt;a href=&#34;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&#34;&gt;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/a96dcaf397d342e18edb0ec24757f821.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;　　在这里执行的 train_step 是当我们要求 TensorFlow 最小化交叉熵时获得的。这是计算梯度和更新权重和偏置的步骤。&lt;/p&gt;

&lt;p&gt;最终，我们还需要一些值来显示，以便我们可以追踪我们模型的性能。&lt;/p&gt;

&lt;p&gt;在训练回路中使用该代码来计算准确度和交叉熵（例如每 10 次迭代）：&lt;/p&gt;

&lt;p&gt;　　# success ?&lt;/p&gt;

&lt;p&gt;　　a,c = sess.run([accuracy, cross_entropy], feed_dict=train_data)&lt;/p&gt;

&lt;p&gt;通过在馈送 dictionary 中提供测试而不是训练数据，可以对测试数据进行同样的计算（例如每 100 次迭代计算一次。有 10,000 个测试数字，所以会耗费 CPU 一些时间）：&lt;/p&gt;

&lt;p&gt;　　# success on test data ?&lt;/p&gt;

&lt;p&gt;　　test&lt;em&gt;data={X: mnist.test.images, Y&lt;/em&gt;: mnist.test.labels}&lt;/p&gt;

&lt;p&gt;　　a,c = sess.run([accuracy, cross_entropy], feed=test_data)&lt;/p&gt;

&lt;p&gt;TensorFlow 和 Numpy 是朋友：在准备计算图时，你只需要操纵 TensorFlow 张量和命令，比如 tf.matmul, tf.reshape 等。&lt;/p&gt;

&lt;p&gt;然而，只要执行 Session.run 命令，它的返回值就是 Numpy 张量，即 Numpy 可以使用的 numpy.ndarray 对象以及基于它的所有科学计算库。这就是使用 matplotlib（基于 Numpy 的标准 Python 绘图库）为本实验构建实时可视化的方法。&lt;/p&gt;

&lt;p&gt;这个简单的模型已经能识别 92% 的数字了。这不错，但是你现在要显著地改善它。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/e65dd7cd3b2f43ab85279a8d39de470a_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验-增加层&#34;&gt;实验:增加层&lt;/h3&gt;

&lt;p&gt;为了提高识别的准确度，我们将为神经网络增加更多的层。第二层神经元将计算前一层神经元输出的加权和，而非计算像素的加权和。这里有一个 5 层全相连的神经网络的例子：&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/1b4c2da0004c41988eb4bd83f28449dd_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们继续用 softmax 来作为最后一层的激活函数，这也是为什么在分类这个问题上它性能优异的原因。但在中间层，我们要使用最经典的激活函数：sigmoid：在这一节中你的任务是为你的模型增加一到两个中间层以提高它的性能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/6e069d408df443c49317ccd7784874b8_th.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;答案可以在 mnist_2.0_five_layers_sigmoid.py 中找到。只有当你实在想不出来的时候再使用它！为了增加一个层，你需要为中间层增加一个额外的权重矩阵和一个额外的偏置向量：&lt;/p&gt;

&lt;p&gt;　　W1 = tf.Variable(tf.truncated_normal([28*28, 200] ,stddev=0.1))&lt;/p&gt;

&lt;p&gt;　　B1 = tf.Variable(tf.zeros([200]))&lt;/p&gt;

&lt;p&gt;　　W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))&lt;/p&gt;

&lt;p&gt;　　B2 = tf.Variable(tf.zeros([10]))&lt;/p&gt;

&lt;p&gt;对，就这么做。通过 2 个中间层以及例子中 200 个和 100 个神经元，你现在应该能够把你的神经网络的准确度推高到 97% 了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/e60d8b29dad048fa8e5917c95c7a6153_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验-深度网络需要特别注意的地方&#34;&gt;实验：深度网络需要特别注意的地方&lt;/h3&gt;

&lt;p&gt;随着层数的增加，神经网络越来越难以收敛。但现在我们知道如何控制它们的行为了。这里是一些只用 1 行就可以实现的改进，当你看到准确度曲线出现如下情况的时候，这些小技巧会帮到你：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/5b31861eff624a32bca2e5c29169a7c1_th.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;修正线性单元（ReLU）激活函数&lt;/p&gt;

&lt;p&gt;在深度网络里，sigmoid 激活函数确实能带来很多问题。它把所有的值都挤到了 0 到 1 之间，而且当你重复做的时候，神经元的输出和它们的梯度都归零了。值得一提的是，出于历史原因，一些现代神经网络使用了 ReLU（修正线性单元），它大致是如下这个样子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/a06abc6c13f743c28dd4b28018854629.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;升级 1/4：用 RELU 替换你所有的 sigmoid，然后你会得到一个更快的初始收敛并且当我们继续增加层的时候也避免了一些后续问题的产生。仅仅在代码中简单地用 tf.nn.relu 来替换 tf.nn.sigmoid 就可以了。&lt;/p&gt;

&lt;p&gt;一个更好的优化器&lt;/p&gt;

&lt;p&gt;在一个特别多维的空间里，就像当前这个情况——我们有 10K 量级的权值和偏置值——「鞍点 (saddle points）」会频繁出现。这些点不是局部最小值点，但它的梯度却是零，那么梯度降的优化会卡在这里。TensorFlow 有一系列可以用的优化器，包括一些带有一定的惯性，能够安全越过鞍点的优化器。&lt;/p&gt;

&lt;p&gt;升级 2/4：现在将你的 tf.train.GradientDescentOptimiser 替换为 tf.train.AdamOptimizer。&lt;/p&gt;

&lt;p&gt;随机初始化&lt;/p&gt;

&lt;p&gt;准确性一直卡在 0.1？你把你的权值初始化成随机值了没？对于偏置值，如果用 ReLU 的话，最好的办法就是把它们都初始化成小的正值，这样神经元一开始就会工作在 ReLU 的非零区域内。&lt;/p&gt;

&lt;p&gt;　　W = tf.Variable(tf.truncated_normal([K, L] ,stddev=0.1))&lt;/p&gt;

&lt;p&gt;　　B = tf.Variable(tf.ones([L])/10)&lt;/p&gt;

&lt;p&gt;升级 3/4：现在检查是否你所有的权值和偏置值都被初始化好了。上图所示的 0.1 会作为偏置值。&lt;/p&gt;

&lt;p&gt;不定值（NaN）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/18bf142ee7894aa18c015ddba8c645ab_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如果你看到你的精确曲线陡然下滑并且调试口输出的交叉熵是 NaN，不用感到头疼，你其实是正在尝试计算 log(0)，而这肯定是个不定值（NaN）。还记得吗，交叉熵的计算涉及到对 softmax 层的输出取对数。鉴于 softmax 基本上是一个指数，它肯定不是 0，我们如果用 32 位精度的浮点运算就还好，exp(-100) 基本上可以算作是 0 了。&lt;/p&gt;

&lt;p&gt;很幸运，TensorFlow 有一个非常方便的函数可以在单步内计算 softmax 和交叉熵，它是以一种数值上较为稳定的方式实现的。如果要使用它，你需要在应用 softmax 之前将原始的权重和加上你最后一层的偏置隔离开来（在神经网络的术语里叫「logits」）。&lt;/p&gt;

&lt;p&gt;　　如果你模型的最后一行是这样的：&lt;/p&gt;

&lt;p&gt;　　Y = tf.nn.softmax(tf.matmul(Y4, W5) + B5)&lt;/p&gt;

&lt;p&gt;　　你需要把它替换成：&lt;/p&gt;

&lt;p&gt;　　Ylogits = tf.matmul(Y4, W5) + B5Y = tf.nn.softmax(Ylogits)&lt;/p&gt;

&lt;p&gt;　　并且你现在能以一种安全的方式计算交叉熵了：&lt;/p&gt;

&lt;p&gt;　　cross_entropy = tf.nn.softmax_cross_entropy_with&lt;em&gt;logits(Ylogits, Y&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;同样加上下面这行代码使得测试和训练的交叉熵能够同框显示：&lt;/p&gt;

&lt;p&gt;　　cross_entropy = tf.reduce_mean(cross_entropy)*100&lt;/p&gt;

&lt;p&gt;升级 4/4：请把 tf.nn.softmax_cross_entropy_with_logits 加到你的代码里。你也可以跳过这一步，等你真在你的输出里看到 NaN 以后再来做这步。现在，你已经准备好实现「深度」了。&lt;/p&gt;

&lt;h3 id=&#34;实验-学习速率衰退&#34;&gt;实验：学习速率衰退&lt;/h3&gt;

&lt;p&gt;通过两个、三个或者四个中间层，你现在可以将准确度提升至接近 98%，当然，你的迭代次数要达到 5000 次以上。不过你会发现你并不总是会得到这样的结果。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/a4492d6cbbb343aab07a355246c915de_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这些曲线很嘈杂，看看测试精确度吧：它在全百分比范围内跳上跳下。这意味着即使 0.003 的学习率我们还是太快了。但我们不能仅仅将学习率除以十或者永远不停地做训练。一个好的解决方案是开始很快随后将学习速率指数级衰减至比如说 0.0001。&lt;/p&gt;

&lt;p&gt;这个小改变的影响是惊人的。你会看到大部分的噪声消失了并且测试精确度持续稳定在 98% 以上。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/41294f08d52d4959bb4b01c8e925f056_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;再看看训练精确度曲线。在好多个 epoch 里都达到了 100%（一个 epoch=500 次迭代=全部训练图片训练一次）。第一次我们能很好地识别训练图片了。&lt;/p&gt;

&lt;p&gt;请把学习率衰退加到你的代码里。为了把一个不同的学习率在每次迭代时传给 AdamOptimizer，你需要定义一个新的占位符（placeholder）并在每次迭代时通过 feed_dict 赋给它一个新的参数。&lt;/p&gt;

&lt;p&gt;这里是一个指数级衰减的方程：lr = lrmin+(lrmax-lrmin)*exp(-i/2000) 答案可以在这个文件里找到：mnist_2.1_five_layers_relu_lrdecay.py。如果你被卡住了可以用它。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/260e6a3277984432adc3e945251e5f18_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;实验-dropout-过拟合&#34;&gt;实验：dropout、过拟合&lt;/h3&gt;

&lt;p&gt;你可能已经注意到在数千次迭代之后，测试和训练数据的交叉熵曲线开始不相连。学习算法只是在训练数据上做工作并相应地优化训练的交叉熵。它再也看不到测试数据了，所以这一点也不奇怪：过了一会儿它的工作不再对测试交叉熵产生任何影响，交叉熵停止了下降，有时甚至反弹回来。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/2b215ed2c1984612bdba47bf0dba1188_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;它不会立刻影响你模型对于真实世界的识别能力，但是它会使你运行的众多迭代毫无用处，而且这基本上是一个信号——告诉我们训练已经不能再为模型提供进一步改进了。这种无法连接通常会被标明「过拟合（overfitting）」，而且当你看到这个的时候，你可以尝试采用一种规范化（regularization）技术，称之为「dropout」。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/8ba81eadbd6f4383a4af3a8dc13bbe53_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在 dropout 里，在每一次训练迭代的时候，你可以从网络中随机地放弃一些神经元。你可以选择一个使神经元继续保留的概率 pkeep，通常是 50% 到 75% 之间，然后在每一次训练的迭代时，随机地把一些神经元连同它们的权重和偏置一起去掉。在一次迭代里，不同的神经元可以被一起去掉（而且你也同样需要等比例地促进剩余神经元的输出，以确保下一层的激活不会移动）。当测试你神经网络性能的时候，你再把所有的神经元都装回来 (pkeep=1)。&lt;/p&gt;

&lt;p&gt;TensorFlow 提供一个 dropout 函数可以用在一层神经网络的输出上。它随机地清零一些输出并且把剩下的提升 1/pkeep。这里是如何把它用在一个两层神经网络上的例子。&lt;/p&gt;

&lt;p&gt;　　# feed in 1 when testing, 0.75 when training&lt;/p&gt;

&lt;p&gt;　　pkeep = tf.placeholder(tf.float32)&lt;/p&gt;

&lt;p&gt;　　Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)&lt;/p&gt;

&lt;p&gt;　　Y1d = tf.nn.dropout(Y1, pkeep)&lt;/p&gt;

&lt;p&gt;　　Y = tf.nn.softmax(tf.matmul(Y1d, W2) + B2)&lt;/p&gt;

&lt;p&gt;你现在可以在网络中每个中间层以后插入 dropout。如果你没时间深入阅读的话，这是本项目里的可选步骤。&lt;/p&gt;

&lt;p&gt;该解决方案可以在 &lt;a href=&#34;https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.2_five_layers_relu_lrdecay_dropout.py&#34;&gt;mnist_2.2_five_layers_relu_lrdecay_dropout.py&lt;/a&gt;里找到。如果你被难住了，可以用它。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/59419fef6dbd401aa484dfb9983712f7_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;你会看到测试损失已经被搞回来了，已经在可控范围内了，不过至少在这个例子中噪声重新出现了（如果你知道 dropout 的工作原理的话，这一点也不奇怪）。测试的准确度依然没变，这倒是有点小失望。这个「过拟合」一定还有其它原因。在我们继续进行下一步之前，我们先扼要重述一下我们到目前为止用过的所有工具：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/cfcb109bbbe040d29e61d02d78059124_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;无论我们做什么，我们看上去都不可能很显著地解决 98% 的障碍，而且我们的损失曲线依然显示「过拟合」无法连接。什么是真正的「过拟合」？过拟合发生在该神经网络学得「不好」的时候，在这种情况下该神经网络对于训练样本做得很好，对真实场景却并不是很好。有一些像 dropout 一样的规范化技术能够迫使它学习得更好，不过过拟合还有更深层的原因。&lt;/p&gt;

&lt;p&gt;基本的过拟合发生在一个神经网络针对手头的问题有太多的自由度的时候。想象一下我们有如此多的神经元以至于所组成的网络可以存储我们所有的训练图像并依靠特征匹配来识别它们。它会在真实世界的数据里迷失。一个神经网络必须有某种程度上的约束以使它能够归纳推理它在学习中所学到的东西。&lt;/p&gt;

&lt;p&gt;如果你只有很少的训练数据，甚至一个很小的网络都能够用心学习它。一般来说，你总是需要很多数据来训练神经网络。&lt;/p&gt;

&lt;p&gt;最后，如果你已经做完了所有的步骤，包括实验了不同大小的网络以确保它的自由度已经约束好了、采用了 dropout、并且训练了大量的数据，你可能会发现你还是被卡在了当前的性能层次上再也上不去了。这说明你的神经网络在它当前的形态下已经无法从你提供的数据中抽取到更多的信息了，就像我们这个例子这样。&lt;/p&gt;

&lt;p&gt;还记得我们如何使用我们的图像吗？是所有的像素都展平到一个向量里么？这是一个很糟糕的想法。手写的数字是由一个个形状组成的，当我们把像素展平后我们会丢掉这些形状信息。不过，有一种神经网络可以利用这些形状信息：卷积网络（convolutional network）。让我们来试试。&lt;/p&gt;

&lt;h3 id=&#34;理论-卷积网络&#34;&gt;理论：卷积网络&lt;/h3&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/0eb4bf030ecc488a8d2f990c11e7272c_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在卷积网络层中，一个「神经元」仅对该图像上的一个小部分的像素求加权和。然后，它通常会添加一个偏置单元，并且将得到的加权和传递给激活函数。与全连接网络相比，其最大的区别在于卷积网络的每个神经元重复使用相同的权重，而不是每个神经元都有自己的权重。&lt;/p&gt;

&lt;p&gt;在上面的动画中，你可以看到通过连续修改图片上两个方向的权重（卷积），能够获得与图片上的像素点数量相同的输出值（尽管在边缘处需要填充（padding））。&lt;/p&gt;

&lt;p&gt;要产生一个输出值平面，我们使用了一张 4x4 大小的彩色图片作为出输入。在这个动画当中，我们需要 4x4x3=48 个权重，这还不够，为了增加更多自由度，我们还需要选取不同组的权重值重复实验。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/cda66c477aa946eeba45408d4129b274_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过向权重张量添加一个维度，能够将两组或更多组的权重重写为一组权重，这样就给出了一个卷积层的权重张量的通用实现。由于输入、输出通道的数量都是参数，我们可以开始堆叠式（stacking）和链式（chaining）的卷积层。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/10228a2a4d134aa0aa3077cb019d34a9_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;最后，我们需要提取信息。在最后一层中，我们仅仅想使用 10 个神经元来分类 0-9 十个不同的数字。传统上，这是通过「最大池化（max-pooling）」层来完成的。即使今天有许多更简单的方法能够实现这分类任务，但是，「最大池化」能够帮助我们直觉地理解卷积神经网络是怎么工作的。如果你认为在训练的过程中，我们的小块权重会发展成能够过滤基本形状（水平线、垂直线或曲线等）的过滤器（filter），那么，提取有用信息的方式就是识别输出层中哪种形状具有最大的强度。实际上，在最大池化层中，神经元的输出是在 2x2 的分组中被处理，最后仅仅保留输出最大强度的神经元。&lt;/p&gt;

&lt;p&gt;这里有一种更简单的方法：如果你是以一步两个像素移动图片上的滑块而不是以每步一个像素地移动图片上的滑块。这种方法就是有效的，今天的卷积网络仅仅使用了卷积层。&lt;/p&gt;

&lt;p&gt;让我们建立一个用于手写数字识别的卷积网络。在顶部，我们将使用 3 个卷积层；在底部，我们使用传统的 softmax 读出层，并将它们用完全连接层连接。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/d3ae69380cae4dddbfc0f3523a264f4e_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;注意，第二与第三卷积层神经元数量以 2x2 为倍数减少，这就解释了为什么它们的输出值从 28x28 减少为 14x14，然后再到 7x7。卷积层的大小变化使神经元的数量在每层下降约为：28x28x14≈3000-&amp;gt;14x14x8≈1500 → 7x7x12≈500 → 200。下一节中，我们将给出该网络的具体实现。&lt;/p&gt;

&lt;h3 id=&#34;实现-一个卷积网络&#34;&gt;实现：一个卷积网络&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/bf6ccfdee1bc440db5e14229a5799aa3_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了将我们的代码转化为卷积模型，我们需要为卷积层定义适当的权重张量，然后将该卷积层添加到模型中。我们已经理解到卷积层需要以下形式的权重张量。下面代码是用 TensorFlow 语法来对其初始化：&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/dee5ee4d2bda42298813d9819ce61a24_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;　　W = tf.Variable(tf.truncated_normal([4, 4, 3, 2], stddev=0.1))&lt;/p&gt;

&lt;p&gt;　　B = tf.Variable(tf.ones([2])/10) # 2 is the number of output channels&lt;/p&gt;

&lt;p&gt;在 TensorFlow 中，使用 tf.nn.conv2d 函数实现卷积层，该函数使用提供的权重在两个方向上扫描输入图片。这仅仅是神经元的加权和部分，你需要添加偏置单元并将加权和提供给激活函数。&lt;/p&gt;

&lt;p&gt;　　stride = 1 # output is still 28x28&lt;/p&gt;

&lt;p&gt;　　Ycnv = tf.nn.conv2d(X, W, strides=[1, stride, stride, 1], padding=&amp;lsquo;SAME&amp;rsquo;)&lt;/p&gt;

&lt;p&gt;　　Y = tf.nn.relu(Ycnv + B)&lt;/p&gt;

&lt;p&gt;不要过分在意 stride 的复杂语法，查阅文档就能获取完整的详细信息。这里的填充（padding）策略是为了复制图片的边缘的像素。所有的数字都在一个统一的背景下，所以这仅仅是扩展了背景，并且不应该添加不需要的任何样式。&lt;/p&gt;

&lt;p&gt;现在该你了。修改你的模型并将其转化为卷积模型。你可以使用上图中的值来修改它，你可以减小你的学习速率但是务必先移除 dropout。&lt;/p&gt;

&lt;p&gt;你的模型的准确率应该会超过 98%，并且最终达到约 99%。眼看目标就要实现，我们不能停止！看看测试的交叉熵曲线。在你的头脑中，此时，是否解决方案正在形成？&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/cb5d786d95ac44a5a9bc42c90d212f69_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;99-准确率的挑战&#34;&gt;99% 准确率的挑战&lt;/h3&gt;

&lt;p&gt;调整你的神经网络的一个好方法：先去实现一个限制较多的神经网络，然后给它更多的自由度并且增加 dropout，使神经网络避免过拟合。最终你将得到一个相当不错的神经网络。&lt;/p&gt;

&lt;p&gt;例如，我们在第一层卷积层中仅仅使用了 4 个 patch，如果这些权重的 patch 在训练的过程中发展成不同的识别器，你可以直观地看到这对于解决我们的问题是不够的。手写数字模式远多于 4 种基本样式。&lt;/p&gt;

&lt;p&gt;因此，让我们稍微增加 patch 的数量，将我们卷积层中 patch 的数量从 4，8，12 增加到 6，12，24，并且在全连接层上添加 dropout。它们的神经元重复使用相同的权重，在一次训练迭代中，通过冻结（限制）一些不会对它们起作用的权重，dropout 能够有效地工作。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/5792c4f41a734c44b4f1c3ee40e47742_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;加油吧，去打破 99％的限制。增加 patch 数量和通道的数量，如上图所示，在卷积层中添加 dropout。&lt;/p&gt;

&lt;p&gt;　　&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/e2e13da25b6146f2b0274a19187e18c0_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;解决方案可以在文件 mnist_3.1_convolutional_bigger_dropout.py 中找到。&lt;/p&gt;

&lt;p&gt;使用上图所示的模型，在 10000 个测试的数字中，结果仅仅错误了 72 个。你可以在 MNIST 网站上发现，数字识别准确率的世界纪录大约为 99.7%，这仅比我们用 100 行 Python/TensorFlow 代码构建的模型的准确率高 0.4%。&lt;/p&gt;

&lt;p&gt;最后，不同的 dropout 使我们能够训练更大的卷积网络。增加神经网络的额外自由度，使模型的最终准确率从 98.9% 达到 99.1%。向卷积层中增加 dropout 不仅减少了测试误差，而且使我们模型的准确率突破 99%，甚至达到了 99.3%。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170124/70c0a9eed13d48e793d06de2317a006f_th.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;恭喜&#34;&gt;恭喜&lt;/h3&gt;

&lt;p&gt;你已经建立了你的第一个神经网络，并且训练精度达到了 99%。在这个学习过程中，你所学到的技术，并不局限于 MNIST 数据集。实际上，这些技术在训练神经网络的过程中被广泛使用。作为礼物，下面提供的内容可以用来帮助你回忆已经所学的内容。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在完成了完全神经网络和卷积网络后，你应该学习循环神经网络：&lt;a href=&#34;https://www.tensorflow.org/tutorials/recurrent/。&#34;&gt;https://www.tensorflow.org/tutorials/recurrent/。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;在本教程中，你已经学习了如何在矩阵层次构建 TensorFlow 模型。Tensorflow 还有更高级的 API，称为 tf.learn：&lt;a href=&#34;https://www.tensorflow.org/tutorials/tflearn/&#34;&gt;https://www.tensorflow.org/tutorials/tflearn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;要在云上的分布式框架上训练，我们提供 Cloud ML 服务：&lt;a href=&#34;https://cloud.google.com/ml&#34;&gt;https://cloud.google.com/ml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;最后，我们希望收到你的反馈。如果你在发现了本实验中的些许错误，或者你认为有什么需要改进的地方，请告诉我们。我们通过 GitHub 处理反馈，&lt;a href=&#34;https://github.com/googlecodelabs/feedback/issues/new?title=[cloud-tensorflow-mnist]:&amp;amp;labels[]=content-platform&amp;amp;labels[]=cloud&#34;&gt;反馈链接&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;虽然看不懂，但是先混个脸熟，以后慢慢再看。本文大部分内容复制自&lt;a href=&#34;http://it.sohu.com/20170124/n479480999.shtml&#34;&gt;教程 | 没有博士学位，照样玩转TensorFlow深度学习&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes heapster监控插件安装文档</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</link>
      <pubDate>Wed, 05 Apr 2017 18:41:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20170403064.jpg&#34; alt=&#34;嗑猫薄荷的白化孟加拉虎&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;前面几篇文章中记录了我们&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;安装好了Kubernetes集群&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;配置好了flannel网络&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/&#34;&gt;安装了Kubernetes Dashboard&lt;/a&gt;，但是还没法查看Pod的监控信息，虽然kubelet默认集成了&lt;strong&gt;cAdvisor&lt;/strong&gt;（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;下载heapster的代码&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;直接现在Github上的最新代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull https://github.com/kubernetes/heapster.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前的最高版本是1.3.0。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;heapster/deploy/kube-config/influxdb&lt;/code&gt;目录下有几个&lt;code&gt;yaml&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grafana-deployment.yaml
grafana-service.yaml
heapster-deployment.yaml
heapster-service.yaml
influxdb-deployment.yaml
influxdb-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看下用了哪些镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grafana-deployment.yaml:16:        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
heapster-deployment.yaml:16:        image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1
influxdb-deployment.yaml:16:        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;下载镜像&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们下载好了这些images后，存储到私有镜像仓库里：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我已经将官方镜像克隆到了&lt;a href=&#34;www.tenxcloud.com&#34;&gt;时速云&lt;/a&gt;上，镜像地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker login index.tendcloud.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;

&lt;p&gt;参考&lt;a href=&#34;https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md&#34;&gt;Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI&lt;/a&gt;和&lt;a href=&#34;https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md&#34;&gt;Configuring Source&lt;/a&gt;，需要修改yaml文件中的几个配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的&lt;/li&gt;
&lt;li&gt;修改heapster-deployment.yaml文件中的&lt;code&gt;--source&lt;/code&gt;参数为&lt;code&gt;—source=kubernetes:http://sz-pg-oam-docker-test-001.tendcloud.com:8080?inClusterConfig=false&amp;amp;useServiceAccount=false&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;修改完配置的&lt;code&gt;heapster-deployment.yaml&lt;/code&gt;文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: heapster
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: heapster
    spec:
      containers:
      - name: heapster
        image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1
        imagePullPolicy: IfNotPresent
        command:
        - /heapster
        - --source=kubernetes:http://sz-pg-oam-docker-test-001.tendcloud.com:8080?inClusterConfig=false&amp;amp;useServiceAccount=false
        - --sink=influxdb:http://monitoring-influxdb:8086
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;在准备好镜像和修改完配置文件后就可以一键启动了，这不就是使用kbuernetes的方便之处吗？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动heaspter&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create -f deploy/kube-config/influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看状态&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get -f deploy/kube-config/influxdb/
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/monitoring-grafana   1         1         1            1           1h

NAME                     CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
svc/monitoring-grafana   10.254.250.27   &amp;lt;none&amp;gt;        80/TCP    1h

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/heapster   1         1         1            1           1h

NAME           CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
svc/heapster   10.254.244.187   &amp;lt;none&amp;gt;        80/TCP    1h

NAME                         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/monitoring-influxdb   1         1         1            1           1h

NAME                      CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/monitoring-influxdb   10.254.151.157   &amp;lt;none&amp;gt;        8086/TCP   1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看页面&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;现在再打开Dashboard页面就可以看到CPU和Memory的监控信息了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-heapster-01.jpg&#34; alt=&#34;kubernetes-heapster&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;虽然在安装了heapster插件后可以在dashboard中看到CPU和Memory的监控信息，但是这仅仅是近实时的监控，收集的metrics被保存到了InfluxDB中，还可以通过Kibana或者Grafana来展示更详细的信息和历史数据，还是有很多事情可以做的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard/Web UI安装全记录</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</link>
      <pubDate>Wed, 05 Apr 2017 14:28:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017040301.jpg&#34; alt=&#34;晒太阳的袋鼠&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;前几天&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;在CentOS7.2上安装Kubernetes1.6&lt;/a&gt;和安装好&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;flannel网络配置&lt;/a&gt;，今天我们来安装下kuberentnes的dashboard。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Dashboard&lt;/a&gt;是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟&lt;a href=&#34;https://www.docker.com/enterprise-edition&#34;&gt;Docker Enterprise Edition&lt;/a&gt;的&lt;strong&gt;Docker Datacenter&lt;/strong&gt;很像了。&lt;/p&gt;

&lt;h2 id=&#34;安装dashboard&#34;&gt;安装Dashboard&lt;/h2&gt;

&lt;p&gt;官网的安装文档&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。&#34;&gt;https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先需要一个&lt;strong&gt;kubernetes-dashboard.yaml&lt;/strong&gt;的配置文件，可以直接在&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/src/deploy/kubernetes-dashboard.yaml&#34;&gt;Github的src/deploy/kubernetes-dashboard.yaml&lt;/a&gt;下载。&lt;/p&gt;

&lt;p&gt;我们能看下这个文件的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Configuration to deploy release version of the Dashboard UI.
#
# Example usage: kubectl create -f &amp;lt;this_file&amp;gt;

kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kubernetes-dashboard
  template:
    metadata:
      labels:
        app: kubernetes-dashboard
      # Comment the following annotation if Dashboard must not be deployed on master
      annotations:
        scheduler.alpha.kubernetes.io/tolerations: |
          [
            {
              &amp;quot;key&amp;quot;: &amp;quot;dedicated&amp;quot;,
              &amp;quot;operator&amp;quot;: &amp;quot;Equal&amp;quot;,
              &amp;quot;value&amp;quot;: &amp;quot;master&amp;quot;,
              &amp;quot;effect&amp;quot;: &amp;quot;NoSchedule&amp;quot;
            }
          ]
    spec:
      containers:
      - name: kubernetes-dashboard
        image: sz-pg-oam-docker-hub-001.tendcloud.com/library/kubernetes-dashboard-amd64:v1.6.0
        imagePullPolicy: Always
        ports:
        - containerPort: 9090
          protocol: TCP
        args:
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
          - --apiserver-host=http://sz-pg-oam-docker-test-001.tendcloud.com:8080
        livenessProbe:
          httpGet:
            path: /
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 9090
  selector:
    app: kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;⚠️ 官方提供的image名为&lt;code&gt;gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0&lt;/code&gt;，需要翻墙才能访问，我自己拉下来push到我们的私有镜像仓库了。我将这个镜像push到了docker hub上，如果你无法翻墙的话，可以到下载这个镜像：&lt;code&gt;index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0&lt;/code&gt;。时速云的镜像存储，速度就是快。&lt;/p&gt;

&lt;p&gt;准备好image后就可以部署了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f kubernetes-dashboard.yaml
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
$kubectl get -f kubernetes-dashboard.yaml
NAME                          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kubernetes-dashboard   1         1         1            1           9s

NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
svc/kubernetes-dashboard   10.254.113.226   &amp;lt;nodes&amp;gt;       80:31370/TCP   8s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以访问&lt;a href=&#34;http://sz-pg-oam-docker-test-001.tendcloud.com:8080/ui了，效果如图：&#34;&gt;http://sz-pg-oam-docker-test-001.tendcloud.com:8080/ui了，效果如图：&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-dashboard-01.jpg&#34; alt=&#34;kubernetes-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;

&lt;h3 id=&#34;如果你没启动service-account身份认证&#34;&gt;如果你没启动Service Account身份认证&lt;/h3&gt;

&lt;p&gt;那就好办了，检查下你的&lt;strong&gt;kubernetes-dashboard.yaml&lt;/strong&gt;文件，看看是不是API Server地址配错了，或者查看下pod的log，我就是在log里发现，原来API Server的主机名无法解析导致服务启动失败。在DNS里添加API Server主机的DNS记录即可。&lt;/p&gt;

&lt;h3 id=&#34;如果你启动api-server的serviceaccount身份认证&#34;&gt;如果你启动API Server的ServiceAccount身份认证&lt;/h3&gt;

&lt;p&gt;启动service的时候出错。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace=kube-system logs kubernetes-dashboard-1680927228-pdv45
Using HTTP port: 9090
Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
Refer to the troubleshooting guide for more information: https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md&#34;&gt;troubleshooting.md&lt;/a&gt;文件已经说明了，这是可能是你配置API server地址或&lt;strong&gt;Service Account&lt;/strong&gt;的问题。&lt;/p&gt;

&lt;p&gt;如果是配置Service Account的问题，可以借鉴Tony Bai的&lt;a href=&#34;http://tonybai.com/2017/01/19/install-dashboard-addon-for-k8s/&#34;&gt;Kubernetes集群Dashboard插件安装&lt;/a&gt;这篇文章。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl proxy --address=&#39;0.0.0.0&#39; --accept-hosts=&#39;^*$&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {},
  &amp;quot;status&amp;quot;: &amp;quot;Failure&amp;quot;,
  &amp;quot;message&amp;quot;: &amp;quot;no endpoints available for service \&amp;quot;kubernetes-dashboard\&amp;quot;&amp;quot;,
  &amp;quot;reason&amp;quot;: &amp;quot;ServiceUnavailable&amp;quot;,
  &amp;quot;code&amp;quot;: 503
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# start a container that contains curl
$ kubectl run test --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/curl:latest -- sleep 10000
$kubectl get pod
NAME                     READY     STATUS    RESTARTS   AGE
test-2428763157-pxkps    1/1       Running   0          6s
$kubectl exec test-2428763157-pxkps ls /var/run/secrets/kubernetes.io/serviceaccount/
ls: cannot access /var/run/secrets/kubernetes.io/serviceaccount/: No such file or directory
$kubectl get secrets
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/run/secrets/kubernetes.io/serviceaccount/&lt;/code&gt;这个目录还是不存在，我们安装的Kubernetes压根就没有设置secret。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md&#34;&gt;troubleshooting.md&lt;/a&gt;上说需要用&lt;code&gt;—admission-control&lt;/code&gt;配置API Server，在配置这个之前还要了解下&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;Service Accounts&lt;/a&gt;和&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;如何管理Service Accounts&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;一年前我安装Kubernetes Dashboard（那时候好像还叫Kube-UI）的时候没有其功能还极其不完善，经过一年多的发展，已经有模有样了，如果不启用&lt;strong&gt;Service Account&lt;/strong&gt;的话，安装Dashboard还是很简单的。接下来我还要在Dashboard上安装其它Add-on，如Heapster用来监控Pod状态。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>容器技术在大数据场景下的应用——Yarn on Docker</title>
      <link>http://rootsongjc.github.io/projects/yarn-on-docker/</link>
      <pubDate>Tue, 04 Apr 2017 00:19:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/yarn-on-docker/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;作者：&lt;a href=&#34;rootsongjc.github.io/about/&#34;&gt;宋净超&lt;/a&gt; TalkingData云计算及大数据工程师&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;我已就该话题已在2016年上海Qcon上发表过演讲，&lt;a href=&#34;http://www.infoq.com/cn/presentations/yarn-on-docker-container-technology-in-big-data-scenarios&#34;&gt;点此观看&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;另外InfoQ网站上的文字版&lt;a href=&#34;http://www.infoq.com/cn/articles/YarnOnDocker-forDCCluster&#34;&gt;数据中心的Yarn on Docker集群方案&lt;/a&gt;，即本文。&lt;/p&gt;

&lt;p&gt;项目代码开源在Github上：&lt;a href=&#34;github.com/rootsongjc/magpie&#34;&gt;Magpie&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;当前数据中心存在的问题&#34;&gt;当前数据中心存在的问题&lt;/h2&gt;

&lt;p&gt;数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源  数据中心中普遍存在如下问题：&lt;/p&gt;

&lt;p&gt;1.主机资源利用率低&lt;/p&gt;

&lt;p&gt;2.部署和扩展复杂&lt;/p&gt;

&lt;p&gt;3.资源隔离无法动态调整&lt;/p&gt;

&lt;p&gt;4.无法快速响应业务&lt;/p&gt;

&lt;p&gt;为何使用Yarnon Docker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;彻底隔离队列 &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  为了合理利用Hadoopyarn的资源，队列间会互相抢占计算资源，造成重要任务阻塞&lt;/p&gt;

&lt;p&gt;•  根据部门申请的机器数量划分Yarn集群方便财务管理&lt;/p&gt;

&lt;p&gt;•  更细粒度的资源分配 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;统一的资源分配&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  每个NodeManager和容器都可以限定CPU、内存资源&lt;/p&gt;

&lt;p&gt;•  Yarn资源划分精确到CPU核数和内存大小 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;弹性伸缩性服务&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  每个容器中运行一个NodeManager，增减yarn资源只需增减容器个数&lt;/p&gt;

&lt;p&gt;•  可以指定每个NodeManager拥有的计算资源多少，按需申请资源 &lt;/p&gt;

&lt;h2 id=&#34;给我们带来什么好处&#34;&gt;给我们带来什么好处？ &lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Swarm统一集群资源调度&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; •  统一资源&lt;/p&gt;

&lt;p&gt;•  增加Docker虚拟化层，降低运维成本&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;增加Hadoop集群资源利用率&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  Fordatacenter：避免了静态资源隔离&lt;/p&gt;

&lt;p&gt;•  Forcluster：加强集群内部资源隔离&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;系统架构&#34;&gt;系统架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch.jpg&#34; alt=&#34;td_yarn_arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;  比如数据中心中运行的Hadoop集群，我们将HDFS依然运行在物理机上，即DataNode依然部署在实体机器上，将Yarn计算层运行在Docker容器中，整个系统使用二层资源调度，Spark、Flinek、MapReduce等应用运行在Yarn上。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    Swarm调度最底层的主机硬件资源，CPU和内存封装为Docker容器，容器中运行NodeManager，提供给Yarn集群，一个Swarm集群中可以运行多个Yarn集群，形成圈地式的Yarn计算集群。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch2.jpg&#34; alt=&#34;td_yarn_arch2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体流程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1.swarm node向swarm master注册主机资源并加入到swarmcluster中&lt;/p&gt;

&lt;p&gt;2.swarm master向cluster申请资源请求启动容器&lt;/p&gt;

&lt;p&gt;3.swarm根据调度策略选择在某个node上启动dockercontainer&lt;/p&gt;

&lt;p&gt;4.swarm node的docker deamon根据容器启动参数启动相应资源大小的NodeManager&lt;/p&gt;

&lt;p&gt;5.NodeManager自动向YARN的ResourceManager注册资源一个NodeManager资源添加完成。&lt;/p&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;p&gt;Swarm为数据中心做容器即主机资源调度，每个swarmnode的节点结构如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch3.jpg&#34; alt=&#34;td_yarn_arch3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个Swarmnode就是一台物理机，每台主机上可以起多个同类型的dockercontainer，每个container的资源都有限制包括CPU、内存NodeManager容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是24核64G，我们可以分给一个容器5核12G，NodeManager占用4核10G的资源提供给Yarn。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KubernetesVS Swarm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;   关于容器集群管理系统的选型，用Kubernetes还是Swarm？我们结合自己的经验和业务需求，对比如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_compare.jpg&#34; alt=&#34;td_yarn_compare&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基于以上四点，我们最终选择了Swarm，它基本满足我们的需求，掌握和开发时常较短。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;镜像制作与发布&#34;&gt;镜像制作与发布&lt;/h2&gt;

&lt;p&gt;镜像制作和发布流程如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_ci.jpg&#34; alt=&#34;td_yarn_ci&#34; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    用户从客户端提交代码到Gitlab中，需要包含Dockerfile文件，通过集成了docker插件的Jenkins的自动编译发布机制，自动build镜像后push到docker镜像仓库中，同一个项目每提交一次代码都会重新build一次镜像，生成不同的tag来标识镜像，Swarm集群使用该镜像仓库就可以直接拉取镜像。&lt;/p&gt;

&lt;p&gt;Dockerfile的编写技巧&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_dockerfile.jpg&#34; alt=&#34;td_yarn_dockerfile&#34; /&gt;&lt;/p&gt;

&lt;p&gt; Dockerfile相当于docker镜像的编译打包流程说明，其中也不乏一些技巧。&lt;/p&gt;

&lt;p&gt;     &lt;/p&gt;

&lt;p&gt;    很多应用需要配置文件，如果想为每次启动容器的时候使用不同的配置参数，可以通过传递环境变量的方式来修改配置文件，前提是需要写一个bash脚本，脚本中来处理配置文件，再将这个脚本作为entrypoint入口，每当容器启动时就会执行这个脚本从而替换配置文件中的参数，也可以通过CMD传递参数给该脚本。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;启动容器的时候通过传递环境变量的方式修改配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run -d 
--net=mynet 
-e NAMESERVICE=nameservice 
-e ACTIVE_NAMENODE_ID=namenode29 \
-e STANDBY_NAMENODE_ID=namenode63 \
-e HA_ZOOKEEPER_QUORUM=zk1:2181,zk2:2181,zk3:2181 \
-e YARN_ZK_DIR=rmstore \
-e YARN_CLUSTER_ID=yarnRM \
-e YARN_RM1_IP=rm1 \
-e YARN_RM2_IP=rm2 \
-e CPU_CORE_NUM=5
-e NODEMANAGER_MEMORY_MB=12288 \
-e YARN_JOBHISTORY_IP=jobhistory \
-e ACTIVE_NAMENODE_IP=active-namenode \
-e STANDBY_NAMENODE_IP=standby-namenode \
-e HA=yes \
docker-registry/library/hadoop-yarn:v0.1 resourcemanager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后传递resourcemanager或者nodemanager参数指定启动相应的服务。&lt;/p&gt;

&lt;p&gt;集群管理&lt;/p&gt;

&lt;p&gt;我开发的命令行工具&lt;a href=&#34;https://github.com/rootsongjc/magpie&#34;&gt;magpie&lt;/a&gt;，也可以通过其他开源可视化页面来管理集群，比如shipyard。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_shipyard.jpg&#34; alt=&#34;td_yarn_shipyard&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;自定义网络&#34;&gt;自定义网络&lt;/h2&gt;

&lt;p&gt; Docker容器跨主机互访一直是一个问题，Docker官方为了避免网络上带来的诸多麻烦，故将跨主机网络开了比较大的口子，而由用户自己去实现。我们开发并开源了Shrike这个docker网络插件，大家可以在这里下载到：&lt;a href=&#34;https://github.com/rootsongjc/docker-ipam-plugin&#34;&gt;https://github.com/rootsongjc/docker-ipam-plugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    目前Docker跨主机的网络实现方案也有很多种, 主要包括端口映射，ovs,fannel等。但是这些方案都无法满足我们的需求，端口映射服务内的内网IP会映射成外网的IP，这样会给开发带来困惑，因为他们往往在跨网络交互时是不需要内网IP的，而ovs与fannel则是在基础网络协议上又包装了一层自定义协议，这样当网络流量大时，却又无端的增加了网络负载，最后我们采取了自主研发扁平化网络插件，也就是说让所有的容器统统在大二层上互通。架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_network.jpg&#34; alt=&#34;td_yarn_network&#34; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;我们首先需要创建一个br0自定义网桥，这个网桥并不是通过系统命令手动建立的原始Linux网桥，而是通过Docker的cerate network命令来建立的自定义网桥，这样避免了一个很重要的问题就是我们可以通过设置DefaultGatewayIPv4参数来设置容器的默认路由，这个解决了原始Linux自建网桥不能解决的问题. 用Docker创建网络时我们可以通过设置subnet参数来设置子网IP范围，默认我们可以把整个网段给这个子网，后面可以用ipamdriver（地址管理插件）来进行控制。还有一个参数gateway是用来设置br0自定义网桥地址的，其实也就是你这台宿主机的地址。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker network create 
--opt=com.docker.network.bridge.enable_icc=true
--opt=com.docker.network.bridge.enable_ip_masquerade=false
--opt=com.docker.network.bridge.host_binding_ipv4=0.0.0.0
--opt=com.docker.network.bridge.name=br0
--opt=com.docker.network.driver.mtu=1500
--ipam-driver=talkingdata
--subnet=容器IP的子网范围
--gateway=br0网桥使用的IP,也就是宿主机的地址
--aux-address=DefaultGatewayIPv4=容器使用的网关地址
mynet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_ipam.jpg&#34; alt=&#34;td_yarn_ipam&#34; /&gt;&lt;/p&gt;

&lt;p&gt;IPAM驱动是专门管理Docker 容器IP的, Docker 每次启停与删除容器都会调用这个驱动提供的IP管理接口，然后IP接口会对存储IP地址的Etcd有一个增删改查的操作。此插件运行时会起一个UnixSocket, 然后会在docker/run/plugins目录下生成一个.sock文件，Dockerdaemon之后会和这个sock 文件进行沟通去调用我们之前实现好的几个接口进行IP管理，以此来达到IP管理的目的，防止IP冲突。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    通过Docker命令去创建一个自定义的网络起名为“mynet”，同时会产生一个网桥br0，之后通过更改网络配置文件（在/etc/sysconfig/network-scripts/下ifcfg-br0、ifcfg-默认网络接口名）将默认网络接口桥接到br0上，重启网络后，桥接网络就会生效。Docker默认在每次启动容器时都会将容器内的默认网卡桥接到br0上，而且宿主机的物理网卡也同样桥接到了br0上了。其实桥接的原理就好像是一台交换机，Docker 容器和宿主机物理网络接口都是服务器，通过vethpair这个网络设备像一根网线插到交换机上。至此，所有的容器网络已经在同一个网络上可以通信了，每一个Docker容器就好比是一台独立的虚拟机，拥有和宿主机同一网段的IP，可以实现跨主机访问了。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;性能瓶颈与优化&#34;&gt;性能瓶颈与优化&lt;/h2&gt;

&lt;p&gt;    大家可能会担心自定义网络的性能问题，为此我们用iperf进行了网络性能测试。我们对比了不同主机容器间的网速，同一主机上的不同容器和不同主机间的网速，结果如下表：&lt;/p&gt;

&lt;p&gt; &lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_iperf.jpg&#34; alt=&#34;td_yarn_iperf&#34; /&gt;&lt;/p&gt;

&lt;p&gt; 从表中我们可以看到，在这一组测试中，容器间的网速与容器是在想通主机还是在不同主机上的差别不大，说明我们的网络插件性能还是很优异的。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;hadoop配置优化&#34;&gt;Hadoop配置优化 &lt;/h2&gt;

&lt;p&gt;    因为使用docker将原来一台机器一个nodemanager给细化为了多个，会造成nodemanager个数的成倍增加，因此hadoop的一些配置需要相应优化。&lt;/p&gt;

&lt;p&gt;•  yarn.nodemanager.localizer.fetch.thread-count 随着容器数量增加，需要相应调整该参数&lt;/p&gt;

&lt;p&gt;•  yarn.resourcemanager.amliveliness-monitor.interval-ms默认1秒，改为10秒，否则时间太短可能导致有些节点无法注册&lt;/p&gt;

&lt;p&gt;•  yarn.resourcemanager.resource-tracker.client.thread-count默认50，改为100，随着容器数量增加，需要相应调整该参数&lt;/p&gt;

&lt;p&gt;•  yarn.nodemanager.pmem-check-enabled默认true，改为false，不检查任务正在使用的物理内存量&lt;/p&gt;

&lt;p&gt;•  容器中hadoop ulimit值修改，默认4096，改成655350&lt;/p&gt;

&lt;p&gt;集群监控&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;如果使用shipyard管理集群会有一个单独的监控页面，可以看到一定时间段内的CPU、内存、IO、网络使用状况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_monitor.jpg&#34; alt=&#34;td_yarn_monitor&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;关于未来&#34;&gt;关于未来&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_os.jpg&#34; alt=&#34;td_yarn_os&#34; /&gt; &lt;/p&gt;

&lt;p&gt;    我们未来规划做的是DC／OS，基于Docker的应用自动打包编译分发系统，让开发人员可以很便捷的申请资源，上下线服务，管理应用。要达到这个目标还有很多事情要做：&lt;/p&gt;

&lt;p&gt;•  Service Control Panel：统一的根据服务来管理的web页面&lt;/p&gt;

&lt;p&gt;•  Loadbalance：容器根据机器负载情况自动迁移&lt;/p&gt;

&lt;p&gt;•  Scheduler：swarm调度策略优化&lt;/p&gt;

&lt;p&gt;•  服务配置文件：提供镜像启动参数的配置文件，所有启动参数可通过文件配置&lt;/p&gt;

&lt;p&gt;•  监控：服务级别的监控&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;这篇文章写好的时候是2016年10月，距离现在我添加&lt;strong&gt;前言&lt;/strong&gt;和&lt;strong&gt;后记&lt;/strong&gt;的已经快半年时间了，这段时间内业界也发生了很多变化，比如docker推出CE和SE版本，Google的kubernetes发布了1.6版本，人工智能依然大热，在可预见的未来，可以说&lt;u&gt;Kubernetes一定会远远超越Docker成为容器编排领域的王者&lt;/u&gt;，这是毋庸置疑的，对于docker 17.03-CE我也研究过了一段时间，其disgusting的plugin让我对于docker的编排已经失去信心。&lt;/p&gt;

&lt;p&gt;其实容器在大数据场景下的应用并不是很多，毕竟Hadoop那套笨重的东西放在容器下运行，上生产环境? Are you kidding me?如果说做原型验证、研发测试那还可以。这样就大大限制了容器技术在大数据场景下的应用场景。使用容器的编排调度来实现大数据集群的资源优化有点舍本逐末，&lt;u&gt;如果真的要优化集群资源利用率的话，应该让不同的应用混跑，而不应该让集群之间资源隔离，比如Web应用跟大数据应用混布。&lt;/u&gt;目前的这种&lt;strong&gt;Yarn on Docker&lt;/strong&gt;方案实质上是将原来的整体Hadoop Yarn集群划分成多个不同的Yarn，将存储和计算分离了。其实这跟&lt;strong&gt;Nutanix&lt;/strong&gt;的超融合架构有点像，Nutanix是由前Google的工程师创立的，解决虚拟化计算环境下的存储问题，也是将存储和计算分离，共享存储，计算根据需要调度。事实上Yahoo已经有解决Hadoop集群的资源细粒度分配和调度问题的方案，这应该是从Yarn的scheduler层来处理。&lt;/p&gt;

&lt;p&gt;Swarm已死，Swarmkit将继续发展，Docker的Swarm Mode还会在艰难中前行，目前看到的趋势仍然是模仿Kubernentes为主，没有自己鲜明的特色（除了部署管理方便意外，谁让它就集成在了docker里呢，就像当年windows集成IE打败Netscape，不过这不会再此上演了），Kubernentes又是一个通用的资源调度框架，它的最小资源划分是&lt;strong&gt;Pod&lt;/strong&gt;而不是docker，它还可以运行rkt、containerd。&lt;/p&gt;

&lt;p&gt;上周起我开始将注意力转移到kubernentes，以后请关注我的&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;Kuberentes实践&lt;/a&gt;相关文章。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>两款图片处理工具——Google Guetzli和基于AI的Deep Photo Style Transfer</title>
      <link>http://rootsongjc.github.io/talks/picture-process/</link>
      <pubDate>Sun, 02 Apr 2017 20:27:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/picture-process/</guid>
      <description>

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;如果你看过美剧「硅谷」会记得剧中主角们所在的创业公司&lt;a href=&#34;www.piedpiper.com&#34;&gt;PiedPipper&lt;/a&gt;，他们就是靠自己发明的视频压缩算法来跟大公司Hooli竞争的，这部剧现在已经发展到第4季，在&lt;a href=&#34;http://v.qq.com/detail/d/dr2zn76oez8tyt4.html?ptag=baidu.aladdin.tv&#34;&gt;腾讯视频&lt;/a&gt;上可以免费观看。&lt;/p&gt;

&lt;p&gt;最近关注了两个&lt;strong&gt;图像处理&lt;/strong&gt;的Open Source Projects。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/google/guetzli&#34;&gt;Google Guetzli&lt;/a&gt; 图像压缩工具&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/luanfujun/deep-photo-styletransfer&#34;&gt;Luan Fujun&amp;rsquo;s Deep Photo Style Transfer&lt;/a&gt; 图像style转换工具&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外对于图像处理还处于Photoshop、Lightroom这种摄影后期和图像处理命令行工具&lt;a href=&#34;https://www.imagemagick.org/script/index.php&#34;&gt;ImageMagick&lt;/a&gt;的我来说，图像压缩，智能图像风格转换实乃上乘武功，不是我等凡夫俗子驾驭的了，但是乘兴而来，总不能败兴而归吧，下面我们来一探究竟。&lt;/p&gt;

&lt;h2 id=&#34;google-guetzli&#34;&gt;Google Guetzli&lt;/h2&gt;

&lt;p&gt;聊聊架构微信公众号上有一篇介绍&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;amp;mid=2659599074&amp;amp;idx=1&amp;amp;sn=a26ae2a8becdc1f2cfbddf44d8ca1495&amp;amp;chksm=8be997f0bc9e1ee6e33f3e33c73d11884ad66085c0aedc9dd5e482063482887d0733d8e7d187#rd&#34;&gt;Google开源新算法，可将JPEG文件缩小35%&lt;/a&gt;文章。&lt;/p&gt;

&lt;p&gt;我在Mac上试用了一下，安装很简单，只要一条命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shlell&#34;&gt;brew install guetzli
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是当我拿一张&lt;code&gt;22M&lt;/code&gt;大小的照片使用guetzli压缩的时候，我是绝望的，先后三次kill掉了进程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;因为实在是太慢了&lt;/strong&gt;，也能是我软件对内存和CPU的利用率不高，效果你们自己看看。&lt;/p&gt;

&lt;p&gt;原图是这个样子的，拍摄地点在景山上的，俯瞰紫禁城的绝佳位置。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5430.JPG&#34; alt=&#34;原图&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;guetzli --quality 84 --verbose 20160403052.jpg output.jpg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么quality要设置成84呢？因为只能设置为84+的quality，如果要设置的更低的话需要自己修改代码。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5429.JPG&#34; alt=&#34;process&#34; /&gt;&lt;/p&gt;

&lt;p&gt;耗时了一个小时，后台进程信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5428.JPG&#34; alt=&#34;后台进程&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这个是使用&lt;strong&gt;Squash&lt;/strong&gt;压缩后的大小效果，压缩每张照片差不多只要3秒钟。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Squash的logo就是个正在被剥皮的🍊，这是&lt;a href=&#34;http://xclient.info/s/squash.html&#34;&gt;下载地址&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;压缩比分别为&lt;code&gt;70%&lt;/code&gt;和&lt;code&gt;30%&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5434.JPG&#34; alt=&#34;Img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;压缩比70%后的细节放大图&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5432.JPG&#34; alt=&#34;70&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;压缩比30%的细节放大图&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/guetzli/IMG_5433.JPG&#34; alt=&#34;30&#34; /&gt;&lt;/p&gt;

&lt;p&gt;你看出什么区别了吗？反正我是没有。&lt;/p&gt;

&lt;p&gt;下面再来看看耗时一个小时，千呼万唤始出来的guetzli压缩后的效果和使用squash压缩比为30%的效果对比。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/guetzli/FullSizeRender.jpg&#34; alt=&#34;对比&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左面是使用guetzli压缩后（4.1M），右面使用的squash压缩后（3.1M）的照片。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;似乎还是没有什么区别啊？你看出来了吗？&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;guetzli总结&#34;&gt;Guetzli总结&lt;/h3&gt;

&lt;p&gt;可能是我使用Guetzli的方式不对，但是命令行里确实没有设置CPU和内存资源的选项啊，为啥压缩照片会这么慢呢？效果也并不出彩，不改代码的话照片质量只能设置成84以上，但是这个是&lt;strong&gt;Open Source&lt;/strong&gt;的，使用的C++开发，可以研究下它的图像压缩算法。&lt;/p&gt;

&lt;h1 id=&#34;deep-photo-style-transfer&#34;&gt;Deep Photo Style Transfer&lt;/h1&gt;

&lt;p&gt;来自康奈尔大学的Luan Fujun开源的图像sytle转换工具，看了&lt;a href=&#34;https://github.com/luanfujun/deep-photo-styletransfer&#34;&gt;README&lt;/a&gt;的介绍，上面有很多图像风格转换的例子，真的很惊艳，市面上好像还没有这种能够在给定任意一张照片的情况下，自动将另一张照片转换成该照片的style。&lt;/p&gt;

&lt;p&gt;这个工具使用Matlab和Lua开发，基于&lt;a href=&#34;https://github.com/torch/torch7&#34;&gt;Torch&lt;/a&gt;运行的时候需要&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;，&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cudnn&lt;/a&gt;，&lt;a href=&#34;https://www.mathworks.com/&#34;&gt;Matlab&lt;/a&gt;，环境实在太复杂，就没折腾，启动有人发布&lt;a href=&#34;https://github.com/luanfujun/deep-photo-styletransfer/issues/29&#34;&gt;Docker镜像&lt;/a&gt;，已经有人提了issue。&lt;/p&gt;

&lt;p&gt;如果它能够被商用，绝对是继&lt;strong&gt;Prisma&lt;/strong&gt;后又一人工智能照片处理应用利器。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;是不是有了照片风格转换这个东西就不需要做照片后期了？只要选几张自己喜欢的风格照片，再鼠标点几下就可以完成照片处理了？摄影师要失业了？非也！照片风格东西本来就是很主观性的，每个人都有自己喜欢的风格，照相机发明后就有人说画家要失业了，其实不然，画画依然是创造性地劳动，只能说很多写实风格的画家要失业了。Deep Photo Style Transfer也许会成为Lightroom或者手机上一款app的功能，是一个不错的工具。也许还会成为像Prisma一样的现象级产品，who knows?🤷‍♂️&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014100402.jpg&#34; alt=&#34;西安鼓楼&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：西安鼓楼 Oct 4,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;书接上文&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;在CentOS中安装Kubernetes详细指南&lt;/a&gt;，这是一个系列文章，作为学习Kubernetes的心路历程吧。&lt;/p&gt;

&lt;p&gt;本文主要讲解&lt;strong&gt;Kubernetes的网络配置&lt;/strong&gt;，👆文中有一个安装&lt;strong&gt;Flannel&lt;/strong&gt;的步骤，但是安装好后并没有相应的配置说明。&lt;/p&gt;

&lt;h2 id=&#34;配置flannel&#34;&gt;配置flannel&lt;/h2&gt;

&lt;p&gt;我们直接使用的yum安装的flannle，安装好后会生成&lt;code&gt;/usr/lib/systemd/system/flanneld.service&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到flannel环境变量配置文件在&lt;code&gt;/etc/sysconfig/flanneld&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;etcd的地址&lt;code&gt;FLANNEL_ETCD_ENDPOINT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。&lt;code&gt;FLANNEL_ETCD_PREFIX&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;在etcd中创建网络配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;执行下面的命令为docker分配IP地址段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;etcdctl mkdir /kube-centos/network
etcdctl mk /kube-centos/network/config &amp;quot;{ \&amp;quot;Network\&amp;quot;: \&amp;quot;172.30.0.0/16\&amp;quot;, \&amp;quot;SubnetLen\&amp;quot;: 24, \&amp;quot;Backend\&amp;quot;: { \&amp;quot;Type\&amp;quot;: \&amp;quot;vxlan\&amp;quot; } }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Flannel的&lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/running.md&#34;&gt;文档&lt;/a&gt;中有写&lt;strong&gt;Docker Integration&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;Docker daemon accepts &lt;code&gt;--bip&lt;/code&gt; argument to configure the subnet of the docker0 bridge. It also accepts &lt;code&gt;--mtu&lt;/code&gt; to set the MTU for docker0 and veth devices that it will be creating. Since flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Systemd users can use &lt;code&gt;EnvironmentFile&lt;/code&gt; directive in the .service file to pull in &lt;code&gt;/run/flannel/subnet.env&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;下载flannel github release中的tar包，解压后会获得一个&lt;strong&gt;mk-docker-opts.sh&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;这个文件是用来&lt;code&gt;Generate Docker daemon options based on flannel env file&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;执行&lt;code&gt;./mk-docker-opts.sh -i&lt;/code&gt;将会生成如下两个文件环境变量文件。&lt;/p&gt;

&lt;p&gt;/run/flannel/subnet.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.46.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/run/docker_opts.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_OPT_BIP=&amp;quot;--bip=172.30.46.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$etcdctl ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.14.0-24
/kube-centos/network/subnets/172.30.38.0-24
/kube-centos/network/subnets/172.30.46.0-24
$etcdctl get /kube-centos/network/config
{ &amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; } }
$etcdctl get /kube-centos/network/subnets/172.30.14.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.114&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;56:27:7d:1c:08:22&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.38.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.115&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;12:82:83:59:cf:b8&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.46.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.113&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;e6:b2:fd:f6:66:96&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;设置docker0网桥的IP地址&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;source /run/flannel/subnet.env
ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;6: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:da:bf:83:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.1/24 brd 172.30.38.255 scope global docker0
       valid_lft forever preferred_lft forever
7: flannel.1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN 
    link/ether 9a:29:46:61:03:44 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以重启docker了。&lt;/p&gt;

&lt;p&gt;重启了docker后还要重启kubelet，这时又遇到问题，kubelet启动失败。报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mar 31 16:44:41 sz-pg-oam-docker-test-002.tendcloud.com kubelet[81047]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &amp;quot;cgroupfs&amp;quot; is different from docker cgroup driver: &amp;quot;systemd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是kubelet与docker的&lt;strong&gt;cgroup driver&lt;/strong&gt;不一致导致的，kubelet启动的时候有个&lt;code&gt;—cgroup-driver&lt;/code&gt;参数可以指定为&amp;rdquo;cgroupfs&amp;rdquo;或者“systemd”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--cgroup-driver string                                    Driver that the kubelet uses to manipulate cgroups on the host.  Possible values: &#39;cgroupfs&#39;, &#39;systemd&#39; (default &amp;quot;cgroupfs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动flannel&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;systemctl daemon-reload
systemctl start flanneld
systemctl status flanneld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新登录这三台主机，可以看到每台主机都多了一个IP。&lt;/p&gt;

&lt;p&gt;参考Kubernetes官方文档的&lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/&#34;&gt;Exposing an External IP Address to Access an Application in a Cluster&lt;/a&gt;，官方使用的Hello World测试，我们启动Nginx服务测试。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;#启动nginx的pod
kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/nginx:1.9  --port=80
#创建名为example-service的服务
kubectl expose deployment nginx --type=NodePort --name=example-service
#查看状态
kubectl get deployments nginx
kubectl describe deployments nginx
kubectl get replicasets
kubectl describe replicasets
kubectl describe svc example-service
###################################################
Name:			example-service
Namespace:		default
Labels:			run=load-balancer-example
Annotations:		&amp;lt;none&amp;gt;
Selector:		run=load-balancer-example
Type:			NodePort
IP:			10.254.180.209
Port:			&amp;lt;unset&amp;gt;	80/TCP
NodePort:		&amp;lt;unset&amp;gt;	32663/TCP
Endpoints:		172.30.14.2:80,172.30.46.2:80
Session Affinity:	None
Events:			&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们上面启动的serivce的type是&lt;strong&gt;NodePort&lt;/strong&gt;，Kubernetes的service支持三种类型的service，参考&lt;a href=&#34;http://www.cnblogs.com/xuxinkun/p/5331728.html&#34;&gt;Kubernetes Serivce分析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;现在访问三台物理机的IP:80端口就可以看到nginx的页面了。&lt;/p&gt;

&lt;p&gt;稍等一会在访问ClusterIP + Port也可以访问到nginx。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl 10.254.180.209:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;虚拟地址&#34;&gt;虚拟地址&lt;/h2&gt;

&lt;p&gt;Kubernetes中的Service了使用了虚拟地址；该地址无法ping通过，但可以访问其端口。通过下面的命令可以看到，该虚拟地址是若干条iptables的规则。到10.254.124.145:8080端口的请求会被重定向到172.30.38.2或172.30.46.2的8080端口。这些规则是由kube-proxy生成；如果需要某台机器可以访问Service，则需要在该主机启动kube-proxy。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看service的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables-save|grep example-service
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SEP-NCPBOLUH5XTTHG3E -s 172.30.46.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-NCPBOLUH5XTTHG3E -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.46.2:80
-A KUBE-SEP-ONEKQBIWICF7RAR3 -s 172.30.14.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-ONEKQBIWICF7RAR3 -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.14.2:80
-A KUBE-SERVICES -d 10.254.180.209/32 -p tcp -m comment --comment &amp;quot;default/example-service: cluster IP&amp;quot; -m tcp --dport 80 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ONEKQBIWICF7RAR3
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-SEP-NCPBOLUH5XTTHG3E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看clusterIP的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables -t nat -nL|grep 10.254
KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0/0            10.254.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到在PREROUTING环节，k8s设置了一个target: KUBE-SERVICES。而KUBE-SERVICES下面又设置了许多target，一旦destination和dstport匹配，就会沿着chain进行处理。&lt;/p&gt;

&lt;p&gt;比如：当我们在pod网络curl 10.254.198.44 80时，匹配到下面的KUBE-SVC-BR4KARPIGKMRMN3E target：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考&lt;a href=&#34;http://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/&#34;&gt;理解Kubernetes网络之Flannel网络&lt;/a&gt;，Tony Bai的文章中有对flannel的详细介绍。&lt;/p&gt;

&lt;h2 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h2&gt;

&lt;p&gt;在设置网络的过程中遇到了很多问题，记录如下。&lt;/p&gt;

&lt;h3 id=&#34;问题一&#34;&gt;问题一&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kube-proxy开放的&lt;strong&gt;NodePort&lt;/strong&gt;端口无法访问。即无法使用NodeIP加NodePort的方式访问service，而且本地telnet也不通，但是端口确确实实在那。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题状态&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;已解决&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其实这不是问题，是因为从上面的操作记录中我们可以看到，&lt;strong&gt;在启动Nginx的Pod&lt;/strong&gt;时，指定port为80即可。以ClusterIP + Port的方式访问serivce需要等一段时间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;反思&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个问题困扰了我们差不多两天时间，出现这个问题的根源还是因为&lt;u&gt;思想观念没有从运行docker的命令中解放出来&lt;/u&gt;,还把&lt;code&gt;kubelet run —port&lt;/code&gt;当成是docker run中的端口映射，这种想法是大错特错的，该端口是image中的应用实际暴露的端口，如nginx的80端口。😔&lt;/p&gt;

&lt;h3 id=&#34;问题二&#34;&gt;问题二&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在没有删除service和deploy的情况下就重启kubelet的时候，会遇到kubelet启动失败的情况。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;出错信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apr 01 14:24:08 sz-pg-oam-docker-test-001.tendcloud.com kubelet[103932]: I0401 14:24:08.359839  103932 kubelet.go:1752] skipping pod synchronization - [Failed to start ContainerManager failed to initialise top level QOS containers: failed to create top level Burstable QOS cgroup : Unit kubepods-burstable.slice already exists.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://www.osbaike.net/article-show-id-229028.html&#34;&gt;Kubernetes Resource QoS机制解读&lt;/a&gt;，这篇文章详细介绍了QoS的机制。&lt;/p&gt;

&lt;p&gt;Kubernetes根据Pod中Containers Resource的&lt;code&gt;request&lt;/code&gt;和&lt;code&gt;limit&lt;/code&gt;的值来定义Pod的QoS Class。&lt;/p&gt;

&lt;p&gt;对于每一种Resource都可以将容器分为3中QoS Classes: Guaranteed, Burstable, and Best-Effort，它们的QoS级别依次递减。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guaranteed&lt;/strong&gt;：如果Pod中所有Container的所有Resource的&lt;code&gt;limit&lt;/code&gt;和&lt;code&gt;request&lt;/code&gt;都相等且不为0，则这个Pod的QoS Class就是Guaranteed。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Burstable&lt;/strong&gt;：除了符合Guaranteed和Best-Effort的场景，其他场景的Pod QoS Class都属于Burstable。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-Effort&lt;/strong&gt;：如果Pod中所有容器的所有Resource的request和limit都没有赋值，则这个Pod的QoS Class就是Best-Effort。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个暂时还没找到根本的解决办法，参考Github上的&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43856&#34;&gt;Failed to start ContainerManager failed to initialize top level QOS containers #43856&lt;/a&gt;，重启主机后确实正常了，不过这只是临时解决方法。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;其实昨天就已经安装完毕了，是我们使用的姿势不对，白白耽误这么长时间，身边差个老司机啊，滴～学生卡。&lt;/p&gt;

&lt;p&gt;感谢&lt;a href=&#34;tonybai.com&#34;&gt;Tony Bai&lt;/a&gt;、&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;的大力支持。&lt;/p&gt;

&lt;p&gt;Apr 1,2017 愚人节，东直门&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第三章TensorFlow入门</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</link>
      <pubDate>Thu, 30 Mar 2017 21:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2015052401.jpg&#34; alt=&#34;扬州东关&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：扬州东关 May 24,2015）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;这是我阅读&lt;a href=&#34;caicloud.io&#34;&gt;才云科技&lt;/a&gt;郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在&lt;a href=&#34;http://rootsongjc.github.io/tags/tensorflow-practice-reading-notes/&#34;&gt;这里&lt;/a&gt;。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;P.S 本书的&lt;strong&gt;官方读者交流微信群（作者也在群里）&lt;/strong&gt;已经超过100人，您可以先加我微信后我拉您进去，我的二维码在&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;这里&lt;/a&gt;，或者直接搜索我的微信号jimmysong。&lt;/p&gt;

&lt;p&gt;这一章从三个角度带大家入门。&lt;/p&gt;

&lt;p&gt;分别是TensorFlow的&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算模型&lt;/li&gt;
&lt;li&gt;数据模型&lt;/li&gt;
&lt;li&gt;运行模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-1-tensorflow的计算模型-图计算&#34;&gt;3.1 TensorFlow的计算模型——图计算&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;计算图&lt;/strong&gt;是TensorFlow中的一个最基本的概念，&lt;u&gt;TensorFlow中的所有计算都会转化成计算图上的节点&lt;/u&gt;。&lt;/p&gt;

&lt;p&gt;其实TensorFlow的名字已经暗示了它的实现方式了，&lt;strong&gt;Tensor&lt;/strong&gt;表示的是数据结构——张量，&lt;strong&gt;Flow&lt;/strong&gt;表示数据流——Tensor通过数据流相互转化。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;常用的方法&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在python中导入tensorflow：import tensorflow as tf&lt;/li&gt;
&lt;li&gt;获取当前默认的计算图：tf.get_default_graph()&lt;/li&gt;
&lt;li&gt;生成新的计算图：tf.Graph()&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;书中这里都有例子讲解，可以从Github中&lt;a href=&#34;https://github.com/caicloud/tensorflow-tutorial&#34;&gt;下载代码&lt;/a&gt;，或者如果你使用才云提供的docker镜像的方式安装的话，在jupyter中可以看到各个章节的代码。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义两个不同的图&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

g1 = tf.Graph()
with g1.as_default():
    v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.zeros_initializer) # 设置初始值为0

g2 = tf.Graph()
with g2.as_default():
    v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.ones_initializer())  # 设置初始值为1
    
with tf.Session(graph = g1) as sess:
    tf.global_variables_initializer().run()
    with tf.variable_scope(&amp;quot;&amp;quot;, reuse=True):
        print(sess.run(tf.get_variable(&amp;quot;v&amp;quot;)))

with tf.Session(graph = g2) as sess:
    tf.global_variables_initializer().run()
    with tf.variable_scope(&amp;quot;&amp;quot;, reuse=True):
        print(sess.run(tf.get_variable(&amp;quot;v&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们看到这里面用了&lt;a href=&#34;https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/&#34;&gt;python中的with语法&lt;/a&gt;，不了解的可以到前面那个链接看看。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;TensorFlow中维护的集合列表&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;集合名称&lt;/th&gt;
&lt;th&gt;集合内容&lt;/th&gt;
&lt;th&gt;使用场景&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;tf.GraphKeys.VARIABLES&lt;/td&gt;
&lt;td&gt;所有变量&lt;/td&gt;
&lt;td&gt;持久化TensorFlow模型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;tf.GraphKeys.TRAINABLE_VARIABLES&lt;/td&gt;
&lt;td&gt;可学习的变量（一般指神经网络中的参数）&lt;/td&gt;
&lt;td&gt;模型训练、生活从呢个模型可视化内容&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;tf.GraphKeys.SUMMARIES&lt;/td&gt;
&lt;td&gt;与日志有关的张量&lt;/td&gt;
&lt;td&gt;TensorFlow计算可视化&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;tf.GraphKeys.QUEUE_RUNNERS&lt;/td&gt;
&lt;td&gt;处理输入的QueueRunner&lt;/td&gt;
&lt;td&gt;输入处理&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;tf.GraphKeys.MOVING_AVERAGE_VARIABLES&lt;/td&gt;
&lt;td&gt;所有计算了滑动平均值的变量&lt;/td&gt;
&lt;td&gt;计算变量的滑动平均值&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;所谓的滑动平均值即移动平均值，熟悉股票的应该都知道均线的概念吧，5日均线，20日均线，30日均线啥的，一般称作MA(Moving Average)。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-2-tensorflow数据模型-张量&#34;&gt;3.2 TensorFlow数据模型——张量&lt;/h2&gt;

&lt;p&gt;张量（Tensor）是TensorFlow中所有数据的表现形式。我们可以简单的将Tensor理解为&lt;strong&gt;多维数组&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0阶的话就是一个标量（Scalar），可以是一个数也可以是一个字符串&lt;/li&gt;
&lt;li&gt;一阶的话是向量（Vector）&lt;/li&gt;
&lt;li&gt;n阶的话就是n维数组&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tensor在TensorFlow中并不是直接采用数组的形式，而是&lt;strong&gt;对TF中计算结果的引用&lt;/strong&gt;，保存的是如何得到这些数字的过程。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;举个例子&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import tensorflow as tf
a = tf.constant([1.0, 2.0], name=&amp;quot;a&amp;quot;)
b = tf.constant([2.0, 3.0], name=&amp;quot;b&amp;quot;)
result = tf.add(a,b,name=&amp;quot;add&amp;quot;)
print result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出结果：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Tensor(&amp;quot;add:0&amp;quot;, shape=(2,), dtype=float32)
[ 3.  5.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个例子只是简单的做了个加法，下面结合上面的例子来讲解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensor的3个属性&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;名字（Name）&lt;/strong&gt;：Tensor的唯一标识符，如例子中的a, b, result，这是我们手动指定的，实际上Tensor是与计算图上的每个节点一一对应的，tensor的命名可以通过&lt;code&gt;node:src_output&lt;/code&gt;的形式给出，如例子输出中的计算结果名字为&lt;strong&gt;add:0&lt;/strong&gt;，0表示的是计算节点&lt;strong&gt;add&lt;/strong&gt;输出的第一个结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;维度（Shape）&lt;/strong&gt;：如上面例子结果输出中的&lt;strong&gt;shape=(2,)&lt;/strong&gt;，表示这个张量是一维数组，数组的长度是2。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类型（Type）&lt;/strong&gt;：所有参与运算的张量的类型必须是相同的，比如不能float和int之间运算。TensorFlow会自动检查张量的类型，可以通过&lt;strong&gt;dtype=df.float32&lt;/strong&gt;这样的声明来指定类型，如果不指定的话，TF会根据值确定默认类型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Tensor的用途&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;作为对计算中间结果的引用，比如上面例子中的a和b，在复杂计算中使用tensor能够增加代码的可阅读性。&lt;/li&gt;
&lt;li&gt;用来获取计算结果。张量本身没有存储具体的数字，但是可以使用&lt;strong&gt;会话&lt;/strong&gt;的tf.Session().run(result)来获取计算结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-3-tensorflow的运行模型-会话&#34;&gt;3.3 TensorFlow的运行模型——会话&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;会话（Session）拥有和管理TensorFlow中所有的运算，可以执行定义好的运算。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建和关闭会话&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 创建一个会话。
sess = tf.Session()

# 使用会话得到之前计算的结果。
print(sess.run(result))

# 关闭会话使得本次运行中使用到的资源可以被释放。
sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;建议使用Python的&lt;strong&gt;上下文管理器&lt;/strong&gt;，即通过&lt;strong&gt;with&lt;/strong&gt;语法来创建会话，能够避免资源泄漏。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Session() as sess:
    print(sess.run(result))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在交互式环境下（比如Jupyter）直接使用&lt;strong&gt;tf.InteractiveSession&lt;/strong&gt;函数创建&lt;strong&gt;交互式会话&lt;/strong&gt;，将产生的会话直接注册为默认会话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sess = tf.InteractiveSession ()
print(result.eval())
sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不论使用哪种会话都可以通过&lt;strong&gt;ConfigProto&lt;/strong&gt;来配置&lt;code&gt;并行线程数&lt;/code&gt;、&lt;code&gt;GPU分配策略&lt;/code&gt;、&lt;code&gt;运算超时时间&lt;/code&gt;等。&lt;/p&gt;

&lt;p&gt;常用的两个配置有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;allow_soft_placement&lt;/strong&gt;：布尔值，默认false，以下任何一个条件满足时则为true，允许将GPU上的运算放到CPU上进行。

&lt;ul&gt;
&lt;li&gt;运算无法在GPU上运行&lt;/li&gt;
&lt;li&gt;GPU资源不足（没有GPU或者指定的GPU个数不够）&lt;/li&gt;
&lt;li&gt;运算输入包括对CPU计算结果的引用&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;log_device_placement&lt;/strong&gt;：布尔值，默认值是？如果为true的话，日志中将会记录每个节点被安排在哪个设备上，这方便调试。生产环境中一般设置为false，能够减少日志量。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-4-tensorflow实现神经网络&#34;&gt;3.4 TensorFlow实现神经网络&lt;/h2&gt;

&lt;p&gt;To be continued…&lt;/p&gt;

&lt;p&gt;10:39 p.m Thu Mar 30,2017&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014082501.jpg&#34; alt=&#34;圆明园&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：北京圆明园 Aug 25,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;作者：&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;Jimmy Song&lt;/a&gt;，&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;，2017年3月30日&lt;/p&gt;

&lt;p&gt;最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是&lt;a href=&#34;https://www.cncf.io/&#34;&gt;CNCF&lt;/a&gt;的成员之一。&lt;/p&gt;

&lt;p&gt;这篇是根据&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/#prerequisites&#34;&gt;官方安装文档&lt;/a&gt;实践整理的，操作系统是纯净的CentOS7.2。&lt;/p&gt;

&lt;p&gt;另外还有一个Peter Ma写的&lt;a href=&#34;https://godliness.github.io/2017/03/29/%E5%9C%A8CentOS7%E4%B8%8A%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Kubernetes/&#34;&gt;在CentOS上手动安装kubernetes的文档&lt;/a&gt;可以参考。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;角色分配&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面以在三台主机上安装Kubernetes为例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel
172.20.0.114 node kubectl kube-proxy flannel
172.20.0.115 node kubectl kube-proxy flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一台主机既作为master也作为node。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;系统环境&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Centos 7.2.1511&lt;/li&gt;
&lt;li&gt;docker 1.12.6&lt;/li&gt;
&lt;li&gt;etcd 3.1.5&lt;/li&gt;
&lt;li&gt;kubernetes 1.6.0&lt;/li&gt;
&lt;li&gt;flannel 0.7.0-1&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;下面给出两种安装方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。&lt;/li&gt;
&lt;li&gt;使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们最终选择使用第二种方式安装。&lt;/p&gt;

&lt;p&gt;本文的很多安装步骤和命令是参考的Kubernetes官网&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/&#34;&gt;CentOS Manual Config&lt;/a&gt;文档。&lt;/p&gt;

&lt;h2 id=&#34;第一种方式-centos系统中直接使用yum安装&#34;&gt;第一种方式：CentOS系统中直接使用yum安装&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;给yum源增加一个Repo&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[virt7-docker-common-release]
name=virt7-docker-common-release
baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装docker、kubernetes、etcd、flannel一步到位&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装好了之后需要修改一系列配置文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这个repo在CentOS7.3下是毫无意义的，因为CentOS官方源的extras中已经包含了Kubernetes1.5.2，如果你使用的是CentOS7.3的话，会自动下载安装Kubernetes1.5.2（Till March 30,2017）。如果你使用的是CentOS7.2的化，这个源就有用了，但是不幸的是，它会自动下载安装Kubernentes1.1。我们现在要安装目前的最新版本Kubernetes1.6，而使用的又是CentOS7.2，所以我们不使用yum安装（当前yum源支持的最高版本的kuberentes是1.5.2）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;第二种方式-使用二进制文件安装&#34;&gt;第二种方式：使用二进制文件安装&lt;/h2&gt;

&lt;p&gt;这种方式安装的话，需要自己一个一个组件的安装。&lt;/p&gt;

&lt;h3 id=&#34;安装docker&#34;&gt;安装Docker&lt;/h3&gt;

&lt;p&gt;yum localinstall ./docker-engine*&lt;/p&gt;

&lt;p&gt;将使用CentOS的&lt;strong&gt;extras&lt;/strong&gt; repo下载。&lt;/p&gt;

&lt;h3 id=&#34;关闭防火墙和selinux&#34;&gt;关闭防火墙和SELinux&lt;/h3&gt;

&lt;p&gt;这是官网上建议的，我是直接将iptables-services和firewlld卸载掉了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;setenforce 0
systemctl disable iptables-services firewalld
systemctl stop iptables-services firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;安装etcd&#34;&gt;安装etcd&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;下载二进制文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DOWNLOAD_URL=https://storage.googleapis.com/etcd  #etcd存储地址
ETCD_VER=v3.1.5  #设置etcd版本号
wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xvf etcd-${ETCD_VER}-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;部署文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将如下内容写入文件 /etc/etcd/etcd.conf 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;# [member]
ETCD_NAME=default
ETCD_DATA_DIR=&amp;quot;/var/lib/etcd/default.etcd&amp;quot;
# ETCD_WAL_DIR=&amp;quot;&amp;quot;
# ETCD_SNAPSHOT_COUNT=&amp;quot;10000&amp;quot;
# ETCD_HEARTBEAT_INTERVAL=&amp;quot;100&amp;quot;
# ETCD_ELECTION_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_LISTEN_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_MAX_SNAPSHOTS=&amp;quot;5&amp;quot;
# ETCD_MAX_WALS=&amp;quot;5&amp;quot;
# ETCD_CORS=&amp;quot;&amp;quot;
#
# [cluster]
# ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &amp;quot;test=http://...&amp;quot;
# ETCD_INITIAL_CLUSTER=&amp;quot;default=http://localhost:2380&amp;quot;
# ETCD_INITIAL_CLUSTER_STATE=&amp;quot;new&amp;quot;
# ETCD_INITIAL_CLUSTER_TOKEN=&amp;quot;etcd-cluster&amp;quot;
ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_DISCOVERY=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_SRV=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_FALLBACK=&amp;quot;proxy&amp;quot;
# ETCD_DISCOVERY_PROXY=&amp;quot;&amp;quot;
#
# [proxy]
# ETCD_PROXY=&amp;quot;off&amp;quot;
# ETCD_PROXY_FAILURE_WAIT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_REFRESH_INTERVAL=&amp;quot;30000&amp;quot;
# ETCD_PROXY_DIAL_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_PROXY_WRITE_TIMEOUT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_READ_TIMEOUT=&amp;quot;0&amp;quot;
#
# [security]
# ETCD_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_PEER_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# [logging]
# ETCD_DEBUG=&amp;quot;false&amp;quot;
# examples for -log-package-levels etcdserver=WARNING,security=DEBUG
# ETCD_LOG_PACKAGE_LEVELS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 etcd, etcdctl放入 /usr/bin/下，并将如下内容写进/usr/lib/systemd/system/etcd.service文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${ETCD_LISTEN_CLIENT_URLS}\&amp;quot;&amp;quot;
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动并校验&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;systemctl start etcd
systemctl enable etcd
systemctl status etcd
etcdctl ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;若要部署多节点集群也比较简单，只要更改etcd.conf文件以及etcd.service添加相应配置即可&lt;/p&gt;

&lt;p&gt;可以参考链接：&lt;a href=&#34;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&#34;&gt;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装flannel&#34;&gt;安装flannel&lt;/h3&gt;

&lt;p&gt;可以直接使用&lt;code&gt;yum install flannel&lt;/code&gt;安装。&lt;/p&gt;

&lt;p&gt;因为网络这块的配置比较复杂，我将在后续文章中说明。&lt;/p&gt;

&lt;h3 id=&#34;安装kubernetes&#34;&gt;安装Kubernetes&lt;/h3&gt;

&lt;p&gt;根据《Kubernetes权威指南（第二版）》中的介绍，直接使用GitHub上的release里的二进制文件安装。&lt;/p&gt;

&lt;p&gt;执行下面的命令安装。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
tar kubernetes.tar.gz
cd kubernetes
./cluster/get-kube-binaries.sh
cd server
tar xvf kubernetes-server-linux-amd64.tar.gz
rm -f *_tag *.tar
chmod 755 *
mv * /usr/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际下载kubernetes-server-linux-amd64.tar.gz from &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;解压完后获得的二进制文件有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cloud-controller-manager
hyperkube
kubeadm
kube-aggregator
kube-apiserver
kube-controller-manager
kubectl
kubefed
kubelet
kube-proxy
kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;cluster/juju/layers/kubernetes-master/templates&lt;/code&gt;目录下有service和环境变量配置文件的模板，这个模板本来是为了使用&lt;a href=&#34;https://jujucharms.com/&#34;&gt;juju&lt;/a&gt;安装写的。&lt;/p&gt;

&lt;h4 id=&#34;master节点的配置&#34;&gt;Master节点的配置&lt;/h4&gt;

&lt;p&gt;Master节点需要配置的kubernetes的组件有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;配置kube-apiserver&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-apiserver.service&lt;/code&gt;文件。&lt;a href=&#34;http://blog.csdn.net/yuesichiu/article/details/51485147&#34;&gt;CentOS中的service配置文件参考&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_ETCD_SERVERS \
	    $KUBE_API_ADDRESS \
	    $KUBE_API_PORT \
	    $KUBELET_PORT \
	    $KUBE_ALLOW_PRIV \
	    $KUBE_SERVICE_ADDRESSES \
	    $KUBE_ADMISSION_CONTROL \
	    $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建kubernetes的配置文件目录&lt;code&gt;/etc/kubernetes&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;添加&lt;code&gt;config&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=false&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&amp;quot;--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加&lt;code&gt;apiserver&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS=&amp;quot;--address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## The port on the local server to listen on.
KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=http://127.0.0.1:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDREKUBELET_POD_INFRA_CONTAINERSSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&amp;quot;
#
## Add your own!
#KUBE_API_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;—admission-control&lt;/code&gt;参数是Kubernetes的安全机制配置，这些安全机制都是以插件的形式用来对API Serve进行准入控制，一开始我们没有配置&lt;code&gt;ServiceAccount&lt;/code&gt;，这是为了方便集群之间的通信，不需要进行身份验证。如果你需要更高级的身份验证和鉴权的话就需要加上它了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;配置kube-controller-manager&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-controller.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;controller-manager&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-scheduler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-scheduler.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;scheduler&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-proxy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-proxy.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;proxy&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kubelet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kubelet.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBELET_API_SERVER \
	    $KUBELET_ADDRESS \
	    $KUBELET_PORT \
	    $KUBELET_HOSTNAME \
	    $KUBE_ALLOW_PRIV \
	    $KUBELET_POD_INFRA_CONTAINER \
	    $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;kubelet&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=0.0.0.0&amp;quot;
#
## The port for the info server to serve on
KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;⚠️&lt;code&gt;KUBELET_POD_INFRA_CONTAINER&lt;/code&gt;在生产环境中配置成自己私有仓库里的image。&lt;/p&gt;

&lt;h4 id=&#34;node节点配置&#34;&gt;Node节点配置&lt;/h4&gt;

&lt;p&gt;Node节点需要配置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt;的配置与master节点的kube-proxy配置相同。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt;的配置需要修改&lt;code&gt;KUBELET_HOST&lt;/code&gt;为本机的hostname，其它配置相同。&lt;/p&gt;

&lt;h1 id=&#34;启动&#34;&gt;启动&lt;/h1&gt;

&lt;p&gt;在&lt;strong&gt;Master&lt;/strong&gt;节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另外两台Node节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;在Master节点上运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get all
NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   10.254.0.1   &amp;lt;none&amp;gt;        443/TCP   1h
$kubectl get nodes
NAME                                      STATUS    AGE       VERSION
sz-pg-oam-docker-test-001.tendcloud.com   Ready     7m        v1.6.0
sz-pg-oam-docker-test-002.tendcloud.com   Ready     4m        v1.6.0
sz-pg-oam-docker-test-003.tendcloud.com   Ready     10s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以正常使用啦。&lt;/p&gt;

&lt;h3 id=&#34;后记&#34;&gt;后记&lt;/h3&gt;

&lt;p&gt;另外Kuberntes还提供第三中安装方式，请看Tony Bai写的&lt;a href=&#34;http://tonybai.com/2017/01/24/explore-kubernetes-cluster-installed-by-kubeadm/&#34;&gt;使用Kubeadm方式安装Kubernetes集群的探索&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;时隔一年重新捡起kubernetes，正好现在KubeCon正在德国柏林举行，IDC 发布的报告显示，2017年大数据全球市场规模将达324亿美元，年复合增长率为27%，其中市场增长最快的领域是数据存储领域（53.4%）&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go语言中的并发编程总结</title>
      <link>http://rootsongjc.github.io/projects/golang-concurrency-summary/</link>
      <pubDate>Fri, 24 Mar 2017 08:36:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/golang-concurrency-summary/</guid>
      <description>

&lt;h1 id=&#34;go语言并发编程总结&#34;&gt;Go语言并发编程总结&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Golang :不要通过共享内存来通信，而应该通过通信来共享内存。这句风靡在Go社区的话,说的就是 goroutine中的 channel。他在go并发编程中充当着类型安全的管道作用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;1-通过golang中的-goroutine-与sync-mutex进行并发同步&#34;&gt;1、通过golang中的 goroutine 与sync.Mutex进行并发同步&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import( 

    &amp;quot;fmt&amp;quot;

    &amp;quot;sync&amp;quot;

    &amp;quot;runtime&amp;quot;

)

var count int =0;

func counter(lock * sync.Mutex){

      lock.Lock()

      count++

      fmt.Println(count)

      lock.Unlock()

}

func main(){

   lock:=&amp;amp;sync.Mutex{}

   for i:=0;i&amp;lt;10;i++{

      //传递指针是为了防止 函数内的锁和 调用锁不一致

      go counter(lock)  

     }

   for{

      lock.Lock()

      c:=count

      lock.Unlock()

      ///把时间片给别的goroutine  未来某个时刻运行该routine

      runtime.Gosched()

      if c&amp;gt;=10{

        fmt.Println(&amp;quot;goroutine end&amp;quot;)

        break

        }

   }    

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-goroutine之间通过-channel进行通信&#34;&gt;2、goroutine之间通过 channel进行通信&lt;/h2&gt;

&lt;p&gt;channel是和类型相关的 可以理解为  是一种类型安全的管道。&lt;/p&gt;

&lt;p&gt;简单的channel 使用&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;
package main  

import &amp;quot;fmt&amp;quot;

func Count(ch chan int) {

    ch &amp;lt;- 1  

    fmt.Println(&amp;quot;Counting&amp;quot;)

}

func main() {

    chs := make([]chan int, 10)

for i := 0; i &amp;lt; 10; i++ {

        chs[i] = make(chan int)

  go Count(chs[i])

  fmt.Println(&amp;quot;Count&amp;quot;,i)

    }

for i, ch := range chs {

  &amp;lt;-ch

  fmt.Println(&amp;quot;Counting&amp;quot;,i)

    }  

} 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-go语言中的select是语言级内置-非堵塞&#34;&gt;3、Go语言中的select是语言级内置  非堵塞&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;select {

case &amp;lt;-chan1: // 如果chan1成功读到数据，则进行该case处理语句  

case chan2 &amp;lt;- 1: // 如果成功向chan2写入数据，则进行该case处理语句  

default: // 如果上面都没有成功，则进入default处理流程  

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出，select不像switch，后面并不带判断条件，而是直接去查看case语句。每个&lt;/p&gt;

&lt;p&gt;case语句都必须是一个面向channel的操作。比如上面的例子中，第一个case试图从chan1读取&lt;/p&gt;

&lt;p&gt;一个数据并直接忽略读到的数据，而第二个case则是试图向chan2中写入一个整型数1，如果这&lt;/p&gt;

&lt;p&gt;两者都没有成功，则到达default语句。&lt;/p&gt;

&lt;h2 id=&#34;4-channel-的带缓冲读取写入&#34;&gt;4、channel 的带缓冲读取写入&lt;/h2&gt;

&lt;p&gt;之前我们示范创建的都是不带缓冲的channel，这种做法对于传递单个数据的场景可以接受，&lt;/p&gt;

&lt;p&gt;但对于需要持续传输大量数据的场景就有些不合适了。接下来我们介绍如何给channel带上缓冲，&lt;/p&gt;

&lt;p&gt;从而达到消息队列的效果。&lt;/p&gt;

&lt;p&gt;要创建一个带缓冲的channel，其实也非常容易：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;c := make(chan int, 1024)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在调用make()时将缓冲区大小作为第二个参数传入即可，比如上面这个例子就创建了一个大小&lt;/p&gt;

&lt;p&gt;为1024的int类型channel，即使没有读取方，写入方也可以一直往channel里写入，在缓冲区被&lt;/p&gt;

&lt;p&gt;填完之前都不会阻塞。&lt;/p&gt;

&lt;p&gt;从带缓冲的channel中读取数据可以使用与常规非缓冲channel完全一致的方法，但我们也可&lt;/p&gt;

&lt;p&gt;以使用range关键来实现更为简便的循环读取：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for i := range c {

    fmt.Println(&amp;quot;Received:&amp;quot;, i)

} 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-用goroutine模拟生产消费者&#34;&gt;5、用goroutine模拟生产消费者&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

import &amp;quot;time&amp;quot;

func Producer (queue chan&amp;lt;- int){

        for i:= 0; i &amp;lt; 10; i++ {

                queue &amp;lt;- i  

                }

}

func Consumer( queue &amp;lt;-chan int){

        for i :=0; i &amp;lt; 10; i++{

                v := &amp;lt;- queue

                fmt.Println(&amp;quot;receive:&amp;quot;, v)

        }

}

func main(){

        queue := make(chan int, 1)

        go Producer(queue)

        go Consumer(queue)

        time.Sleep(1e9) //让Producer与Consumer完成

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-通过make-创建通道&#34;&gt;6、 通过make 创建通道&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;make(c1 chan int)   创建的是 同步channel ...读写完全对应

make(c1 chan int ,10) 闯进带缓冲的通道 上来可以写10次
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;7-随机向通道中写入0或者1&#34;&gt;7、随机向通道中写入0或者1&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

import &amp;quot;time&amp;quot;

func main(){

       ch := make(chan int, 1)

 for {

   ///不停向channel中写入 0 或者1

  select {

   case ch &amp;lt;- 0:

   case ch &amp;lt;- 1:

  }

    //从通道中取出数据

    i := &amp;lt;-ch

    fmt.Println(&amp;quot;Value received:&amp;quot;,i)

    time.Sleep(1e8)

    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;8-带缓冲的channel&#34;&gt;8、带缓冲的channel&lt;/h2&gt;

&lt;p&gt;之前创建的都是不带缓冲的channel，这种做法对于传递单个数据的场景可以接受，&lt;/p&gt;

&lt;p&gt;但对于需要持续传输大量数据的场景就有些不合适了。接下来我们介绍如何给channel带上缓冲，&lt;/p&gt;

&lt;p&gt;从而达到消息队列的效果。&lt;/p&gt;

&lt;p&gt;要创建一个带缓冲的channel，其实也非常容易：&lt;/p&gt;

&lt;p&gt;c := make(chan int, 1024)&lt;/p&gt;

&lt;p&gt;在调用make()时将缓冲区大小作为第二个参数传入即可，比如上面这个例子就创建了一个大小&lt;/p&gt;

&lt;p&gt;为1024的int类型channel，即使没有读取方，写入方也可以一直往channel里写入，在缓冲区被&lt;/p&gt;

&lt;p&gt;填完之前都不会阻塞。&lt;/p&gt;

&lt;p&gt;从带缓冲的channel中读取数据可以使用与常规非缓冲channel完全一致的方法，但我们也可&lt;/p&gt;

&lt;p&gt;以使用range关键来实现更为简便的循环读取：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for i := range c {

    fmt.Println(&amp;quot;Received:&amp;quot;, i)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;下面是测试代码&lt;/strong&gt;&amp;lsquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

import &amp;quot;time&amp;quot;

func A(c chan int){

 for i:=0;i&amp;lt;10;i++{

        c&amp;lt;- i

    }

}

func B(c chan int){

 for val:=range c {

      fmt.Println(&amp;quot;Value:&amp;quot;,val)  

    }

}

func main(){

    chs:=make(chan int,10)

    //只要有通道操作一定要放到goroutine中否则 会堵塞当前的主线程 并且导致程序退出

    //对于同步通道 或者带缓冲的通道 一定要封装成函数 使用 goroutine 包装

    go A(chs)

    go B(chs)

    time.Sleep(1e9)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;9-关于创建多个goroutine具体到go语言会创建多少个线程&#34;&gt;9、关于创建多个goroutine具体到go语言会创建多少个线程&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;os&amp;quot;

func main() {

    for i:=0; i&amp;lt;20; i++ {

        go func() {

            for {

                b:=make([]byte, 10)

                os.Stdin.Read(b) // will block

            }

        }()

    }

    select{}

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;会产生21个线程：&lt;/p&gt;

&lt;p&gt;runtime scheduler(src/pkg/runtime/proc.c)会维护一个线程池，当某个goroutine被block后，scheduler会创建一个新线程给其他ready的goroutine&lt;/p&gt;

&lt;p&gt;GOMAXPROCS控制的是未被阻塞的所有goroutine被multiplex到多少个线程上运行&lt;/p&gt;

&lt;h2 id=&#34;10-在channel中也是可以传递channel的-go语言的channel和map-slice等一样都是原生类型&#34;&gt;10、在channel中也是可以传递channel的,Go语言的channel和map、slice等一样都是原生类型&lt;/h2&gt;

&lt;p&gt;需要注意的是，在Go语言中channel本身也是一个原生类型，与map之类的类型地位一样，因&lt;/p&gt;

&lt;p&gt;此channel本身在定义后也可以通过channel来传递。&lt;/p&gt;

&lt;p&gt;我们可以使用这个特性来实现*nix上非常常见的管道（pipe）特性。管道也是使用非常广泛&lt;/p&gt;

&lt;p&gt;的一种设计模式，比如在处理数据时，我们可以采用管道设计，这样可以比较容易以插件的方式&lt;/p&gt;

&lt;p&gt;增加数据的处理流程。&lt;/p&gt;

&lt;p&gt;下面我们利用channel可被传递的特性来实现我们的管道。 为了简化表达， 我们假设在管道中&lt;/p&gt;

&lt;p&gt;传递的数据只是一个整型数，在实际的应用场景中这通常会是一个数据块。&lt;/p&gt;

&lt;p&gt;首先限定基本的数据结构：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type PipeData struct {

    value int

    handler func(int) int

    next chan int

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我们写一个常规的处理函数。我们只要定义一系列PipeData的数据结构并一起传递给&lt;/p&gt;

&lt;p&gt;这个函数，就可以达到流式处理数据的目的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func handle(queue chan *PipeData) {

for data := range queue {

        data.next &amp;lt;- data.handler(data.value)

    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;11-我们默认创建的是双向通道-单向通道没有意义-但是我们却可以通过强制转换-将双向通道-转换成为单向通道&#34;&gt;11、我们默认创建的是双向通道,单向通道没有意义,但是我们却可以通过强制转换 将双向通道 转换成为单向通道 。&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var ch1 chan int  // ch1是一个正常的channel，不是单向的  

var ch2 chan&amp;lt;- float64// ch2是单向channel，只用于写float64数据

var ch3 &amp;lt;-chan int // ch3是单向channel，只用于读取int数据 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;channel是一个原生类型，因此不仅 支持被传递，还支持类型转换。只有在介绍了单向channel的概念后，读者才会明白类型转换对于&lt;/p&gt;

&lt;p&gt;channel的意义：就是在单向channel和双向channel之间进行转换。&lt;/p&gt;

&lt;p&gt;示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ch4 := make(chan int)

ch5 := &amp;lt;-chan int(ch4) // ch5就是一个单向的读取channel

ch6 := chan&amp;lt;- int(ch4) // ch6 是一个单向的写入channel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;基于ch4，我们通过类型转换初始化了两个单向channel：单向读的ch5和单向写的ch6。&lt;/p&gt;

&lt;p&gt;从设计的角度考虑，所有的代码应该都遵循“最小权限原则” ，&lt;/p&gt;

&lt;p&gt;从而避免没必要地使用泛滥问题， 进而导致程序失控。 写过C++程序的读者肯定就会联想起const 指针的用法。非const指针具备const指针的所有功能，将一个指针设定为const就是明确告诉&lt;/p&gt;

&lt;p&gt;函数实现者不要试图对该指针进行修改。单向channel也是起到这样的一种契约作用。&lt;/p&gt;

&lt;p&gt;下面我们来看一下单向channel的用法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Parse(ch &amp;lt;-chan int) {

for value := range ch {

        fmt.Println(&amp;quot;Parsing value&amp;quot;, value)  

    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除非这个函数的实现者无耻地使用了类型转换，否则这个函数就不会因为各种原因而对ch 进行写，避免在ch中出现非期望的数据，从而很好地实践最小权限原则。&lt;/p&gt;

&lt;h2 id=&#34;12-只读只写单向-channel-代码例子-遵循权限最小化的原则&#34;&gt;12、只读只写单向 channel 代码例子，遵循权限最小化的原则&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

import &amp;quot;time&amp;quot;

//接受一个参数 是只允许读取通道  除非直接强制转换 要么你只能从channel中读取数据

func sCh(ch &amp;lt;-chan int){

   for val:= range ch {

     fmt.Println(val)

   }

}

func main(){

    //创建一个带100缓冲的通道 可以直接写入 而不会导致 主线程堵塞

    dch:=make(chan int,100)

    for i:=0;i&amp;lt;100;i++{

      dch&amp;lt;- i  

    }

    //传递进去 只读通道

    go sCh(dch)

    time.Sleep(1e9)

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;13-channel的关闭-以及判断channel的关闭&#34;&gt;13、channel的关闭,以及判断channel的关闭&lt;/h2&gt;

&lt;p&gt;关闭channel非常简单，直接使用Go语言内置的close()函数即可：&lt;/p&gt;

&lt;p&gt;close(ch)&lt;/p&gt;

&lt;p&gt;在介绍了如何关闭channel之后，我们就多了一个问题：如何判断一个channel是否已经被关&lt;/p&gt;

&lt;p&gt;闭？我们可以在读取的时候使用多重返回值的方式：&lt;/p&gt;

&lt;p&gt;x, ok := &amp;lt;-ch&lt;/p&gt;

&lt;p&gt;这个用法与map中的按键获取value的过程比较类似，只需要看第二个bool返回值即可，如&lt;/p&gt;

&lt;p&gt;果返回值是false则表示ch已经被关闭。&lt;/p&gt;

&lt;h2 id=&#34;14-go的多核并行化编程&#34;&gt;14、Go的多核并行化编程&lt;/h2&gt;

&lt;p&gt;高性能并发编程 必须设置GOMAXPROCS 为最大核数目,这个值由runtime.NumCPU()获取&lt;/p&gt;

&lt;p&gt;在执行一些昂贵的计算任务时， 我们希望能够尽量利用现代服务器普遍具备的多核特性来尽&lt;/p&gt;

&lt;p&gt;量将任务并行化，从而达到降低总计算时间的目的。此时我们需要了解CPU核心的数量，并针对&lt;/p&gt;

&lt;p&gt;性地分解计算任务到多个goroutine中去并行运行。&lt;/p&gt;

&lt;p&gt;下面我们来模拟一个完全可以并行的计算任务：计算N个整型数的总和。我们可以将所有整&lt;/p&gt;

&lt;p&gt;型数分成M份，M即CPU的个数。让每个CPU开始计算分给它的那份计算任务，最后将每个CPU&lt;/p&gt;

&lt;p&gt;的计算结果再做一次累加，这样就可以得到所有N个整型数的总和：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Vector []float64

// 分配给每个CPU的计算任务

func (v Vector) DoSome(i, n int, u Vector, c chan int) {

for ; i &amp;lt; n; i++ {

         v[i] += u.Op(v[i])

     }

     c &amp;lt;- 1       

// 发信号告诉任务管理者我已经计算完成了

}

const NCPU = 16     

// 假设总共有16核   

func (v Vector) DoAll(u Vector) {   

    c := make(chan int, NCPU)  // 用于接收每个CPU的任务完成信号   

for i := 0; i &amp;lt; NCPU; i++ {   

go v.DoSome(i*len(v)/NCPU, (i+1)*len(v)/NCPU, u, c)

    } 

// 等待所有CPU的任务完成

for i := 0; i &amp;lt; NCPU; i++ {   

&amp;lt;-c    // 获取到一个数据，表示一个CPU计算完成了

    }

// 到这里表示所有计算已经结束

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这两个函数看起来设计非常合理。DoAll()会根据CPU核心的数目对任务进行分割，然后开&lt;/p&gt;

&lt;p&gt;辟多个goroutine来并行执行这些计算任务。&lt;/p&gt;

&lt;p&gt;是否可以将总的计算时间降到接近原来的1/N呢？答案是不一定。如果掐秒表（正常点的话，&lt;/p&gt;

&lt;p&gt;应该用7.8节中介绍的Benchmark方法） ，会发现总的执行时间没有明显缩短。再去观察CPU运行&lt;/p&gt;

&lt;p&gt;状态， 你会发现尽管我们有16个CPU核心， 但在计算过程中其实只有一个CPU核心处于繁忙状态，&lt;/p&gt;

&lt;p&gt;这是会让很多Go语言初学者迷惑的问题。&lt;/p&gt;

&lt;p&gt;官方的答案是，这是当前版本的Go编译器还不能很智能地去发现和利用多核的优势。虽然&lt;/p&gt;

&lt;p&gt;我们确实创建了多个goroutine，并且从运行状态看这些goroutine也都在并行运行，但实际上所有&lt;/p&gt;

&lt;p&gt;这些goroutine都运行在同一个CPU核心上， 在一个goroutine得到时间片执行的时候， 其他goroutine&lt;/p&gt;

&lt;p&gt;都会处于等待状态。从这一点可以看出，虽然goroutine简化了我们写并行代码的过程，但实际上&lt;/p&gt;

&lt;p&gt;整体运行效率并不真正高于单线程程序。&lt;/p&gt;

&lt;p&gt;在Go语言升级到默认支持多CPU的某个版本之前，我们可以先通过设置环境变量&lt;/p&gt;

&lt;p&gt;GOMAXPROCS的值来控制使用多少个CPU核心。具体操作方法是通过直接设置环境变量&lt;/p&gt;

&lt;p&gt;GOMAXPROCS的值，或者在代码中启动goroutine之前先调用以下这个语句以设置使用16个CPU&lt;/p&gt;

&lt;p&gt;核心：&lt;/p&gt;

&lt;p&gt;runtime.GOMAXPROCS(16)&lt;/p&gt;

&lt;p&gt;到底应该设置多少个CPU核心呢，其实runtime包中还提供了另外一个函数NumCPU()来获&lt;/p&gt;

&lt;p&gt;取核心数。可以看到，Go语言其实已经感知到所有的环境信息，下一版本中完全可以利用这些&lt;/p&gt;

&lt;p&gt;信息将goroutine调度到所有CPU核心上，从而最大化地利用服务器的多核计算能力。抛弃&lt;/p&gt;

&lt;p&gt;GOMAXPROCS只是个时间问题。&lt;/p&gt;

&lt;h2 id=&#34;15-主动出让时间片给其他-goroutine-在未来的某一时刻再来执行当前goroutine&#34;&gt;15、主动出让时间片给其他 goroutine 在未来的某一时刻再来执行当前goroutine&lt;/h2&gt;

&lt;p&gt;我们可以在每个goroutine中控制何时主动出让时间片给其他goroutine，这可以使用runtime&lt;/p&gt;

&lt;p&gt;包中的Gosched()函数实现。&lt;/p&gt;

&lt;p&gt;实际上，如果要比较精细地控制goroutine的行为，就必须比较深入地了解Go语言开发包中&lt;/p&gt;

&lt;p&gt;runtime包所提供的具体功能。&lt;/p&gt;

&lt;h2 id=&#34;16-go中的同步&#34;&gt;16、Go中的同步&lt;/h2&gt;

&lt;p&gt;倡导用通信来共享数据，而不是通过共享数据来进行通信，但考虑&lt;/p&gt;

&lt;p&gt;到即使成功地用channel来作为通信手段，还是避免不了多个goroutine之间共享数据的问题，Go&lt;/p&gt;

&lt;p&gt;语言的设计者虽然对channel有极高的期望，但也提供了妥善的资源锁方案。&lt;/p&gt;

&lt;h2 id=&#34;17-go中的同步锁&#34;&gt;17、Go中的同步锁&lt;/h2&gt;

&lt;p&gt;倡导用通信来共享数据，而不是通过共享数据来进行通信，但考虑&lt;/p&gt;

&lt;p&gt;到即使成功地用channel来作为通信手段，还是避免不了多个goroutine之间共享数据的问题，Go&lt;/p&gt;

&lt;p&gt;语言的设计者虽然对channel有极高的期望，但也提供了妥善的资源锁方案。&lt;/p&gt;

&lt;p&gt;对于这两种锁类型， 任何一个Lock()或RLock()均需要保证对应有Unlock()或RUnlock()&lt;/p&gt;

&lt;p&gt;调用与之对应，否则可能导致等待该锁的所有goroutine处于饥饿状态，甚至可能导致死锁。锁的&lt;/p&gt;

&lt;p&gt;典型使用模式如下：&lt;/p&gt;

&lt;p&gt;var l sync.Mutex&lt;/p&gt;

&lt;p&gt;func foo() {&lt;/p&gt;

&lt;p&gt;l.Lock()&lt;/p&gt;

&lt;p&gt;//延迟调用 在函数退出 并且局部资源被释放的时候 调用&lt;/p&gt;

&lt;p&gt;defer l.Unlock()&lt;/p&gt;

&lt;p&gt;//&amp;hellip;&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;这里我们再一次见证了Go语言defer关键字带来的优雅&lt;/p&gt;

&lt;h2 id=&#34;18-全局唯一操作-sync-once-do-sync-atomic原子操作子包&#34;&gt;18、全局唯一操作 sync.Once.Do()     sync.atomic原子操作子包&lt;/h2&gt;

&lt;p&gt;对于从全局的角度只需要运行一次的代码，比如全局初始化操作，Go语言提供了一个Once&lt;/p&gt;

&lt;p&gt;类型来保证全局的唯一性操作，具体代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var a string

var once sync.Once  

func setup() {

a = &amp;quot;hello, world&amp;quot;

}  

func doprint() {

once.Do(setup)

print(a)  

}  

func twoprint() {

go doprint()

go doprint()  

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果这段代码没有引入Once， setup()将会被每一个goroutine先调用一次， 这至少对于这个&lt;/p&gt;

&lt;p&gt;例子是多余的。在现实中，我们也经常会遇到这样的情况。Go语言标准库为我们引入了Once类&lt;/p&gt;

&lt;p&gt;型以解决这个问题。once的Do()方法可以保证在全局范围内只调用指定的函数一次（这里指&lt;/p&gt;

&lt;p&gt;setup()函数） ，而且所有其他goroutine在调用到此语句时，将会先被阻塞，直至全局唯一的&lt;/p&gt;

&lt;p&gt;once.Do()调用结束后才继续。&lt;/p&gt;

&lt;p&gt;这个机制比较轻巧地解决了使用其他语言时开发者不得不自行设计和实现这种Once效果的&lt;/p&gt;

&lt;p&gt;难题，也是Go语言为并发性编程做了尽量多考虑的一种体现。&lt;/p&gt;

&lt;p&gt;如果没有once.Do()，我们很可能只能添加一个全局的bool变量，在函数setup()的最后&lt;/p&gt;

&lt;p&gt;一行将该bool变量设置为true。在对setup()的所有调用之前，需要先判断该bool变量是否已&lt;/p&gt;

&lt;p&gt;经被设置为true，如果该值仍然是false，则调用一次setup()，否则应跳过该语句。实现代码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var done bool = false

func setup() {

a = &amp;quot;hello, world&amp;quot; 

done = true

}     

func doprint() { 

if !done {

        setup()

    }   

print(a)  

}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这段代码初看起来比较合理， 但是细看还是会有问题， 因为setup()并不是一个原子性操作，&lt;/p&gt;

&lt;p&gt;这种写法可能导致setup()函数被多次调用，从而无法达到全局只执行一次的目标。这个问题的&lt;/p&gt;

&lt;p&gt;复杂性也更加体现了Once类型的价值。&lt;/p&gt;

&lt;p&gt;为了更好地控制并行中的原子性操作，sync包中还包含一个atomic子包，它提供了对于一&lt;/p&gt;

&lt;p&gt;些基础数据类型的原子操作函数，比如下面这个函数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func CompareAndSwapUint64(val *uint64, old, new uint64) (swapped bool)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就提供了比较和交换两个uint64类型数据的操作。这让开发者无需再为这样的操作专门添加Lock操作。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://lib.csdn.net/article/53/36140?knId=1441&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pivotal Cloud foundry快速开始指南</title>
      <link>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</link>
      <pubDate>Thu, 23 Mar 2017 22:54:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2013100302.jpg&#34; alt=&#34;黄山日出&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：黄山日出后的云海 Oct 3,2013）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;最近研究了下&lt;strong&gt;Pivotal&lt;/strong&gt;的&lt;strong&gt;Cloud foundry&lt;/strong&gt;，CF本身是一款开源软件，很多PAAS厂商都加入了CF，我们用的是的&lt;strong&gt;PCF Dev&lt;/strong&gt;（PCF Dev是一款可以在工作站上运行的轻量级PCF安装）来试用的，因为它可以部署在自己的环境里，而&lt;strong&gt;Pivotal Web Services&lt;/strong&gt;只免费两个月，之后就要收费。&lt;a href=&#34;https://pivotal.io/cn/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/introduction&#34;&gt;这里&lt;/a&gt;有官方的详细教程。&lt;/p&gt;

&lt;h2 id=&#34;开始&#34;&gt;开始&lt;/h2&gt;

&lt;p&gt;根据官网的示例，我们将运行一个Java程序示例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装命令行终端&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pivotal.io/cn/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/install-the-cf-cli&#34;&gt;下载&lt;/a&gt;后双击安装即可，然后执行&lt;code&gt;cf help&lt;/code&gt;能够看到帮助。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装PCF Dev&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;先&lt;a href=&#34;https://network.pivotal.io/products/pcfdev&#34;&gt;下载&lt;/a&gt;，如果你没有Pivotal network账号的话，还需要注册个用户，然后用以下命令安装：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;
$./pcfdev-VERSION-osx &amp;amp;&amp;amp; \
cf dev start
Less than 4096 MB of free memory detected, continue (y/N): &amp;gt; y
Please sign in with your Pivotal Network account.
Need an account? Join Pivotal Network: https://network.pivotal.io

Email&amp;gt; 849122844@qq.com

Password&amp;gt; 
Downloading VM...
Progress: |+++++++++++++=======&amp;gt;| 100% 
VM downloaded.
Allocating 4096 MB out of 16384 MB total system memory (3514 MB free).
Importing VM...
Starting VM...
Provisioning VM...
Waiting for services to start...
8 out of 57 running
8 out of 57 running
8 out of 57 running
46 out of 57 running
57 out of 57 running
 _______  _______  _______    ______   _______  __   __
|       ||       ||       |  |      | |       ||  | |  |
|    _  ||       ||    ___|  |  _    ||    ___||  |_|  |
|   |_| ||       ||   |___   | | |   ||   |___ |       |
|    ___||      _||    ___|  | |_|   ||    ___||       |
|   |    |     |_ |   |      |       ||   |___  |     |
|___|    |_______||___|      |______| |_______|  |___|
is now running.
To begin using PCF Dev, please run:
   cf login -a https://api.local.pcfdev.io --skip-ssl-validation
Apps Manager URL: https://local.pcfdev.io
Admin user =&amp;gt; Email: admin / Password: admin
Regular user =&amp;gt; Email: user / Password: pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动过程中还需要&lt;strong&gt;Sign In&lt;/strong&gt;，所以注册完后要记住用户名（邮箱地址）和密码（必须超过8位要有特殊字符和大写字母）。这个过程中还要下载VM，对内存要求至少4G。而且下载速度比较慢，我下载的了大概3个多小时吧。&lt;/p&gt;

&lt;p&gt;下面部署一个应用到PCF Dev上试一试。&lt;/p&gt;

&lt;h2 id=&#34;部署应用&#34;&gt;部署应用&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;下载代码&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$git clone https://github.com/cloudfoundry-samples/spring-music
$cd ./spring-music
$cf login -a api.local.pcfdev.io --skip-ssl-validation
API endpoint: api.local.pcfdev.io

Email&amp;gt; user

Password&amp;gt; pass
Authenticating...
OK

Targeted org pcfdev-org

Targeted space pcfdev-space


                
API endpoint:   https://api.local.pcfdev.io (API version: 2.65.0)
User:           user
Org:            pcfdev-org
Space:          pcfdev-space
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;编译应用&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用gradle来编译。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$./gradlew assemble
:compileJava UP-TO-DATE
:processResources UP-TO-DATE
:classes UP-TO-DATE
:findMainClass
:jar
:bootRepackage
Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.13/jersey-client-1.13.jar
Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.13/jersey-json-1.13.jar
Download https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.5/httpcore-4.4.5.jar
Download https://repo1.maven.org/maven2/com/nimbusds/oauth2-oidc-sdk/4.5/oauth2-oidc-sdk-4.5.jar
Download https://repo1.maven.org/maven2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar
Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.13/jersey-core-1.13.jar
Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar
Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar
Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar
Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar
Download https://repo1.maven.org/maven2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar
Download https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.3.1/commons-lang3-3.3.1.jar
Download https://repo1.maven.org/maven2/net/minidev/json-smart/1.1.1/json-smart-1.1.1.jar
Download https://repo1.maven.org/maven2/com/nimbusds/lang-tag/1.4/lang-tag-1.4.jar
Download https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/3.1.2/nimbus-jose-jwt-3.1.2.jar
Download https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar
Download https://repo1.maven.org/maven2/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar
Download https://repo1.maven.org/maven2/javax/mail/mail/1.4.7/mail-1.4.7.jar
:assemble

BUILD SUCCESSFUL

Total time: 1 mins 25.649 secs

This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.14/userguide/gradle_daemon.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;上传应用&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;设置应用的主机名为spring-music。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf push --hostname spring-music
Using manifest file /Users/jimmy/Workspace/github/cloudfoundry-samples/spring-music/manifest.yml

Creating app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

Creating route spring-music.local.pcfdev.io...
OK

Binding spring-music.local.pcfdev.io to spring-music...
OK

Uploading spring-music...
Uploading app files from: /var/folders/61/f7mqkyjn1nz5mfmfvdztgzjw0000gn/T/unzipped-app139680305
Uploading 38.9M, 234 files
Done uploading               
OK

Starting app spring-music in org pcfdev-org / space pcfdev-space as user...
Downloading dotnet-core_buildpack...
Downloading go_buildpack...
Downloading python_buildpack...
Downloading php_buildpack...
Downloading staticfile_buildpack...
Downloaded staticfile_buildpack
Downloading binary_buildpack...
Downloaded binary_buildpack (9.3K)
Downloading java_buildpack...
Downloaded java_buildpack (249.1M)
Downloaded dotnet-core_buildpack (169.3M)
Downloading ruby_buildpack...
Downloading nodejs_buildpack...
Downloaded python_buildpack (255.3M)
Downloaded nodejs_buildpack (109.4M)
Downloaded go_buildpack (392M)
Downloaded php_buildpack (310.4M)
Downloaded ruby_buildpack (260.8M)
Creating container
Successfully created container
Downloading app package...
Downloaded app package (38.8M)
Staging...
-----&amp;gt; Java Buildpack Version: v3.10 (offline) | https://github.com/cloudfoundry/java-buildpack.git#193d6b7
-----&amp;gt; Downloading Open Jdk JRE 1.8.0_111 from https://java-buildpack.cloudfoundry.org/openjdk/trusty/x86_64/openjdk-1.8.0_111.tar.gz (found in cache)
       Expanding Open Jdk JRE to .java-buildpack/open_jdk_jre (1.4s)
-----&amp;gt; Downloading Open JDK Like Memory Calculator 2.0.2_RELEASE from https://java-buildpack.cloudfoundry.org/memory-calculator/trusty/x86_64/memory-calculator-2.0.2_RELEASE.tar.gz (found in cache)
       Memory Settings: -Xss349K -Xmx681574K -XX:MaxMetaspaceSize=104857K -Xms681574K -XX:MetaspaceSize=104857K
-----&amp;gt; Downloading Spring Auto Reconfiguration 1.10.0_RELEASE from https://java-buildpack.cloudfoundry.org/auto-reconfiguration/auto-reconfiguration-1.10.0_RELEASE.jar (found in cache)
Exit status 0
Staging complete
Uploading droplet, build artifacts cache...
Uploading build artifacts cache...
Uploading droplet...
Uploaded build artifacts cache (108B)
Uploaded droplet (83.9M)
Uploading complete
Destroying container
Successfully destroyed container

0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
1 of 1 instances running

App started


OK

App spring-music was started using this command `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) &amp;amp;&amp;amp; JAVA_OPTS=&amp;quot;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY&amp;quot; &amp;amp;&amp;amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher`

Showing health and status for app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

requested state: started
instances: 1/1
usage: 1G x 1 instances
urls: spring-music.local.pcfdev.io
last uploaded: Thu Mar 23 14:29:46 UTC 2017
stack: cflinuxfs2
buildpack: java-buildpack=v3.10-offline-https://github.com/cloudfoundry/java-buildpack.git#193d6b7 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.10.0_RELEASE

     state     since                    cpu      memory       disk             details
#0   running   2017-03-23 10:31:36 PM   160.7%   442M of 1G   165.6M of 512M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;在浏览器中访问app&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;spring-music.local.pcfdev.io&#34;&gt;spring-music.local.pcfdev.io&lt;/a&gt;页面如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/pivotal-cloudfoundry-spring-music.jpg&#34; alt=&#34;spring-music&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;requested state: started
instances: 1/1
usage: 512M x 1 instances
urls: spring-music.local.pcfdev.io
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;查看日志&#34;&gt;查看日志&lt;/h2&gt;

&lt;p&gt;PCF提供应用的日志聚合功能，你可以查看HTTP请求、对应用操作时候的output，如扩容、重启等。&lt;/p&gt;

&lt;p&gt;每行日志中都包括如下信息：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Timestamp&lt;/li&gt;
&lt;li&gt;Log type&lt;/li&gt;
&lt;li&gt;Channel&lt;/li&gt;
&lt;li&gt;Message&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;查看刚才那个应用的日志信息：&lt;/p&gt;

&lt;p&gt;查看最近输出：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf logs spring-music --recent
2017-03-23T22:34:05.17+0800 [RTR/0]      OUT spring-music.local.pcfdev.io - [23/03/2017:14:34:05.163 +0000] &amp;quot;GET /templates/albumForm.html HTTP/1.1&amp;quot; 200 0 2518 &amp;quot;http://spring-music.local.pcfdev.io/&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36&amp;quot; 192.168.11.1:52097 10.0.2.15:60012 x_forwarded_for:&amp;quot;-&amp;quot; x_forwarded_proto:&amp;quot;http&amp;quot; vcap_request_id:c6b5f34d-bc5a-4c66-77aa-cb768b273f21 response_time:0.007390127 app_id:fdc7a43e-61b8-40e9-b1dc-38b858037da9 app_index:0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看实时输出流：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf logs spring-music
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;连接数据库&#34;&gt;连接数据库&lt;/h2&gt;

&lt;p&gt;在上面的那个例子中用的是内存数据库。我们可以改用mysql数据库。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看可用的数据&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf marketplace -s p-mysql
Getting service plan information for service p-mysql as user...
OK

service plan   description            free or paid
512mb          PCF Dev MySQL Server   free
1gb            PCF Dev MySQL Server   free
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建数据库&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf create-service p-mysql 512mb my-spring-db
Creating service instance my-spring-db in org pcfdev-org / space pcfdev-space as user...
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将它与我们上面的示例应用程序绑定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf bind-service spring-music my-spring-db
Binding service my-spring-db to app spring-music in org pcfdev-org / space pcfdev-space as user...
OK
TIP: Use &#39;cf restage spring-music&#39; to ensure your env variable changes take effect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启app&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf restart spring-music
Stopping app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

Starting app spring-music in org pcfdev-org / space pcfdev-space as user...

0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
0 of 1 instances running, 1 starting
1 of 1 instances running

App started


OK

App spring-music was started using this command `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) &amp;amp;&amp;amp; JAVA_OPTS=&amp;quot;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY&amp;quot; &amp;amp;&amp;amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher`

Showing health and status for app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

requested state: started
instances: 1/1
usage: 1G x 1 instances
urls: spring-music.local.pcfdev.io
last uploaded: Thu Mar 23 14:29:46 UTC 2017
stack: cflinuxfs2
buildpack: java-buildpack=v3.10-offline-https://github.com/cloudfoundry/java-buildpack.git#193d6b7 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.10.0_RELEASE

     state     since                    cpu      memory         disk             details
#0   running   2017-03-23 10:44:18 PM   150.4%   461.6M of 1G   165.6M of 512M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在我们再查看下自己的service。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf services
Getting services in org pcfdev-org / space pcfdev-space as user...
OK

name           service   plan    bound apps     last operation
my-spring-db   p-mysql   512mb   spring-music   create succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;应用扩容&#34;&gt;应用扩容&lt;/h2&gt;

&lt;p&gt;扩展应用的示例数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf scale spring-music -i 2
Scaling app spring-music in org pcfdev-org / space pcfdev-space as user...
OK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再查看下&lt;code&gt;spring-music&lt;/code&gt;应用的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf app spring-music
Showing health and status for app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

requested state: started
instances: 2/2
usage: 1G x 2 instances
urls: spring-music.local.pcfdev.io
last uploaded: Thu Mar 23 14:29:46 UTC 2017
stack: cflinuxfs2
buildpack: java-buildpack=v3.10-offline-https://github.com/cloudfoundry/java-buildpack.git#193d6b7 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.10.0_RELEASE

     state      since                    cpu    memory         disk             details
#0   running    2017-03-23 10:44:18 PM   0.5%   451.4M of 1G   165.6M of 512M
#1   starting   2017-03-23 10:46:19 PM   0.0%   348.3M of 1G   165.6M of 512M

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以对内存进行扩容。这个操作会重启应用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf scale spring-music -m 1G
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
1 of 2 instances running, 1 down

App started


OK

App spring-music was started using this command `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) &amp;amp;&amp;amp; JAVA_OPTS=&amp;quot;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY&amp;quot; &amp;amp;&amp;amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher`

Showing health and status for app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

requested state: started
instances: 1/2
usage: 1G x 2 instances
urls: spring-music.local.pcfdev.io
last uploaded: Thu Mar 23 14:29:46 UTC 2017
stack: cflinuxfs2
buildpack: java-buildpack=v3.10-offline-https://github.com/cloudfoundry/java-buildpack.git#193d6b7 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.10.0_RELEASE

     state     since                    cpu      memory         disk             details
#0   running   2017-03-23 10:48:43 PM   145.1%   412.2M of 1G   165.6M of 512M
#1   down      2017-03-23 10:48:14 PM   0.7%     436.2M of 1G   165.6M of 512M   insufficient resources: memory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以增加应用的磁盘大小。这个操作也会重启应用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$cf scale spring-music -k 512M
This will cause the app to restart. Are you sure you want to scale spring-music?&amp;gt; y

Scaling app spring-music in org pcfdev-org / space pcfdev-space as user...
OK
Stopping app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

Starting app spring-music in org pcfdev-org / space pcfdev-space as user...

0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
0 of 2 instances running, 1 starting, 1 down
1 of 2 instances running, 1 down

App started


OK

App spring-music was started using this command `CALCULATED_MEMORY=$($PWD/.java-buildpack/open_jdk_jre/bin/java-buildpack-memory-calculator-2.0.2_RELEASE -memorySizes=metaspace:64m..,stack:228k.. -memoryWeights=heap:65,metaspace:10,native:15,stack:10 -memoryInitials=heap:100%,metaspace:100% -stackThreads=300 -totMemory=$MEMORY_LIMIT) &amp;amp;&amp;amp; JAVA_OPTS=&amp;quot;-Djava.io.tmpdir=$TMPDIR -XX:OnOutOfMemoryError=$PWD/.java-buildpack/open_jdk_jre/bin/killjava.sh $CALCULATED_MEMORY&amp;quot; &amp;amp;&amp;amp; SERVER_PORT=$PORT eval exec $PWD/.java-buildpack/open_jdk_jre/bin/java $JAVA_OPTS -cp $PWD/. org.springframework.boot.loader.JarLauncher`

Showing health and status for app spring-music in org pcfdev-org / space pcfdev-space as user...
OK

requested state: started
instances: 1/2
usage: 1G x 2 instances
urls: spring-music.local.pcfdev.io
last uploaded: Thu Mar 23 14:29:46 UTC 2017
stack: cflinuxfs2
buildpack: java-buildpack=v3.10-offline-https://github.com/cloudfoundry/java-buildpack.git#193d6b7 java-main open-jdk-like-jre=1.8.0_111 open-jdk-like-memory-calculator=2.0.2_RELEASE spring-auto-reconfiguration=1.10.0_RELEASE

     state     since                    cpu      memory         disk             details
#0   running   2017-03-23 10:50:57 PM   130.8%   376.2M of 1G   165.6M of 512M
#1   down      2017-03-23 10:50:32 PM   0.6%     438.5M of 1G   165.6M of 512M   insufficient resources: memory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的操作中可以看到，连续性特别强，所有的操作都可以在几秒钟内完成，特别适合&lt;strong&gt;微服务&lt;/strong&gt;的部署和&lt;strong&gt;Cloud Native&lt;/strong&gt; APP。&lt;/p&gt;

&lt;p&gt;关于&lt;strong&gt;Pivotal Cloud Foundry&lt;/strong&gt;的更多文档可以访问：&lt;a href=&#34;https://pivotal.io/cn/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/next-steps&#34;&gt;https://pivotal.io/cn/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/next-steps&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</link>
      <pubDate>Thu, 23 Mar 2017 19:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20140810002.jpg&#34; alt=&#34;广州海珠桥&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：广州海珠桥 Aug 10,2014）&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这是我阅读&lt;a href=&#34;caicloud.io&#34;&gt;才云科技&lt;/a&gt;郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在&lt;a href=&#34;http://rootsongjc.github.io/tags/tensorflow-practice-reading-notes/&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;P.S 本书的&lt;strong&gt;官方读者交流微信群（作者也在群里）&lt;/strong&gt;已经超过100人，您可以先加我微信后我拉您进去，我的二维码在&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;这里&lt;/a&gt;，或者直接搜索我的微信号jimmysong。&lt;/p&gt;

&lt;p&gt;睇完这一章后应该就可以自己搭建出一个TensorFlow的环境，我之前在docker里玩过，镜像比较大，下载慢一点，不过用起来很方便，如果你仅仅是想试用一下TensorFlow，看看它能干什么的话，可以直接在docker里试用一下。在Mac上安装的详细步骤，&lt;a href=&#34;https://www.tensorflow.org/install/install_mac&#34;&gt;官方安装说明文档&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;2-1-tensorflow的主要依赖包&#34;&gt;2.1 TensorFlow的主要依赖包&lt;/h2&gt;

&lt;p&gt;TensorFlow主要用到以下两个依赖：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;Protocol buffer&lt;/a&gt;：数据结构化工具。Google开源的结构化数据格式，用于网络传输数据时候的序列化和反序列化，使用的时候需要先定义schema，github地址&lt;a href=&#34;https://github.com/google/protobuf。分布式TensorFlow使用到额gRPC也是使用Protocol&#34;&gt;https://github.com/google/protobuf。分布式TensorFlow使用到额gRPC也是使用Protocol&lt;/a&gt; Buffer来组织的，&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bazel.build/&#34;&gt;Bazel&lt;/a&gt;:自动化编译构建工具。Google开源的，github地址&lt;a href=&#34;https://github.com/bazelbuild/bazel，它支持多语言、多平台、可重复编译和可伸缩，构建大型软件速度也是很快的。Bazel使用**项目空间**的形式管理编译的，每个项目空间需要包含[BUILD文件](https://github.com/tensorflow/tensorflow/blob/master/bower.BUILD)（定义编译目标）和[WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)文件（定义编译的依赖环境）。这两个文件都有点类似python语法。&#34;&gt;https://github.com/bazelbuild/bazel，它支持多语言、多平台、可重复编译和可伸缩，构建大型软件速度也是很快的。Bazel使用**项目空间**的形式管理编译的，每个项目空间需要包含[BUILD文件](https://github.com/tensorflow/tensorflow/blob/master/bower.BUILD)（定义编译目标）和[WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)文件（定义编译的依赖环境）。这两个文件都有点类似python语法。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-2-tensorflow安装&#34;&gt;2.2 TensorFlow安装&lt;/h2&gt;

&lt;p&gt;TensorFlow的安装方式包括docker镜像、pip安装、源码编译安装。&lt;/p&gt;

&lt;p&gt;&lt;u&gt;我选择最方便的docker镜像方式&lt;/u&gt;，其他方式对本地环境做很多配置，折腾起来比较麻烦。&lt;/p&gt;

&lt;p&gt;我早就在docker中安装过TensorFlow0.9小试过牛刀。现在&lt;a href=&#34;https://github.com/tensorflow/tensorflow/releases&#34;&gt;1.0.1版本&lt;/a&gt;已经released了。TensorFlow的所有版本都有对应的docker镜像发布在&lt;a href=&#34;https://hub.docker.com/r/tensorflow/tensorflow/tags/&#34;&gt;docker hub&lt;/a&gt;，可以直接&lt;code&gt;docker pull&lt;/code&gt;安装。&lt;/p&gt;

&lt;p&gt;为了和书中所用的镜像保持统一，我将使用caicloud提供的镜像，基于TensorFlow0.12.0（这个版本是2016年12月20日发布的），他们增加了一些其他机器学习工具包和TensorFlow可视化工具TensorBoard。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;docker镜像方式安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先下载镜像，这个image比较大，下载下来比较费时间，我用了差不多15分钟吧。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull cargo.caicloud.io/tensorflow/tensorflow:0.12.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载下来后我们再check下这个大小为&lt;strong&gt;1.41GB&lt;/strong&gt;镜像的layers。&lt;/p&gt;

&lt;p&gt;另外还有个&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;strong&gt;nvidia&lt;/strong&gt;版本的docker&lt;/a&gt;，可以将你电脑的&lt;strong&gt;GPU&lt;/strong&gt;派山用场，我暂时没用到GPU，我电脑装的是&lt;code&gt;docker17.03-ce&lt;/code&gt;，就不折腾GPU版本的TensorFlow了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
c8a8409297f2        5 weeks ago         /bin/sh -c #(nop)  CMD [&amp;quot;/run_tf.sh&amp;quot;]           0 B                 
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c #(nop) COPY file:78332d36244852...   122 B               
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c #(nop) COPY dir:8b6ab7d235e3975...   21 MB               
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c #(nop) COPY dir:fca915671040399...   360 MB              
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c #(nop) COPY dir:69314aa937be649...   89.9 kB             
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c rm -rf /notebooks/*                  0 B                 
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c pip install caicloud.tensorflow      21.4 MB             
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c pip install -U scikit-learn          39.9 kB             
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get insta...   23.9 MB             
&amp;lt;missing&amp;gt;           5 weeks ago         /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0 B                 
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop)  CMD [&amp;quot;/run_jupyter.sh&amp;quot;]      0 B                 
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop)  WORKDIR /notebooks           0 B                 
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop)  EXPOSE 8888/tcp              0 B                 
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop)  EXPOSE 6006/tcp              0 B                 
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop) COPY file:5485384c641ba7...   733 B               
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop) COPY dir:388d24701b3b5bc...   400 kB              
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop) COPY file:822af972b63c44...   1.06 kB             
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c pip --no-cache-dir install http...   191 MB              
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c pip --no-cache-dir install     ...   379 MB              
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c curl -O https://bootstrap.pypa....   11.4 MB             
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c apt-get update &amp;amp;&amp;amp; apt-get insta...   212 MB              
&amp;lt;missing&amp;gt;           3 months ago        /bin/sh -c #(nop)  MAINTAINER Craig Citro ...   0 B                 
&amp;lt;missing&amp;gt;           9 months ago        /bin/sh -c #(nop) CMD [&amp;quot;/bin/bash&amp;quot;]             0 B                 
&amp;lt;missing&amp;gt;           9 months ago        /bin/sh -c sed -i &#39;s/^#\s*\(deb.*universe\...   1.9 kB              
&amp;lt;missing&amp;gt;           9 months ago        /bin/sh -c rm -rf /var/lib/apt/lists/*          0 B                 
&amp;lt;missing&amp;gt;           9 months ago        /bin/sh -c set -xe   &amp;amp;&amp;amp; echo &#39;#!/bin/sh&#39; &amp;gt;...   195 kB              
&amp;lt;missing&amp;gt;           9 months ago        /bin/sh -c #(nop) ADD file:aca501360d0937b...   188 MB 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到这是一个基于&lt;strong&gt;Ubuntu&lt;/strong&gt;的docker image，这其中还包含了一个&lt;strong&gt;Jupyter notebook&lt;/strong&gt;和一些python packages。&lt;/p&gt;

&lt;p&gt;使用&lt;code&gt;docker image history —no-trunc $IMAGE_ID&lt;/code&gt;命令可以看到每一层的详细信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动TensorFlow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;直接在docker中启动。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run -it -d -p 8888:8888 -p 6006:6006 --name tf-dev cargo.caicloud.io/tensorflow/tensorflow:0.12.0 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动后进入&lt;code&gt;localhost:8888&lt;/code&gt;页面，发现登陆jupyter居然还要输入密码，书中没说要输入密码啊，也没说密码是什么，密码在哪里呢？&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;如何获取Jupyter的登录密码书中没有介绍。其实没必要修改镜像活着进入容器中需钙jupyter的配置，直接查看刚启动的&lt;code&gt;tf-dev&lt;/code&gt;容器的日志即可，里面包含了登录密码。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;docker logs tf-dev
[I 10:52:46.200 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
[W 10:52:46.244 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
[I 10:52:46.267 NotebookApp] Serving notebooks from local directory: /notebooks
[I 10:52:46.267 NotebookApp] 0 active kernels 
[I 10:52:46.267 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/?token=e64afc31eec843717733d6e4527aecf833ce18383214dc47
[I 10:52:46.267 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
Starting TensorBoard 39 on port 6006
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看到了吗，&lt;code&gt;tf-dev&lt;/code&gt;容器的日志输出里就包括了密码，我的容器的jupyter的密码是&lt;strong&gt;token后面的那个字符串&lt;/strong&gt;e64afc31eec843717733d6e4527aecf833ce18383214dc47。&lt;/p&gt;

&lt;p&gt;现在用刚才从日志里看到的密码就可以登录了，Jupyter页面上可以看到本书所有章节的代码了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-practice-chapter2-jupyter-web.jpg&#34; alt=&#34;jupyter页面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;使用pip安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;另外我在mac上也用pip方式安装了。我安装的是最新版的1.0.1的CPU-only，加上&lt;code&gt;—user -U&lt;/code&gt;是为了规避mac上的各种权限问题。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install --upgrade tensorflow --user -U
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载的整个软件包只有39.3MB，速度还是很快的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>容器的应用场景</title>
      <link>http://rootsongjc.github.io/talks/container-applications-scenarios/</link>
      <pubDate>Thu, 23 Mar 2017 15:26:11 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/container-applications-scenarios/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014080101.jpg&#34; alt=&#34;深圳大梅沙&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：深圳大梅沙 Aug 1,2014）&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;如果你对容器到底有什么用存在疑惑的话，推荐你看下我今天碰到的一篇阿里云的&lt;a href=&#34;https://help.aliyun.com/document_detail/25977.html?spm=5176.2020520152.201.2.Oc3baB&#34;&gt;容器服务-产品简介-应用场景&lt;/a&gt;的文章，觉得比较好，把容器的典型应用场景都概括了，容器对于互联网的弹性扩展和微服务架构有很好的应用场景，P.S这里不是在帮阿里云做广告，这里的推荐搭配确实是很多常用的配置选项。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;devops-持续交付&#34;&gt;DevOps 持续交付&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;最优化的持续交付流程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;配合 Jenkins 帮您自动完成从代码提交到应用部署的 DevOps 完整流程，确保只有通过自动测试的代码才能交付和部署，高效替代业内部署复杂、迭代缓慢的传统方式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;能够实现：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DevOps 自动化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实现从代码变更到代码构建，镜像构建和应用部署的全流程自动化。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;环境一致性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;容器技术让您交付的不仅是代码，还有基于不可变架构的运行环境。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持续反馈&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每次集成或交付，都会第一时间将结果实时反馈。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐搭配使用：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;云服务器 ECS + 容器服务&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/25977/cn_zh/1488845113820/%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1---%E6%9E%B6%E6%9E%84%E5%9B%BE---DevOps.png&#34; alt=&#34;1&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;基于高性能计算的机器学习&#34;&gt;基于高性能计算的机器学习&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;专注机器学习本身，快速实现从 0 到 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;帮助数据工程师在 HPC 集群上轻松部署机器学习应用，跟踪试验和训练、发布模型，数据部署在分布式存储，无需关心繁琐部署运维，专注核心业务，快速从 0 到 1。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;能够实现：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;快速弹性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一键部署机器学习应用，秒级启动和弹性伸缩。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;简单可控&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一行配置轻松获取 GPU 计算能力，并且可以监控 GPU 的资源。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;深度整合&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;无缝接入阿里云存储、日志监控和安全基础架构能力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐搭配使用：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;高性能计算 (Alibaba Cloud HPC) + 容器服务 + 阿里云文件存储 NAS + 对象存储 OSS&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/25977/cn_zh/1488845332018/%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1%20-%20%E6%9E%B6%E6%9E%84%E5%9B%BE%20-%20%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97HPC.png&#34; alt=&#34;2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;微服务架构&#34;&gt;微服务架构&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;实现敏捷开发和部署落地，加速企业业务迭代&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;企业生产环境中，通过合理微服务拆分，将每个微服务应用存储在阿里云镜像仓库帮您管理。您只需迭代每个微服务应用，由阿里云提供调度、编排、部署和灰度发布能力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;能够实现：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;负载均衡和服务发现&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持 4 层和 7 层的请求转发和后端绑定。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;丰富的调度和异常恢复策略&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持服务级别的亲和性调度，支持跨可用区的高可用和灾难恢复。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;微服务监控和弹性伸缩&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支持微服务和容器级别的监控，支持微服务的自动伸缩。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐搭配使用：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;云服务器 ECS + 云数据库 RDS 版 + 对象存储 OSS + 容器服务&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/25977/cn_zh/1488846287403/%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1---%E6%9E%B6%E6%9E%84%E5%9B%BE---%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png&#34; alt=&#34;3&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;混合云架构&#34;&gt;混合云架构&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;统一运维多个云端资源&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在容器服务控制台上同时管理云上云下的资源，不需在多中云管理控制台中反复切换。基于容器基础设施无关的特性，使用同一套镜像和编排同时在云上云下部署应用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;能够实现：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在云上伸缩应用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;业务高峰期，在云端快速扩容，把一些业务流量引到云端。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;云上容灾&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;业务系统同时部署到云上和云下，云下提供服务，云上容灾。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;云下开发测试&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;云下开发测试后的应用无缝发布到云上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐搭配使用：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;云服务器 ECS + 专有网络 VPC + 高速通道（ExpressConnect）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/25977/cn_zh/1488846639288/%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1%20-%20%E6%9E%B6%E6%9E%84%E5%9B%BE%20-%20%E6%B7%B7%E5%90%88%E4%BA%91.png&#34; alt=&#34;4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;弹性伸缩架构&#34;&gt;弹性伸缩架构&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;根据业务流量自动对业务扩容/缩容&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;容器服务可以根据业务流量自动对业务扩容/缩容，不需要人工干预，避免流量激增扩容不及时导致系统挂掉，以及平时大量闲置资源造成浪费。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;能够实现：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;快速响应&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;业务流量达到扩容指标，秒级触发容器扩容操作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全自动&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个扩容/缩容过程完全自动化，无需人工干预。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;低成本&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;流量降低自动缩容，避免资源浪费。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐搭配使用：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;云服务器 ECS + 云监控&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/25977/cn_zh/1488846792028/%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1---%E6%9E%B6%E6%9E%84%E5%9B%BE---%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9.png&#34; alt=&#34;5&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第一章深度学习简介</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</link>
      <pubDate>Mon, 20 Mar 2017 22:04:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/tensorflow-book-page.jpg&#34; alt=&#34;tensorflow实战图书封面&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：TensofFlow实战图书封面）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;🙏电子工业出版社编辑赠书，能够这么快的拿到这本书，也🙏&lt;a href=&#34;www.caicloud.io&#34;&gt;才云科技&lt;/a&gt;的郑泽宇大哥耐心的写了这本书，能够让我等小白一窥深度学习的真容。另外要强烈推荐下这本书，这是本TensorFlow深度学习很好的入门书。书中提供的代码&lt;a href=&#34;https://github.com/caicloud/tensorflow-tutorial&#34;&gt;下载地址&lt;/a&gt;，整本书的笔记归档在&lt;a href=&#34;http://rootsongjc.github.io/tags/tensorflow-practice-reading-notes&#34;&gt;这里&lt;/a&gt;。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;P.S 本书的&lt;strong&gt;官方读者交流微信群（作者也在群里）&lt;/strong&gt;已经超过100人，您可以先加我微信后我拉您进去，我的二维码在&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;这里&lt;/a&gt;，或者直接搜索我的微信号jimmysong。&lt;/p&gt;

&lt;h2 id=&#34;1-1-人工智能-机器学习与深度学习&#34;&gt;1.1 人工智能、机器学习与深度学习&lt;/h2&gt;

&lt;p&gt;这一节是讲解三者之间的关系。&lt;/p&gt;

&lt;p&gt;首先以&lt;strong&gt;垃圾邮件分类问题&lt;/strong&gt;引入机器学习的&lt;strong&gt;逻辑回归算法&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;逻辑回归算法的准确性取决于训练数据中的特征的提取，以及训练的数据数量。&lt;/p&gt;

&lt;p&gt;文章中又提了一个从实体中提取特征的例子：通过笛卡尔坐标系活极角坐标系来表示不同颜色的点，看看能否用一条直线划分。这个例子用来说明&lt;strong&gt;一旦解决了数据表达和特征提取，很多人工智能的问题就能迎刃而解&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;深度学习是机器学习的一个分支，除了能够学习特征和任务之间的关联之外，还能&lt;strong&gt;自动从简单特征中提取更加复杂的特征&lt;/strong&gt;，这是其区别于机器学习的关键点。&lt;/p&gt;

&lt;p&gt;总的来说，人工智能&amp;gt;机器学习&amp;gt;深度学习。&lt;/p&gt;

&lt;h2 id=&#34;1-2深度学习的发展历程&#34;&gt;1.2深度学习的发展历程&lt;/h2&gt;

&lt;p&gt;本节介绍了深度网络历史的三个发展阶段。&lt;/p&gt;

&lt;p&gt;2012年的&lt;strong&gt;ImageNet&lt;/strong&gt;图像分类竞赛上，深度学习系统&lt;strong&gt;AlexNet&lt;/strong&gt;赢得冠军，自此深度学习作为深层神经网络的代名词而被人熟知。&lt;/p&gt;

&lt;h2 id=&#34;1-3深度学习的应用&#34;&gt;1.3深度学习的应用&lt;/h2&gt;

&lt;p&gt;这一节讲的是深度学习的应用，首先还是从ImageNet的图像识别开始，应用到了OCR（提到了卷积神经网络）、语音识别（提到了混合搞高斯模型）、自然语言处理（提到了语料库、单词向量、机器翻译、情感分析）、人机对弈（提到了AlphaGO）。&lt;/p&gt;

&lt;h2 id=&#34;1-4-深度学习工具介绍与对比&#34;&gt;1.4 深度学习工具介绍与对比&lt;/h2&gt;

&lt;p&gt;TensorFlow的渊源是Google大脑团队在2011年开发，在内部使用的&lt;strong&gt;DistBelief&lt;/strong&gt;，并赢得了ImageNet 2014年的比赛，TF是其开源版本，还发表了一篇论文&lt;code&gt;TensorFlow: Large-Scale Machine Learning on Heteogeneous Distributed systems&lt;/code&gt;，这就跟当年的&lt;strong&gt;HDFS&lt;/strong&gt;、&lt;strong&gt;MapReduce&lt;/strong&gt;一个套路啊。&lt;/p&gt;

&lt;p&gt;Google还把它用来做&lt;strong&gt;RankBrain&lt;/strong&gt;和很多其他的产品线上使用。&lt;/p&gt;

&lt;p&gt;当然，还有很多其他的深度学习工具，比如&lt;strong&gt;Caffe&lt;/strong&gt;、&lt;strong&gt;Deeplearning4j&lt;/strong&gt;、&lt;strong&gt;Torch&lt;/strong&gt;等不一而足。从各种指标来看，TensorFlow都是目前最受关注的深度学习框架。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>