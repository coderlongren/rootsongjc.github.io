<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/blogs/</link>
    <description>Recent content in Blogs on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jun 2017 20:52:57 +0800</lastBuildDate>
    
	<atom:link href="http://rootsongjc.github.io/blogs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用Jenkins进行持续构建与发布应用到kubernetes集群中</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-jenkins-ci-cd/</link>
      <pubDate>Tue, 27 Jun 2017 20:52:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-jenkins-ci-cd/</guid>
      <description>本文已归档到kubernetes-handbook中的【最佳实践—使用Jenkins进行持续构建与发布】章节中，一切内容以kubernetes-handbook中稳准。
我们基于Jenkins的CI/CD流程如下所示。
流程说明 应用构建和发布流程说明。
 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数等，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置 Jenkins调用kubernetes的API，部署应用  关于应用的更新、滚动升级、灰度发布请留意博客中的后续文章或关注kubernetes-handbook的更新。</description>
    </item>
    
    <item>
      <title>云原生微服务治理框架Linkerd简介</title>
      <link>http://rootsongjc.github.io/blogs/linkerd-introduction/</link>
      <pubDate>Mon, 26 Jun 2017 21:02:13 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/linkerd-introduction/</guid>
      <description>（题图：青岛 May 26,2017）
前言 Linkerd是一个用于云原生应用的开源、可扩展的service mesh（一般翻译成服务网格，还有一种说法叫”服务啮合层“，见Istio：用于微服务的服务啮合层）。同时，Linkerd也是CNCF（云原生计算基金会）中的组件之一。
P.S 本文已归档到kubernetes-handbook中的【领域应用—微服务架构】章节中。
Linkerd是什么 Linkerd的出现是为了解决像twitter、google这类超大规模生产系统的复杂性问题。Linkerd不是通过控制服务之间的通信机制来解决这个问题，而是通过在服务实例之上添加一个抽象层来解决的。
Linkerd负责跨服务通信中最困难、易出错的部分，包括延迟感知、负载均衡、连接池、TLS、仪表盘、请求路由等——这些都会影响应用程序伸缩性、性能和弹性。
如何运行 Linkerd作为独立代理运行，无需特定的语言和库支持。应用程序通常会在已知位置运行linkerd实例，然后通过这些实例代理服务调用——即不是直接连接到目标服务，服务连接到它们对应的linkerd实例，并将它们视为目标服务。
在该层上，linkerd应用路由规则，与现有服务发现机制通信，对目标实例做负载均衡——与此同时调整通信并报告指标。
通过延迟调用linkerd的机制，应用程序代码与以下内容解耦：
 生产拓扑 服务发现机制 负载均衡和连接管理逻辑  应用程序也将从一致的全局流量控制系统中受益。这对于多语言应用程序尤其重要，因为通过库来实现这种一致性是非常困难的。
Linkerd实例可以作为sidecar（既为每个应用实体或每个主机部署一个实例）来运行。 由于linkerd实例是无状态和独立的，因此它们可以轻松适应现有的部署拓扑。它们可以与各种配置的应用程序代码一起部署，并且基本不需要去协调它们。
参考 Buoyant发布服务网格Linkerd的1.0版本
Linkerd documentation
Istio：用于微服务的服务啮合层</description>
    </item>
    
    <item>
      <title>Kubernetes Pod Cheat Sheet——Pod数据结构参考图</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-pod-cheetsheet/</link>
      <pubDate>Sat, 24 Jun 2017 14:20:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-pod-cheetsheet/</guid>
      <description>昨天晚上构思，今天花了一上午的时间翻阅了下kubernetes api reference，画了一个Kubernetes Pod Cheat Sheet。
从Pod的数据结构和API入手，管中窥豹，可见一斑。通过该图基本可以对kubernetes中这个最基本的object——Pod的功能和配置有一个感性的认识了，也许具体的某个组件的实现你不了解，但是从high level的视角来看待Pod整体有助于今后深入研究某个feature。
该图是根据kubernetes 1.6版本的Pod v1 core API绘制。
图片归档在kubernetes-handbook,请以GitHub中的图片为准。
注：在移动设备上打开该链接后会提示图片太大无法查看请选择Desktop version方可查看原图。
今后我将陆续推出其他object的cheat sheet，敬请关注。</description>
    </item>
    
    <item>
      <title>使用API blueprint创建API文档</title>
      <link>http://rootsongjc.github.io/blogs/creating-api-document-with-api-blueprint/</link>
      <pubDate>Fri, 23 Jun 2017 12:24:12 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/creating-api-document-with-api-blueprint/</guid>
      <description>在进行微服务开发的过程中，为了保证最终开发的系统跟最初的设计保持一致，约定RESTful接口之间的调用方法，我们需要将API设计文档化，因此我们引入了API Blueprint。
API Blueprint 是什么 API Blueprint 用来编写API文档的一种标记语言，类似于Markdown，因为是纯文本的，所以方便共享编辑，具体的语法规则可以在 API Blueprint documentation 查看，配合一些开源的工具可以把接口文档渲染成 html 再搭配一个静态服务器，方便于分享。
另外，配合一些工具，可以直接生成一个 mock data 数据，这样只要和后端的同学约定好接口格式，那么前端再开发的时候可以使用 mock data 数据来做测试，等到后端写好接口之后再做联调就可以了。
我们以Cloud Native Go书中的gogo-service示例里的apiary.apib文件为例。
该文件实际上是一个Markdown格式的文件，Github中原生支持该文件，使用Typora打开后是这样子。
在Visual Studio Code中有个API Element extension对于API Blueprint文件的支持也比较好。
生成静态页面和进行冒烟测试 我们分别使用开源的aglio和drakov来生成静态页面和进行冒烟测试。
aglio 是一个可以根据 api-blueprint 的文档生成静态 HTML 页面的工具。
其生成的工具不是简单的 markdown 到 html 的转换, 而是可以生成类似 rdoc 这样的拥有特定格式风格的查询文档。
在本地安装有node环境的情况下，使用下面的命令安装和使用aglio。
$ npm install -g aglio $ aglio -i apiary.apib -o api.html  打开api.html文件后，如图：
安装和使用drakov。
$ npm install -g drakov $ drakov -f apiary.</description>
    </item>
    
    <item>
      <title>使用Wercker进行持续构建与发布</title>
      <link>http://rootsongjc.github.io/blogs/continuous-integration-with-wercker/</link>
      <pubDate>Thu, 22 Jun 2017 18:08:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/continuous-integration-with-wercker/</guid>
      <description>本文介绍了wercker和它的基本用法，并用我GitHub上的magpie应用作为示例，讲解如何给GitHub项目增加wercker构建流程，并将生成的镜像自动上传到Docker Hub上。
注：本文参考了Cloud Native Go书中的”持续交付“章节。
CI工具 开源项目的构建离不开CI工具，你可能经常会在很多GitHub的开源项目首页上看到这样的东西：
这些图标都是CI工具提供的，可以直观的看到当前的构建状态，例如wercker中可以在Application-magpie-options中看到：
将文本框中的代码复制到你的项目的README文件中，就可以在项目主页上看到这样的标志了。
现在市面上有很多流行的CI/CD工具和DevOps工具有很多，这些工具提高了软件开发的效率，增加了开发人员的幸福感。这些工具有：
适用于GitHub上的开源项目，可以直接使用GitHub账户登陆，对于公开项目可以直接使用：Travis-ci、CircleCI、Wercker。从目前GitHub上开源项目的使用情况来看，Travis-ci的使用率更高一些。
适用于企业级的：Jenkins
不仅包括CI/CD功能的DevOps平台：JFrog、Spinnaker、Fabric8
Wercker简介 Wercker是一家为现代云服务提供容器化应用及微服务的快速开发、部署工具的初创企业，成立于2012年，总部位于荷兰阿姆斯特丹。其以容器为中心的平台可以对微服务和应用的开发进行自动化。开发者通过利用其命令行工具能够生成容器到桌面，然后自动生成应用并部署到各种云平台上面。其支持的平台包括Heroku、AWS以及Rackspace等。
Wercker于2016年获得450万美元A轮融资，此轮融资由Inkef Capital领投，Notion Capital跟投，融资所得将用于商业版产品的开发。此轮融资过后其总融资额为750万美元。
Wercker于2017年4月被Oracle甲骨文于收购。
为什么使用Wercker 所有的CI工具都可以在市面上获取，但为何要建议使用Wercker呢？依据云之道的准则评估了所有工具，发现Wercker正是我们需要的。
首先，无须在工作站中安装Wecker，仅安装一个命令行客户端即可，构建过程全部在云端进行。
其次，不用通过信用卡就可使用Wercker。当我们迫切希望简化流程时，这是一件令人赞叹的事。付款承诺这一条件大大增加了开发者的压力，这通常是不必要的。
最后，Wercker使用起来非常简单。它非常容易配置，不需要经过高级培训或拥有持续集成的博士学位，也不用制定专门的流程。
通过Wercker搭建CI环境只需经过三个基本步骤。
1．在Wercker网站中创建一个应用程序。
2．将wercker.yml添加到应用程序的代码库中。
3．选择打包和部署构建的位置。
如何使用 可以使用GitHub帐号直接登录Wercker，整个创建应用CI的流程一共3步。
一旦拥有了账户，那么只需简单地点击位于顶部的应用程序菜单，然后选择创建选项即可。如果系统提示是否要创建组织或应用程序，请选择应用程序。Wercker组织允许多个Wercker用户之间进行协作，而无须提供信用卡。下图为设置新应用程序的向导页面。
选择了GitHub中的repo之后，第二步配置访问权限，最后一步Wercker会尝试生成一个wercker.yml文件（后面会讨论）。不过至少对于Go应用程序来说，这个配置很少会满足要求，所以我们总是需要创建自己的Wercker配置文件。
安装Wercker命令行程序 这一步是可选的，如果你希望在本地进行wercker构建的话才需要在本地安装命令行程序。本地构建和云端构建都依赖于Docker的使用。基本上，代码会被置于所选择的docker镜像中（在wercker.yml中定义），然后再选择执行的内容和方法。
要在本地运行Wercker构建，需要使用Wercker CLI。有关如何安装和测试CLI的内容，请查看http://devcenter.wercker.com/docs/cli。Wercker更新文档的频率要比本书更高，所以请在本书中做个标记，然后根据Wercker网站的文档安装Wercker CLI。
如果已经正确安装了CLI，应该可以查询到CLI的版本，代码如下所示。
Version: 1.0.882 Compiled at: 2017-06-02 06:49:39 +0800 CST Git commit: da8bc056ed99e27b4b7a1b608078ddaf025a9dc4 No new version available  本地构建只要在项目的根目录下输入wercker build命令即可，wercker会自动下载依赖的docker镜像在本地运行所有构建流程。
创建Wercker配置文件wercker.yml Wercker配置文件是一个YAML文件，该文件必须在GitHub repo的最顶层目录，该文件主要包含三个部分，对应可用的三个主要管道。
 Dev：定义了开发管道的步骤列表。与所有管道一样，可以选定一个box用于构建，也可以全局指定一个box应用于所有管道。box可以是Wercker内置的预制Docker镜像之一，也可以是Docker Hub托管的任何Docker镜像。
 Build：定义了在Wercker构建期间要执行的步骤和脚本的列表。与许多其他服务（如Jenkins和TeamCity）不同，构建步骤位于代码库的配置文件中，而不是隐藏在服务配置里。
 Deploy：在这里可以定义构建的部署方式和位置。
Wercker中还有工作流的概念，通过使用分支、条件构建、多个部署目标和其他高级功能扩展了管道的功能，这些高级功能读着可以自己在wercker的网站中探索。
示例 我们以我用Go语言开发的管理yarn on docker集群的命令行工具magpie为例，讲解如何使用wercker自动构建，并产生docker镜像发布到Docker Hub中。</description>
    </item>
    
    <item>
      <title>kubernetes client-go包使用示例</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-client-go-sample/</link>
      <pubDate>Wed, 21 Jun 2017 19:23:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-client-go-sample/</guid>
      <description>(题图：青岛栈桥 May 26,2017)
前言 本文将归档到kubernetes-handbook的【开发指南—client-go示例】章节中，最终版本以kubernetes-handbook中为准。
本文中的代码见：https://github.com/rootsongjc/kubernetes-client-go-sample
client-go示例 访问kubernetes集群有几下几种方式：
   方式 特点 支持者     Kubernetes dashboard 直接通过Web UI进行操作，简单直接，可定制化程度低 官方支持   kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持   client-go 从kubernetes的代码中抽离出来的客户端包，简单易用，但需要小心区分kubernetes的API版本 官方支持   client-python python客户端，kubernetes-incubator 官方支持   Java client fabric8中的一部分，kubernetes的java客户端 redhat    下面，我们基于client-go，对Deployment升级镜像的步骤进行了定制，通过命令行传递一个Deployment的名字、应用容器名和新image名字的方式来升级。代码和使用方式见 https://github.com/rootsongjc/kubernetes-client-go-sample 。
kubernetes-client-go-sample 代码如下：
package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;os&amp;quot; &amp;quot;path/filepath&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/api/errors&amp;quot; metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot; &amp;quot;k8s.io/client-go/kubernetes&amp;quot; &amp;quot;k8s.io/client-go/tools/clientcmd&amp;quot; ) func main() { var kubeconfig *string if home := homeDir(); home !</description>
    </item>
    
    <item>
      <title>kubernetes管理的容器命名规则解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/</link>
      <pubDate>Thu, 15 Jun 2017 21:52:38 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/</guid>
      <description>本文将归档到kubernetes-handbook的【运维管理-监控】章节中，最终版本以kubernetes-handbook中为准。
当我们通过cAdvisor获取到了容器的信息后，例如访问${NODE_IP}:4194/api/v1.3/docker获取的json结果中的某个容器包含如下字段：
&amp;quot;labels&amp;quot;: { &amp;quot;annotation.io.kubernetes.container.hash&amp;quot;: &amp;quot;f47f0602&amp;quot;, &amp;quot;annotation.io.kubernetes.container.ports&amp;quot;: &amp;quot;[{\&amp;quot;containerPort\&amp;quot;:80,\&amp;quot;protocol\&amp;quot;:\&amp;quot;TCP\&amp;quot;}]&amp;quot;, &amp;quot;annotation.io.kubernetes.container.restartCount&amp;quot;: &amp;quot;0&amp;quot;, &amp;quot;annotation.io.kubernetes.container.terminationMessagePath&amp;quot;: &amp;quot;/dev/termination-log&amp;quot;, &amp;quot;annotation.io.kubernetes.container.terminationMessagePolicy&amp;quot;: &amp;quot;File&amp;quot;, &amp;quot;annotation.io.kubernetes.pod.terminationGracePeriod&amp;quot;: &amp;quot;30&amp;quot;, &amp;quot;io.kubernetes.container.logpath&amp;quot;: &amp;quot;/var/log/pods/d8a2e995-3617-11e7-a4b0-ecf4bbe5d414/php-redis_0.log&amp;quot;, &amp;quot;io.kubernetes.container.name&amp;quot;: &amp;quot;php-redis&amp;quot;, &amp;quot;io.kubernetes.docker.type&amp;quot;: &amp;quot;container&amp;quot;, &amp;quot;io.kubernetes.pod.name&amp;quot;: &amp;quot;frontend-2337258262-771lz&amp;quot;, &amp;quot;io.kubernetes.pod.namespace&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;io.kubernetes.pod.uid&amp;quot;: &amp;quot;d8a2e995-3617-11e7-a4b0-ecf4bbe5d414&amp;quot;, &amp;quot;io.kubernetes.sandbox.id&amp;quot;: &amp;quot;843a0f018c0cef2a5451434713ea3f409f0debc2101d2264227e814ca0745677&amp;quot; },  这些信息其实都是kubernetes创建容器时给docker container打的Labels。
你是否想过这些label跟容器的名字有什么关系？当你在node节点上执行docker ps看到的容器名字又对应哪个应用的Pod呢？
在kubernetes代码中pkg/kubelet/dockertools/docker.go中的BuildDockerName方法定义了容器的名称规范。
这段容器名称定义代码如下：
// Creates a name which can be reversed to identify both full pod name and container name. // This function returns stable name, unique name and a unique id. // Although rand.Uint32() is not really unique, but it&#39;s enough for us because error will // only occur when instances of the same container in the same pod have the same UID.</description>
    </item>
    
    <item>
      <title>Kubernetes配置最佳实践</title>
      <link>http://rootsongjc.github.io/blogs/configuration-best-practice/</link>
      <pubDate>Wed, 14 Jun 2017 20:03:09 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/configuration-best-practice/</guid>
      <description>（题图：青岛 May 26,2017）
前言 本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提Pull Request。
本文已上传到kubernetes-handbook中的第四章最佳实践章节，本文仅作归档，更新以kubernetes-handbook为准。
通用配置建议  定义配置文件的时候，指定最新的稳定API版本（目前是V1）。 在配置文件push到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。 使用YAML格式而不是JSON格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是YAML格式比起JSON更易读和配置。 尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考guestbook-all-in-one.yaml文件中的配置（注意，尽管你可以在使用kubectl命令时指定配置文件目录，你也可以在配置文件目录下执行kubectl create——查看下面的详细信息）。 为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉ReplicationController的selector和label，如果你希望它们跟podTemplate中的label一样的话，因为那些配置默认是podTemplate的label产生的。更多信息请查看 guestbook app 的yaml文件和 examples 。 将资源对象的描述放在一个annotation中可以更好的内省。  裸奔的Pods vs Replication Controllers和 Jobs  如果有其他方式替代“裸奔的pod”（如没有绑定到replication controller 上的pod），那么就使用其他选择。在node节点出现故障时，裸奔的pod不会被重新调度。Replication Controller总是会重新创建pod，除了明确指定了restartPolicy: Never 的场景。Job 也许是比较合适的选择。  Services  通常最好在创建相关的replication controllers之前先创建service（没有这个必要吧？）你也可以在创建Replication Controller的时候不指定replica数量（默认是1），创建service后，在通过Replication Controller来扩容。这样可以在扩容很多个replica之前先确认pod是正常的。 除非时分必要的情况下（如运行一个node daemon），不要使用hostPort（用来指定暴露在主机上的端口号）。当你给Pod绑定了一个hostPort，该pod可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 kubectl proxy and apiserver proxy 或者 kubectl port-forward。你可使用 Service 来对外暴露服务。如果你确实需要将pod的端口暴露到主机上，考虑使用 NodePort service。 跟hostPort一样的原因，避免使用 hostNetwork。 如果你不需要kube-proxy的负载均衡的话，可以考虑使用使用headless services。  使用Label  定义 labels 来指定应用或Deployment的 semantic attributes 。例如，不是将label附加到一组pod来显式表示某些服务（例如，service:myservice），或者显式地表示管理pod的replication controller（例如，controller:mycontroller），附加label应该是标示语义属性的标签， 例如{app:myapp,tier:frontend,phase:test,deployment:v3}。 这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod的服务或app是“myapp”的所有“测试”阶段组件。 有关此方法的示例，请参阅guestbook应用程序。  可以通过简单地从其service的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配replication controller的选择器，来实现跨越多个部署的服务，例如滚动更新。</description>
    </item>
    
    <item>
      <title>微服务管理框架Istio简介</title>
      <link>http://rootsongjc.github.io/blogs/istio-overview/</link>
      <pubDate>Fri, 02 Jun 2017 11:27:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/istio-overview/</guid>
      <description>（题图：威海朱口 May 29,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
Istio是由Google、IBM和Lyft开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。
简介 使用istio可以很简单的创建具有负载均衡、服务间认证、监控等功能的服务网络，而不需要对服务的代码进行任何修改。你只需要在部署环境中，例如Kubernetes的pod里注入一个特别的sidecar proxy来增加对istio的支持，用来截获微服务之间的网络流量。
目前版本的istio只支持kubernetes，未来计划支持其他其他环境。
特性 使用istio的进行微服务管理有如下特性：
 流量管理：控制服务间的流量和API调用流，使调用更可靠，增强不同环境下的网络鲁棒性。 可观测性：了解服务之间的依赖关系和它们之间的性质和流量，提供快速识别定位问题的能力。 策略实施：通过配置mesh而不是以改变代码的方式来控制服务之间的访问策略。 服务识别和安全：提供在mesh里的服务可识别性和安全性保护。  未来将支持多种平台，不论是kubernetes、Mesos、还是云。同时可以集成已有的ACL、日志、监控、配额、审计等。
架构 Istio架构分为控制层和数据层。
 数据层：由一组智能代理（Envoy）作为sidecar部署，协调和控制所有microservices之间的网络通信。 控制层：负责管理和配置代理路由流量，以及在运行时执行的政策。  Envoy Istio使用Envoy代理的扩展版本，该代理是以C++开发的高性能代理，用于调解service mesh中所有服务的所有入站和出站流量。 Istio利用了Envoy的许多内置功能，例如动态服务发现，负载平衡，TLS终止，HTTP/2＆gRPC代理，断路器，运行状况检查，基于百分比的流量拆分分阶段上线，故障注入和丰富指标。
Envoy在kubernetes中作为pod的sidecar来部署。 这允许Istio将大量关于流量行为的信号作为属性提取出来，这些属性又可以在Mixer中用于执行策略决策，并发送给监控系统以提供有关整个mesh的行为的信息。 Sidecar代理模型还允许你将Istio功能添加到现有部署中，无需重新构建或重写代码。 更多信息参见设计目标。
Mixer Mixer负责在service mesh上执行访问控制和使用策略，并收集Envoy代理和其他服务的遥测数据。代理提取请求级属性，发送到mixer进行评估。有关此属性提取和策略评估的更多信息，请参见Mixer配置。 混音器包括一个灵活的插件模型，使其能够与各种主机环境和基础架构后端进行接口，从这些细节中抽象出Envoy代理和Istio管理的服务。
Istio Manager Istio-Manager用作用户和Istio之间的接口，收集和验证配置，并将其传播到各种Istio组件。它从Mixer和Envoy中抽取环境特定的实现细节，为他们提供独立于底层平台的用户服务的抽象表示。 此外，流量管理规则（即通用4层规则和七层HTTP/gRPC路由规则）可以在运行时通过Istio-Manager进行编程。
Istio-auth Istio-Auth提供强大的服务间和最终用户认证，使用相互TLS，内置身份和凭据管理。它可用于升级service mesh中的未加密流量，并为运营商提供基于服务身份而不是网络控制的策略的能力。 Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括属性和基于角色的访问控制以及授权hook）来控制和监控访问你服务、API或资源的人员。
参考 Istio开源平台发布，Google、IBM和Lyft分别承担什么角色？
Istio：用于微服务的服务啮合层
Istio Overview</description>
    </item>
    
    <item>
      <title>微服务管理框架istio安装笔记</title>
      <link>http://rootsongjc.github.io/blogs/istio-installation/</link>
      <pubDate>Thu, 01 Jun 2017 20:18:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/istio-installation/</guid>
      <description>（题图：威海东部海湾 May 28,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
本文根据官网的文档整理而成，步骤包括安装istio 0.1.5并创建一个bookinfo的微服务来测试istio的功能。
文中使用的yaml文件可以在kubernetes-handbook的manifests/istio目录中找到，所有的镜像都换成了我的私有镜像仓库地址，请根据官网的镜像自行修改。
安装环境  CentOS 7.3.1611 Docker 1.12.6 Kubernetes 1.6.0  安装 1.下载安装包
下载地址：https://github.com/istio/istio/releases
下载Linux版本的当前最新版安装包
wget https://github.com/istio/istio/releases/download/0.1.5/istio-0.1.5-linux.tar.gz  2.解压
解压后，得到的目录结构如下：
. ├── bin │ └── istioctl ├── install │ └── kubernetes │ ├── addons │ │ ├── grafana.yaml │ │ ├── prometheus.yaml │ │ ├── servicegraph.yaml │ │ └── zipkin.yaml │ ├── istio-auth.yaml │ ├── istio-rbac-alpha.yaml │ ├── istio-rbac-beta.yaml │ ├── istio.yaml │ ├── README.</description>
    </item>
    
    <item>
      <title>使用filebeat收集kubernetes中的应用日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-filebeat/</link>
      <pubDate>Wed, 17 May 2017 17:24:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-filebeat/</guid>
      <description>（题图：民生现代美术馆 May 14,2017）
前言 本文已同步更新到Github仓库kubernetes-handbook中。
昨天写了篇文章使用Logstash收集Kubernetes的应用日志，发现logstash十分消耗内存（大约500M），经人提醒改用filebeat（大约消耗10几M内存），因此重写一篇使用filebeat收集kubernetes中的应用日志。
在进行日志收集的过程中，我们首先想到的是使用Logstash，因为它是ELK stack中的重要成员，但是在测试过程中发现，Logstash是基于JDK的，在没有产生日志的情况单纯启动Logstash就大概要消耗500M内存，在每个Pod中都启动一个日志收集组件的情况下，使用logstash有点浪费系统资源，经人推荐我们选择使用Filebeat替代，经测试单独启动Filebeat容器大约会消耗12M内存，比起logstash相当轻量级。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash，现使用filebeat替代 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/filebeat:5.4.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件fielbeat-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: filebeat-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: filebeat-test spec: containers: - image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>使用Logstash收集Kubernetes的应用日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-logstash/</link>
      <pubDate>Tue, 16 May 2017 17:46:15 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-logstash/</guid>
      <description>（题图：798艺术区 May 14,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
很多企业内部都有自己的ElasticSearch集群，我们没有必要在kubernetes集群内部再部署一个，而且这样还难于管理，因此我们考虑在容器里部署logstash收集日志到已有的ElasticSearch集群中。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/logstash:5.3.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件logstash-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: logstash-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: logstash-test spec: containers: - image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>Kubernete概念解析之Deployment</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/</link>
      <pubDate>Sat, 13 May 2017 00:46:37 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/</guid>
      <description>（题图：京广桥@北京国贸 Apr 30,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
本文翻译自kubernetes官方文档：https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/controllers/deployment.md
本文章根据2017年5月10日的Commit 8481c02翻译。
Deployment是Kubernetes中的一个非常重要的概念，从它开始是了解kubernetes中资源概念的一个很好的切入点，看到网上也没什么详细的说明文档，我就随手翻译了一下官方文档（Github中的文档），kubernetes官网上的文档还没有这个新。这篇文章对Deployment的概念解释的面面俱到十分详尽。
Deployment是什么？ Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。
你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。
一个典型的用例如下：
 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清楚旧的不必要的ReplicaSet。  创建Deployment 下面是一个Deployment示例，它创建了一个Replica Set来启动3个nginx pod。
下载示例文件并执行命令：
$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record deployment &amp;quot;nginx-deployment&amp;quot; created  将kubectl的 —record 的flag设置为 true可以在annotation中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个Deployment revision中执行了哪些命令。
然后立即执行getí将获得如下结果：
$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s  输出结果表明我们希望的repalica数是3（根据deployment中的.spec.replicas配置）当前replica数（ .status.replicas）是0, 最新的replica数（.status.updatedReplicas）是0，可用的replica数（.status.availableReplicas）是0。</description>
    </item>
    
    <item>
      <title>Kubernetes中的Rolling Update服务滚动升级</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/</link>
      <pubDate>Wed, 10 May 2017 17:14:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/</guid>
      <description>（题图：后海夜色 Apr 30,2017）
前言 本文已同步到gitbook kubernetes-handbook的第8章第1节。
本文说明在Kubernetes1.6中服务如何滚动升级，并对其进行测试。
当有镜像发布新版本，新版本服务上线时如何实现服务的滚动和平滑升级？
如果你使用ReplicationController创建的pod可以使用kubectl rollingupdate命令滚动升级，如果使用的是Deployment创建的Pod可以直接修改yaml文件后执行kubectl apply即可。
Deployment已经内置了RollingUpdate strategy，因此不用再调用kubectl rollingupdate命令，升级的过程是先创建新版的pod将流量导入到新pod上后销毁原来的旧的pod。
Rolling Update适用于Deployment、Replication Controller，官方推荐使用Deployment而不再使用Replication Controller。
使用ReplicationController时的滚动升级请参考官网说明：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/
ReplicationController与Deployment的关系 ReplicationController和Deployment的RollingUpdate命令有些不同，但是实现的机制是一样的，关于这两个kind的关系我引用了ReplicationController与Deployment的区别中的部分内容如下，详细区别请查看原文。
ReplicationController Replication Controller为Kubernetes的一个核心内容，应用托管到Kubernetes之后，需要保证应用能够持续的运行，Replication Controller就是这个保证的key，主要的功能如下：
 确保pod数量：它会确保Kubernetes中有指定数量的Pod在运行。如果少于指定数量的pod，Replication Controller会创建新的，反之则会删除掉多余的以保证Pod数量不变。 确保pod健康：当pod不健康，运行出错或者无法提供服务时，Replication Controller也会杀死不健康的pod，重新创建新的。 弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过Replication Controller动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取Replication Controller关联pod的整体资源使用情况，做到自动伸缩。 滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。  Deployment Deployment同样为Kubernetes的一个核心内容，主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：
 Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。 事件和状态查看：可以查看Deployment的升级详细进度和状态。 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动：对于每一次升级，都能够随时暂停和启动。 多种升级方案：Recreate：删除所有已存在的pod,重新创建新的; RollingUpdate：滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。  创建测试镜像 我们来创建一个特别简单的web服务，当你访问网页时，将输出一句版本信息。通过区分这句版本信息输出我们就可以断定升级是否完成。
所有配置和代码见Github上的manifests/test/rolling-update-test目录。
Web服务的代码main.go
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; ) func sayhello(w http.</description>
    </item>
    
    <item>
      <title>Kubernetes的边缘节点配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</link>
      <pubDate>Tue, 09 May 2017 12:59:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</guid>
      <description>（题图：南屏晚钟@圆明园 May 6,2017）
前言 为了配置kubernetes中的traefik ingress的高可用，对于kubernetes集群以外只暴露一个访问入口，需要使用keepalived排除单点问题。本文参考了kube-keepalived-vip，但并没有使用容器方式安装，而是直接在node节点上安装。
本文已同步到gitbook kubernetes-handbook的第2章第5节。
定义 首先解释下什么叫边缘节点（Edge Node），所谓的边缘节点即集群内部用来向集群外暴露服务能力的节点，集群外部的服务通过该节点来调用集群内部的服务，边缘节点是集群内外交流的一个Endpoint。
边缘节点要考虑两个问题
 边缘节点的高可用，不能有单点故障，否则整个kubernetes集群将不可用 对外的一致暴露端口，即只能有一个外网访问IP和端口  架构 为了满足边缘节点的以上需求，我们使用keepalived来实现。
在Kubernetes集群外部配置nginx来访问边缘节点的VIP。
选择Kubernetes的三个node作为边缘节点，并安装keepalived。
准备 复用kubernetes测试集群的三台主机。
172.20.0.113
172.20.0.114
172.20.0.115
安装 使用keepalived管理VIP，VIP是使用IPVS创建的，IPVS已经成为linux内核的模块，不需要安装
LVS的工作原理请参考：http://www.cnblogs.com/codebean/archive/2011/07/25/2116043.html
不使用镜像方式安装了，直接手动安装，指定三个节点为边缘节点（Edge node）。
因为我们的测试集群一共只有三个node，所有在在三个node上都要安装keepalived和ipvsadmin。
yum install keepalived ipvsadm  配置说明 需要对原先的traefik ingress进行改造，从以Deployment方式启动改成DeamonSet。还需要指定一个与node在同一网段的IP地址作为VIP，我们指定成172.20.0.119，配置keepalived前需要先保证这个IP没有被分配。。
 Traefik以DaemonSet的方式启动 通过nodeSelector选择边缘节点 通过hostPort暴露端口 当前VIP漂移到了172.20.0.115上 Traefik根据访问的host和path配置，将流量转发到相应的service上  配置keepalived 参考基于keepalived 实现VIP转移，lvs，nginx的高可用，配置keepalived。
keepalived的官方配置文档见：http://keepalived.org/pdf/UserGuide.pdf
配置文件/etc/keepalived/keepalived.conf文件内容如下：
! Configuration File for keepalived global_defs { notification_email { root@localhost } notification_email_from kaadmin@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 172.</description>
    </item>
    
    <item>
      <title>在kubernetes中使用glusterfs做持久化存储</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</link>
      <pubDate>Thu, 04 May 2017 20:06:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</guid>
      <description>（题图：无题@北京奥林匹克森林公园 May 1,2017）
前言 本文章已同步到kubernetes-handbook 7.1章节。
Kubernetes集群沿用跟我一起部署kubernetes1.6集群中的三台机器。
我们复用kubernetes集群的这三台主机做glusterfs存储。
以下步骤参考自：https://www.xf80.com/2017/04/21/kubernetes-glusterfs/
安装glusterfs 我们直接在物理机上使用yum安装，如果你选择在kubernetes上安装，请参考：https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md
# 先安装 gluster 源 $ yum install centos-release-gluster -y # 安装 glusterfs 组件 $ yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel ## 创建 glusterfs 目录 $ mkdir /opt/glusterd ## 修改 glusterd 目录 $ sed -i &#39;s/var\/lib/opt/g&#39; /etc/glusterfs/glusterd.vol # 启动 glusterfs $ systemctl start glusterd.service # 设置开机启动 $ systemctl enable glusterd.service #查看状态 $ systemctl status glusterd.service  配置 glusterfs # 配置 hosts $ vi /etc/hosts 172.</description>
    </item>
    
    <item>
      <title>Kubernetes网络和集群性能测试</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</link>
      <pubDate>Tue, 25 Apr 2017 22:14:49 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</guid>
      <description>（题图：无题@安贞门 Jun 18,2016）
前言 该测试是为了测试在不同的场景下，访问kubernetes的延迟以及kubernetes的性能。进行以下测试前，你需要有一个部署好的kubernetes集群，关于如何部署kuberentes1.6集群，请参考kubernetes-handbook。
准备 测试环境
在以下几种环境下进行测试：
 Kubernetes集群node节点上通过Cluster IP方式访问 Kubernetes集群内部通过service访问 Kubernetes集群外部通过traefik ingress暴露的地址访问  测试地址
Cluster IP: 10.254.149.31
Service Port：8000
Ingress Host：traefik.sample-webapp.io
测试工具
 Locust：一个简单易用的用户负载测试工具，用来测试web或其他系统能够同时处理的并发用户数。 curl kubemark 测试程序：sample-webapp，源码见Github kubernetes的分布式负载测试  测试说明
通过向sample-webapp发送curl请求获取响应时间，直接curl后的结果为：
$ curl &amp;quot;http://10.254.149.31:8000/&amp;quot; Welcome to the &amp;quot;Distributed Load Testing Using Kubernetes&amp;quot; sample web app  网络延迟测试 场景一、 Kubernetes集群node节点上通过Cluster IP访问 测试命令
curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://10.254.149.31:8000/&amp;quot;  10组测试结果
   No time_connect time_starttransfer time_total     1 0.</description>
    </item>
    
    <item>
      <title>运用kubernetes进行分布式负载测试</title>
      <link>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 21:32:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</guid>
      <description>（题图：Kubrick Book Store Mar 25,2016）
前言 Github地址https://github.com/rootsongjc/distributed-load-testing-using-kubernetes
该教程描述如何在Kubernetes中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看Distributed Load Testing Using Kubernetes 。
注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。
准备 不需要GCE及其他组件，你只需要有一个kubernetes集群即可。
如果你还没有kubernetes集群，可以参考kubernetes-handbook部署一个。
部署Web应用 sample-webapp 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像index.tenxcloud.com/jimmy/k8s-sample-webapp:latest。
在kubernetes上部署sample-webapp。
$ cd kubernetes-config $ kubectl create -f sample-webapp-controller.yaml $ kubectl create -f kubectl create -f sample-webapp-service.yaml  部署Locust的Controller和Service locust-master和locust-work使用同样的docker镜像，修改cotnroller中spec.template.spec.containers.env字段中的value为你sample-webapp service的名字。
- name: TARGET_HOST value: http://sample-webapp:8000  创建Controller Docker镜像（可选） locust-master和locust-work controller使用的都是locust-tasks docker镜像。你可以直接下载gcr.io/cloud-solutions-http://olz1di9xf.bkt.clouddn.com/locust-tasks，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。
$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest . $ docker push index.tenxcloud.com/jimmy/locust-tasks:latest  注意：我使用的是时速云的镜像仓库。
每个controller的yaml的spec.template.spec.containers.image 字段指定的是我的镜像：</description>
    </item>
    
    <item>
      <title>Kubernetes中的IP和服务发现体系</title>
      <link>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 16:11:16 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</guid>
      <description>（题图：路边的野花@朝阳公园 Nov 8,2015）
Cluster IP 即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。
Service Name才是对外暴露服务的关键。
在kubeapi的配置中指定该地址范围。
默认配置
--service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=30000-32767  Pod IP 通过配置flannel的network和subnet来实现。
默认配置
FLANNEL_NETWORK=172.30.0.0/16 FLANNEL_SUBNET=172.30.46.1/24  Pod的IP地址不固定，当pod重启时IP地址会变化。
该IP地址也是用户无需关心的。
但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。
主机/Node IP 物理机的IP地址，即kubernetes管理的物理机的IP地址。
$ kubectl get nodes NAME STATUS AGE VERSION 172.20.0.113 Ready 12d v1.6.0 172.20.0.114 Ready 12d v1.6.0 172.20.0.115 Ready 12d v1.6.0  服务发现 集群内部的服务发现
通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。
通过sericename:port即可调用服务。
服务外部的服务发现
通过Ingress来实现，我们是用的Traefik来实现。
参考 Ingress解析
Kubernetes Traefik Ingress安装试用</description>
    </item>
    
    <item>
      <title>Kubernetes中的RBAC支持</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-rbac-support/</link>
      <pubDate>Fri, 21 Apr 2017 19:53:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-rbac-support/</guid>
      <description>（题图：无题 Apr 2,2016）
 在Kubernetes1.6版本中新增角色访问控制机制（Role-Based Access，RBAC）让集群管理员可以针对特定使用者或服务账号的角色，进行更精确的资源访问控制。在RBAC中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。在一个组织中，角色是为了完成各种工作而创造，用户则依据它的责任和资格来被指派相应的角色，用户可以很容易地从一个角色被指派到另一个角色。
 前言 本文翻译自RBAC Support in Kubernetes，转载自kubernetes中文社区，译者催总，Jimmy Song做了稍许修改。该文章是5天内了解Kubernetes1.6新特性的系列文章之一。
One of the highlights of the Kubernetes 1.6中的一个亮点时RBAC访问控制机制升级到了beta版本。RBAC，基于角色的访问控制机制，是用来管理kubernetes集群中资源访问权限的机制。使用RBAC可以很方便的更新访问授权策略而不用重启集群。
本文主要关注新特性和最佳实践。
RBAC vs ABAC 目前kubernetes中已经有一系列l 鉴权机制。鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。
ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。
而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。
因为 Kubernetes 社区的投入和偏好，相对于 ABAC 而言，RBAC 是更好的选择。</description>
    </item>
    
    <item>
      <title>Kubernetes traefik ingress安装试用</title>
      <link>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</link>
      <pubDate>Thu, 20 Apr 2017 22:38:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</guid>
      <description>（题图：🐟@鱼缸 Sep 15,2016）
前言 昨天翻了下Ingress解析，然后安装试用了下traefik，过程已同步到kubernetes-handbook上，Github地址https://github.com/rootsongjc/kubernetes-handbook
Ingress简介 如果你还不了解，ingress是什么，可以先看下我翻译的Kubernetes官网上ingress的介绍Kubernetes Ingress解析。
理解Ingress
简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。
理解Ingress Controller
Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。
部署Traefik 介绍traefik
Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。
以下配置文件可以在kubernetes-handbookGitHub仓库中的manifests/traefik-ingress/目录下找到。
创建ingress-rbac.yaml
将用于service account验证。
apiVersion: v1 kind: ServiceAccount metadata: name: ingress namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ingress subjects: - kind: ServiceAccount name: ingress namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes ingress解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</link>
      <pubDate>Wed, 19 Apr 2017 21:05:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</guid>
      <description>（题图：朝阳门银河SOHO Jan 31,2016）
前言 这是kubernete官方文档中Ingress Resource的翻译，因为最近工作中用到，文章也不长，也很好理解，索性翻译一下，也便于自己加深理解，同时造福kubernetes中文社区。后续准备使用Traefik来做Ingress controller，文章末尾给出了几个相关链接，实际使用案例正在摸索中，届时相关安装文档和配置说明将同步更新到kubernetes-handbook中。
术语
在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。
 节点：Kubernetes集群中的一台物理机或者虚拟机。 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。
 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。
 集群网络：一组逻辑或物理链接，可根据Kubernetes网络模型实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的OVS。
 服务：使用标签选择器标识一组pod成为的Kubernetes服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。
  什么是Ingress？ 通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：
 internet | ------------ [ Services ]  Ingress是授权入站连接到达集群服务的规则集合。
 internet | [ Ingress ] --|-----|-- [ Services ]  你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。
先决条件 在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。
GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 运行多个ingress controller 和 关闭glbc.
确定你已经阅读了Ingress controller的beta版本限制。在非GCE/GKE的环境中，你需要在pod中部署一个controller。
Ingress Resource 最简化的Ingress配置：
1: apiVersion: extensions/v1beta1 2: kind: Ingress 3: metadata: 4: name: test-ingress 5: spec: 6: rules: 7: - http: 8: paths: 9: - path: /testpath 10: backend: 11: serviceName: test 12: servicePort: 80  如果你没有配置Ingress controller就将其POST到API server不会有任何用处</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装EFK</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</link>
      <pubDate>Thu, 13 Apr 2017 12:28:10 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</guid>
      <description>（题图：簋街 Jun 17,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署EFK日志收集监控插件。
配置和安装 EFK 官方文件目录：cluster/addons/fluentd-elasticsearch
$ ls *.yaml es-controller.yaml es-service.yaml fluentd-es-ds.yaml kibana-controller.yaml kibana-service.yaml efk-rbac.yaml  同样EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。
已经修改好的 yaml 文件见：EFK
配置 es-controller.yaml $ diff es-controller.yaml.orig es-controller.yaml 24c24 &amp;lt; - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 --- &amp;gt; - image: sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2  配置 es-service.yaml 无需配置；
配置 fluentd-es-ds.yaml $ diff fluentd-es-ds.yaml.orig fluentd-es-ds.yaml 26c26 &amp;lt; image: gcr.io/google_containers/fluentd-elasticsearch:1.22 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22  配置 kibana-controller.yaml $ diff kibana-controller.yaml.orig kibana-controller.yaml 22c22 &amp;lt; image: gcr.io/google_containers/kibana:v4.6.1-1 --- &amp;gt; image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装heapster</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 20:20:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</guid>
      <description>（题图：大喵 Aug 8,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署heapster，包括influxdb、grafana等组件。
配置和安装Heapster 到 heapster release 页面 下载最新版本的 heapster。
$ wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip $ unzip v1.3.0.zip $ mv v1.3.0.zip heapster-1.3.0  文件目录： heapster-1.3.0/deploy/kube-config/influxdb
$ cd heapster-1.3.0/deploy/kube-config/influxdb $ ls *.yaml grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml heapster-rbac.yaml  我们自己创建了heapster的rbac配置heapster-rbac.yaml。
已经修改好的 yaml 文件见：heapster
配置 grafana-deployment $ diff grafana-deployment.yaml.orig grafana-deployment.yaml 16c16 &amp;lt; image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 40,41c40,41 &amp;lt; # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;lt; value: / --- &amp;gt; value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;gt; #value: /  如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为 /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://10.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装dashboard</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 15:53:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</guid>
      <description>（题图：东直门桥 Aug 20,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署dashboard。
感谢opsnull和ipchy的细心解答。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  配置和安装 dashboard 官方文件目录：kubernetes/cluster/addons/dashboard
我们使用的文件
$ ls *.yaml dashboard-controller.yaml dashboard-service.yaml dashboard-rbac.yaml  已经修改好的 yaml 文件见：dashboard
由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：
Forbidden (403) User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)  增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。
配置dashboard-service $ diff dashboard-service.yaml.orig dashboard-service.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之kubedns配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</link>
      <pubDate>Wed, 12 Apr 2017 13:04:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</guid>
      <description>（题图：雨过天晴@北京定福庄 Aug 27,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，使用yaml文件部署kubedns。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  安装和配置 kubedns 插件 官方的yaml文件目录：kubernetes/cluster/addons/dns。
该插件直接使用kubernetes部署，官方的配置文件中包含以下镜像：
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1  我clone了上述镜像，上传到我的私有镜像仓库：
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:1.14.1  同时上传了一份到时速云备份：
index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1  以下yaml配置文件中使用的是私有镜像仓库中的镜像。
kubedns-cm.yaml kubedns-sa.yaml kubedns-controller.yaml kubedns-svc.yaml  已经修改好的 yaml 文件见：github项目中的manifest/kubedns/目录。
系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；
$ kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes node节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</link>
      <pubDate>Tue, 11 Apr 2017 22:20:31 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</guid>
      <description>（题图：太阳宫桥@北京东北三环 Dec 11,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署node节点上的kube-proxy和kubelet，同时对之前部署的flannel改造。
安装环境配置信息
 CentOS7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  部署kubernetes node节点 kubernetes node 节点包含如下组件：
 Flanneld：参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。 Docker1.12.5：docker的安装很简单，这里也不说了。 kubelet kube-proxy  下面着重讲kubelet和kube-proxy的安装，同时还要将之前安装的flannel集成TLS验证。
目录和文件 我们再检查一下三个节点上，经过前几步操作生成的配置文件。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem $ ls /etc/kubernetes/ apiserver bootstrap.kubeconfig config controller-manager kubelet kube-proxy.kubeconfig proxy scheduler ssl token.csv  配置Flanneld 参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。
service配置文件/usr/lib/systemd/system/flanneld.service。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.</description>
    </item>
    
    <item>
      <title>Kubernetes高可用master节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</link>
      <pubDate>Tue, 11 Apr 2017 19:55:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</guid>
      <description>（题图：鬼见愁@北京西山 Sep 14,2015）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署master节点的kube-apiserver、kube-controller-manager和kube-scheduler的过程。
高可用kubernetes master节点安装 kubernetes master 节点包含的组件：
 kube-apiserver kube-scheduler kube-controller-manager  目前这三个组件需要部署在同一台机器上。
 kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关； 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；  本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）
TLS 证书文件 pem和token.csv证书文件我们在TLS证书和秘钥这一步中已经创建过了。我们再检查一下。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem  下载最新版本的二进制文件 有两种下载方式
方式一
从 github release 页面 下载发布版 tarball，解压后再执行下载脚本
$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz $ tar -xzvf kubernetes.tar.gz ... $ cd kubernetes $ .</description>
    </item>
    
    <item>
      <title>Kubernetes安装之etcd高可用配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</link>
      <pubDate>Tue, 11 Apr 2017 15:21:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</guid>
      <description>（题图：北京夜景@西山）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。
创建高可用 etcd 集群 kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为sz-pg-oam-docker-test-001.tendcloud.com、sz-pg-oam-docker-test-002.tendcloud.com、sz-pg-oam-docker-test-003.tendcloud.com：
 sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113 sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114 sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115  TLS 认证文件 需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl   kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；  下载二进制文件 到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件
$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz $ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz $ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin  创建 etcd 的 systemd unit 文件 注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；</description>
    </item>
    
    <item>
      <title>Kubernetes安装之创建kubeconfig文件</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</link>
      <pubDate>Tue, 11 Apr 2017 14:34:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</guid>
      <description>(题图：北海公园 May 8,2016)
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。 kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权； kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。
创建 TLS Bootstrapping Token Token auth file
Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;) cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot; EOF   后三行是一句，直接复制上面的脚本运行即可。
 将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。
$cp token.csv /etc/kubernetes/  创建 kubelet bootstrapping kubeconfig 文件 $ cd /etc/kubernetes $ export KUBE_APISERVER=&amp;quot;https://172.</description>
    </item>
    
    <item>
      <title>开源微服务管理平台fabric8简介</title>
      <link>http://rootsongjc.github.io/blogs/fabric8-introduction/</link>
      <pubDate>Mon, 10 Apr 2017 21:39:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/fabric8-introduction/</guid>
      <description>前言 无意中发现Fabric8这个对于Java友好的开源微服务管理平台。
其实这在这里发现的Achieving CI/CD with Kubernetes（by Ramit Surana,on February 17, 2017），其实是先在slideshare上看到的，pdf可以在此下载，大小2.04M。
大家可能以前听过一个叫做fabric的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把fabric8跟fabric搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，fabric8不是fabric的一个版本。Fabric是用python开发的，fabric8是java开发的。
如果你想了解简化Fabric可以看它的中文官方文档。
Fabric8简介 fabric8是一个开源集成开发平台，为基于Kubernetes和Jenkins的微服务提供持续发布。
使用fabric可以很方便的通过Continuous Delivery pipelines创建、编译、部署和测试微服务，然后通过Continuous Improvement和ChatOps运行和管理他们。
Fabric8微服务平台提供：
 Developer Console，是一个富web应用，提供一个单页面来创建、编辑、编译、部署和测试微服务。 Continuous Integration and Continous Delivery，使用 Jenkins with a Jenkins Workflow Library更快和更可靠的交付软件。 Management，集中式管理Logging、Metrics, ChatOps、Chaos Monkey，使用Hawtio和Jolokia管理Java Containers。 Integration Integration Platform As A Service with deep visualisation of your Apache Camel integration services, an API Registry to view of all your RESTful and SOAP APIs and Fabric8 MQ provides Messaging As A Service based on Apache ActiveMQ。 Java Tools 帮助Java应用使用Kubernetes:  Maven Plugin for working with Kubernetes ，这真是极好的 Integration and System Testing of Kubernetes resources easily inside JUnit with Arquillian Java Libraries and support for CDI extensions for working with Kubernetes.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之证书验证</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</link>
      <pubDate>Mon, 10 Apr 2017 17:28:41 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</guid>
      <description>（题图：铜牛@颐和园 Aug 25,2014）
前言 昨晚（Apr 9,2017）金山软件的opsnull发布了一个开源项目和我一步步部署kubernetes集群，下文是结合我之前部署kubernetes的过程打造的kubernetes环境和opsnull的文章创建 kubernetes 各组件 TLS 加密通信的证书和秘钥的实践。之前安装过程中一直使用的是非加密方式，一直到后来使用Fluentd和ElasticSearch收集Kubernetes集群日志时发现有权限验证问题，所以为了深入研究kubernentes。
Kubernentes中的身份验证 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
生成的 CA 证书和秘钥文件如下：
 ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem  使用证书的组件如下：
 etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem；  kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。
安装 CFSSL 方式一：直接使用二进制源码包安装
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 $ chmod +x cfssl_linux-amd64 $ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl $ wget https://pkg.</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践（第二版）》读后感</title>
      <link>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</link>
      <pubDate>Sat, 08 Apr 2017 12:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</guid>
      <description>（题图：长江三峡大坝@湖北宜昌 Apr 6,2015）
前言 最近（2017年3月）友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，点这里下载，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。这是该书的第二版，第一版2014年9月出版，第二版2016年9月出版，第二版的编者团队居然有50人之多😓
第二版分享了华为在云计算核心竞争力构建与价值转换方面的经验与建议，并补充了业界在公有云、私有云、行业云以及电信网络云化商用落地与技术应用方面的成功优秀实践。增加了对Docker容器与微服务敏捷迭代、大数据与数据库云化、行业建模与机器学习算法、混合云与管理自动化编排、云生态建设等方面的介绍。
第1章 云计算的商业动力与技术趋势 ​
​
​
​
​
​</description>
    </item>
    
    <item>
      <title>使用Fluentd和ElasticSearch收集Kubernetes集群日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</link>
      <pubDate>Fri, 07 Apr 2017 20:13:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</guid>
      <description>（题图：码头@古北水镇 Apr 30,2016）
前言 在安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard和配置Heapster监控插件后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。
官方文档
Kubernetes Logging and Monitoring Cluster Activity
Logging Using Elasticsearch and Kibana：不过这篇文章是在GCE上配置的，参考价值不大。
容器日志的存在形式 目前容器日志有两种输出形式：
stdout,stderr标准输出
这种形式的日志输出我们可以直接使用docker logs查看日志，kubernetes集群中同样可以使用kubectl logs类似的形式查看日志。
日志文件记录
这种日志输出我们无法从以上方法查看日志内容，只能tail日志文件查看。
Fluentd介绍 Fluentd是使用Ruby编写的，通过在后端系统之间提供统一的日志记录层来从后端系统中解耦数据源。 此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。 统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。 也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。
Fluentd收集的event由以下几个方面组成：
 Tag：字符串，中间用点隔开，如myapp.access Time：UNIX时间格式 Record：JSON格式  Fluentd特点  部署简单灵活 开源 经过验证的可靠性和性能 社区支持，插件较多 使用json格式事件格式 可拔插的架构设计 低资源要求 内置高可靠性  安装 查看cluster/addons/fluentd-elasticsearch插件目录，获取到需要用到的docker镜像名称。
$grep -rn &amp;quot;gcr.io&amp;quot; *.yaml es-controller.yaml:24: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 fluentd-es-ds.yaml:26: image: gcr.io/google_containers/fluentd-elasticsearch:1.22 kibana-controller.yaml:22: image: gcr.io/google_containers/kibana:v4.6.1-1  需要用到的镜像
 gcr.io/google_containers/kibana:v4.6.1-1 gcr.io/google_containers/elasticsearch:v2.4.1-2 gcr.</description>
    </item>
    
    <item>
      <title>Kubernetes的ConfigMap解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</link>
      <pubDate>Thu, 06 Apr 2017 21:24:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</guid>
      <description>（题图：龙形灯笼@古北水镇 Apr 30,2016）
前言 为什么要翻译这篇文章，是因为我在使用Fluentd和ElasticSearch收集Kubernetes集群日志的时候遇到了需要修改镜像中配置的问题，fluent-plugin-kubernetes_metadata里的需要的td-agent.conf文件。
其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
ConfigMap概览 ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。
kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3  data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：
 设置环境变量的值 在容器里设置命令行参数 在数据卷里面创建config文件  用户和系统组件两者都可以在ConfigMap里面存储配置数据。
其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。
Examples: # Create a new configmap named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.</description>
    </item>
    
    <item>
      <title>TensorFlow深度学习手写数字识别初体验</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</link>
      <pubDate>Wed, 05 Apr 2017 21:52:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</guid>
      <description>（题图：禾雀 @北京动物园 Apr 3,2017）
前言 TensorFlow学习曲线是陡峭的，不是所有的IT从业人员都很容易参与的，你需要有一定的数学专业知识，对于对深度学习没有经验的程序员，要想了解这门技术，最快捷的途径是先运行一个示例，我们认识事物都是先从感性、到理性的思辨过程。
下面我们来跟随Martin Gorner的TensorFlow and Deep Learing Without a PhD来编写我们的第一个TensorFlow程序——手写数字识别，这篇文章的中文版没有博士学位如何玩转TensorFlow和深度学习于2017年3月13日发表在发表在机器之心上。这篇文章也是根据3月8日-10日的Google Cloud NEXT&amp;rsquo;17大会上Martin Gorner做的讲解整理而成的，教程 | 没有博士学位，照样玩转TensorFlow深度学习这篇文章是对Martin Gorner的简易教程的原文翻译，我们暂时不要求了解TensorFlow背后复杂的理论，我们先跟随这篇简易教程玩一把TensorFlow的手写数字识别。
如果你想深入了解这本后的原理的话，可以查看哈尔滨工业大学社会计算与信息检索研究中心翻译的《神经网络与深度学习》这本书，该书翻译自Neural Networks and Deep Learning的中文翻译，原文作者 Michael Nielsen，而且这还是一本免费的电子书，该书中系统讲解了使用神经网络识别手写数字背后的原理。该书托管在GitBook上，你可以点击这里直接下载该书中文版的PDF。
准备 下载代码
这个代码仓库里包含了手写数字识别和下载依赖的训练数据的代码，我们将只用到mnist_1.0_softmax.py这一个代码文件。整个mnist_1.0_softmax.py代码并不复杂，不算注释的话只有36行。
git clone https://github.com/martin-gorner/tensorflow-mnist-tutorial.git  下载完后，可以看到有一个INSTALL.txt，这篇文章是运行代码所必需的环境要求说明。
安装TensorFlow
我之前写过详细的TensorFlow安装教程TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建，这篇文章中主要讲怎样在docker里安装TensorFlow。
我使用的Mac而且还是python2.7，所以我这样安装：
pip install --upgrade tensorflow --user -U pip install --upgrade matplotlib --user -U  运行示例 运行手写数字训练示例。
python mnist_1.0_softmax.py  运行过程中你会看到一大段输出：
Collecting matplotlib Downloading matplotlib-2.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (12.8MB) 100% |████████████████████████████████| 12.8MB 26kB/s Requirement already up-to-date: pyparsing!</description>
    </item>
    
    <item>
      <title>Kubernetes heapster监控插件安装文档</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</link>
      <pubDate>Wed, 05 Apr 2017 18:41:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</guid>
      <description>（题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）
前言 前面几篇文章中记录了我们安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard，但是还没法查看Pod的监控信息，虽然kubelet默认集成了cAdvisor（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。
安装 下载heapster的代码
直接现在Github上的最新代码。
git pull https://github.com/kubernetes/heapster.git  目前的最高版本是1.3.0。
在heapster/deploy/kube-config/influxdb目录下有几个yaml文件：
grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml  我们再看下用了哪些镜像：
grafana-deployment.yaml:16: image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 heapster-deployment.yaml:16: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 influxdb-deployment.yaml:16: image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1  下载镜像
我们下载好了这些images后，存储到私有镜像仓库里：
 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1  我已经将官方镜像克隆到了时速云上，镜像地址：
 index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1 index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2  需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。
docker login index.tendcloud.com  配置 参考Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI和Configuring Source，需要修改yaml文件中的几个配置。
 首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的 修改heapster-deployment.</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard/Web UI安装全记录</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</link>
      <pubDate>Wed, 05 Apr 2017 14:28:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</guid>
      <description>（题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）
前言 前几天在CentOS7.2上安装Kubernetes1.6和安装好flannel网络配置，今天我们来安装下kuberentnes的dashboard。
Dashboard是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟Docker Enterprise Edition的Docker Datacenter很像了。
安装Dashboard 官网的安装文档https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。
首先需要一个kubernetes-dashboard.yaml的配置文件，可以直接在Github的src/deploy/kubernetes-dashboard.yaml下载。
我们能看下这个文件的内容：
# Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</description>
    </item>
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>（题图：西安鼓楼 Oct 4,2014）
书接上文在CentOS中安装Kubernetes详细指南，这是一个系列文章，作为学习Kubernetes的心路历程吧。
本文主要讲解Kubernetes的网络配置，👆文中有一个安装Flannel的步骤，但是安装好后并没有相应的配置说明。
配置flannel 我们直接使用的yum安装的flannle，安装好后会生成/usr/lib/systemd/system/flanneld.service配置文件。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service  可以看到flannel环境变量配置文件在/etc/sysconfig/flanneld。
# Flanneld configuration options # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot; # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot; # Any additional options that you want to pass #FLANNEL_OPTIONS=&amp;quot;&amp;quot;   etcd的地址FLANNEL_ETCD_ENDPOINT etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX  在etcd中创建网络配置</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第三章TensorFlow入门</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</link>
      <pubDate>Thu, 30 Mar 2017 21:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</guid>
      <description>（题图：扬州东关 May 24,2015）
这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
这一章从三个角度带大家入门。
分别是TensorFlow的
 计算模型 数据模型 运行模型  3.1 TensorFlow的计算模型——图计算 计算图是TensorFlow中的一个最基本的概念，TensorFlow中的所有计算都会转化成计算图上的节点。
其实TensorFlow的名字已经暗示了它的实现方式了，Tensor表示的是数据结构——张量，Flow表示数据流——Tensor通过数据流相互转化。
常用的方法
 在python中导入tensorflow：import tensorflow as tf 获取当前默认的计算图：tf.get_default_graph() 生成新的计算图：tf.Graph()  书中这里都有例子讲解，可以从Github中下载代码，或者如果你使用才云提供的docker镜像的方式安装的话，在jupyter中可以看到各个章节的代码。
定义两个不同的图
import tensorflow as tf g1 = tf.Graph() with g1.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.zeros_initializer) # 设置初始值为0 g2 = tf.Graph() with g2.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.ones_initializer()) # 设置初始值为1 with tf.Session(graph = g1) as sess: tf.global_variables_initializer().run() with tf.variable_scope(&amp;quot;&amp;quot;, reuse=True): print(sess.</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>（题图：北京圆明园 Aug 25,2014）
作者：Jimmy Song，Peter Ma，2017年3月30日
最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是CNCF的成员之一。
这篇是根据官方安装文档实践整理的，操作系统是纯净的CentOS7.2。
另外还有一个Peter Ma写的在CentOS上手动安装kubernetes的文档可以参考。
角色分配
下面以在三台主机上安装Kubernetes为例。
172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel 172.20.0.114 node kubectl kube-proxy flannel 172.20.0.115 node kubectl kube-proxy flannel  第一台主机既作为master也作为node。
系统环境
 Centos 7.2.1511 docker 1.12.6 etcd 3.1.5 kubernetes 1.6.0 flannel 0.7.0-1  安装 下面给出两种安装方式：
 配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。 使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。  我们最终选择使用第二种方式安装。
本文的很多安装步骤和命令是参考的Kubernetes官网CentOS Manual Config文档。
第一种方式：CentOS系统中直接使用yum安装 给yum源增加一个Repo
[virt7-docker-common-release] name=virt7-docker-common-release baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/ gpgcheck=0  安装docker、kubernetes、etcd、flannel一步到位
yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel  安装好了之后需要修改一系列配置文件。</description>
    </item>
    
    <item>
      <title>Pivotal Cloud foundry快速开始指南</title>
      <link>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</link>
      <pubDate>Thu, 23 Mar 2017 22:54:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</guid>
      <description>（题图：黄山日出后的云海 Oct 3,2013）
前言 最近研究了下Pivotal的Cloud foundry，CF本身是一款开源软件，很多PAAS厂商都加入了CF，我们用的是的PCF Dev（PCF Dev是一款可以在工作站上运行的轻量级PCF安装）来试用的，因为它可以部署在自己的环境里，而Pivotal Web Services只免费两个月，之后就要收费。这里有官方的详细教程。
开始 根据官网的示例，我们将运行一个Java程序示例。
安装命令行终端
下载后双击安装即可，然后执行cf help能够看到帮助。
安装PCF Dev
先下载，如果你没有Pivotal network账号的话，还需要注册个用户，然后用以下命令安装：
$./pcfdev-VERSION-osx &amp;amp;&amp;amp; \ cf dev start Less than 4096 MB of free memory detected, continue (y/N): &amp;gt; y Please sign in with your Pivotal Network account. Need an account? Join Pivotal Network: https://network.pivotal.io Email&amp;gt; 849122844@qq.com Password&amp;gt; Downloading VM... Progress: |+++++++++++++=======&amp;gt;| 100% VM downloaded. Allocating 4096 MB out of 16384 MB total system memory (3514 MB free).</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</link>
      <pubDate>Thu, 23 Mar 2017 19:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</guid>
      <description>（题图：广州海珠桥 Aug 10,2014）
 这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
 P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
睇完这一章后应该就可以自己搭建出一个TensorFlow的环境，我之前在docker里玩过，镜像比较大，下载慢一点，不过用起来很方便，如果你仅仅是想试用一下TensorFlow，看看它能干什么的话，可以直接在docker里试用一下。在Mac上安装的详细步骤，官方安装说明文档。
2.1 TensorFlow的主要依赖包 TensorFlow主要用到以下两个依赖：
 Protocol buffer：数据结构化工具。Google开源的结构化数据格式，用于网络传输数据时候的序列化和反序列化，使用的时候需要先定义schema，github地址https://github.com/google/protobuf。分布式TensorFlow使用到额gRPC也是使用Protocol Buffer来组织的， Bazel:自动化编译构建工具。Google开源的，github地址https://github.com/bazelbuild/bazel，它支持多语言、多平台、可重复编译和可伸缩，构建大型软件速度也是很快的。Bazel使用**项目空间**的形式管理编译的，每个项目空间需要包含[BUILD文件](https://github.com/tensorflow/tensorflow/blob/master/bower.BUILD)（定义编译目标）和[WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)文件（定义编译的依赖环境）。这两个文件都有点类似python语法。  2.2 TensorFlow安装 TensorFlow的安装方式包括docker镜像、pip安装、源码编译安装。
我选择最方便的docker镜像方式，其他方式对本地环境做很多配置，折腾起来比较麻烦。
我早就在docker中安装过TensorFlow0.9小试过牛刀。现在1.0.1版本已经released了。TensorFlow的所有版本都有对应的docker镜像发布在docker hub，可以直接docker pull安装。
为了和书中所用的镜像保持统一，我将使用caicloud提供的镜像，基于TensorFlow0.12.0（这个版本是2016年12月20日发布的），他们增加了一些其他机器学习工具包和TensorFlow可视化工具TensorBoard。
docker镜像方式安装
首先下载镜像，这个image比较大，下载下来比较费时间，我用了差不多15分钟吧。
docker pull cargo.caicloud.io/tensorflow/tensorflow:0.12.0  下载下来后我们再check下这个大小为1.41GB镜像的layers。
另外还有个nvidia版本的docker，可以将你电脑的GPU派山用场，我暂时没用到GPU，我电脑装的是docker17.03-ce，就不折腾GPU版本的TensorFlow了。
IMAGE CREATED CREATED BY SIZE COMMENT c8a8409297f2 5 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;/run_tf.sh&amp;quot;] 0 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY file:78332d36244852... 122 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY dir:8b6ab7d235e3975.</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第一章深度学习简介</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</link>
      <pubDate>Mon, 20 Mar 2017 22:04:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</guid>
      <description>（题图：TensofFlow实战图书封面）
🙏电子工业出版社编辑赠书，能够这么快的拿到这本书，也🙏才云科技的郑泽宇大哥耐心的写了这本书，能够让我等小白一窥深度学习的真容。另外要强烈推荐下这本书，这是本TensorFlow深度学习很好的入门书。书中提供的代码下载地址，整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
1.1 人工智能、机器学习与深度学习 这一节是讲解三者之间的关系。
首先以垃圾邮件分类问题引入机器学习的逻辑回归算法。
逻辑回归算法的准确性取决于训练数据中的特征的提取，以及训练的数据数量。
文章中又提了一个从实体中提取特征的例子：通过笛卡尔坐标系活极角坐标系来表示不同颜色的点，看看能否用一条直线划分。这个例子用来说明一旦解决了数据表达和特征提取，很多人工智能的问题就能迎刃而解。
深度学习是机器学习的一个分支，除了能够学习特征和任务之间的关联之外，还能自动从简单特征中提取更加复杂的特征，这是其区别于机器学习的关键点。
总的来说，人工智能&amp;gt;机器学习&amp;gt;深度学习。
1.2深度学习的发展历程 本节介绍了深度网络历史的三个发展阶段。
2012年的ImageNet图像分类竞赛上，深度学习系统AlexNet赢得冠军，自此深度学习作为深层神经网络的代名词而被人熟知。
1.3深度学习的应用 这一节讲的是深度学习的应用，首先还是从ImageNet的图像识别开始，应用到了OCR（提到了卷积神经网络）、语音识别（提到了混合搞高斯模型）、自然语言处理（提到了语料库、单词向量、机器翻译、情感分析）、人机对弈（提到了AlphaGO）。
1.4 深度学习工具介绍与对比 TensorFlow的渊源是Google大脑团队在2011年开发，在内部使用的DistBelief，并赢得了ImageNet 2014年的比赛，TF是其开源版本，还发表了一篇论文TensorFlow: Large-Scale Machine Learning on Heteogeneous Distributed systems，这就跟当年的HDFS、MapReduce一个套路啊。
Google还把它用来做RankBrain和很多其他的产品线上使用。
当然，还有很多其他的深度学习工具，比如Caffe、Deeplearning4j、Torch等不一而足。从各种指标来看，TensorFlow都是目前最受关注的深度学习框架。</description>
    </item>
    
    <item>
      <title>Docker源码分析第一篇——代码结构</title>
      <link>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</link>
      <pubDate>Sun, 19 Mar 2017 23:00:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</guid>
      <description>(题图：北京八达岭长城 Oct 1,2015)
前言 之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下docker的源码，在网上找到一些相关资料，都比较过时了，发现*孙宏亮*大哥写过一本书叫《Docker源码分析》，而且之前也在InfoQ上陆续发过一些文章，虽然文章都比较老了，基于老的docker版本，但我认为依然有阅读的价值。起码能有这三方面收获：
 一是培养阅读源码的思维方式，为自己阅读docker源码提供借鉴。 二是可以了解docker版本的来龙去脉。 三还可以作为Go语言项目开发作为借鉴。  下载地址 鉴于这本书已经发行一年半了了，基于的docker版本还是1.2.0，而如今都到了1.13.0（docker17.03的老版本号），应该很少有人买了吧，可以说这本书的纸质版本的生命周期也差不多了吧。如果有人感兴趣可以下载pdf版本看看，Docker源码解析-机械工业出版社-孙宏亮著-2015年8月（完整文字版，大小25.86M），Docker源码解析-看云整理版（文字版，有缩略，大小7.62M）。
Out-of-date 有一点必须再次强调一下，这本书中的docker源码分析是基于docker1.2.0，而这个版本的docker源码在github上已经无法下载到了，github上available的最低版本的docker源码是1.4.1。
 顺便感叹一句，科技行业发展实在太快了，尤其是互联网，一本书能连续用上三年都不过时，如果这样的话那么这门技术恐怕都就要被淘汰了吧？
 总体架构 Docker总体上是用的是Client/Server模式，所有的命令都可以通过RESTful接口传递。
整个Docker软件的架构中可以分成三个角色：
 Daemon：常驻后台运行的进程，接收客户端请求，管理docker容器。 Client：命令行终端，包装命令发送API请求。 Engine：真正处理客户端请求的后端程序。  代码结构 Docker的代码结构比较清晰，分成的目录比较多，有以下这些：
 api：定义API，使用了Swagger2.0这个工具来生成API，配置文件在api/swagger.yaml builder：用来build docker镜像的包，看来历史比较悠久了 bundles：这个包是在进行docker源码编译和开发环境搭建的时候用到的，编译生成的二进制文件都在这里。 cli：使用cobra工具生成的docker客户端命令行解析器。 client：接收cli的请求，调用RESTful API中的接口，向server端发送http请求。 cmd：其中包括docker和dockerd两个包，他们分别包含了客户端和服务端的main函数入口。 container：容器的配置管理，对不同的platform适配。 contrib：这个目录包括一些有用的脚本、镜像和其他非docker core中的部分。 daemon：这个包中将docker deamon运行时状态expose出来。 distribution：负责docker镜像的pull、push和镜像仓库的维护。 dockerversion：编译的时候自动生成的。 docs：文档。这个目录已经不再维护，文档在另一个仓库里https://github.com/docker/docker.github.io/。 experimental：从docker1.13.0版本起开始增加了实验特性。 hack：创建docker开发环境和编译打包时用到的脚本和配置文件。 image：用于构建docker镜像的。 integration-cli：集成测试 layer：管理 union file system driver上的read-only和read-write mounts。 libcontainerd：访问内核中的容器系统调用。 man：生成man pages。 migrate：将老版本的graph目录转换成新的metadata。 oci：Open Container Interface库 opts：命令行的选项库。 pkg： plugin：docker插件后端实现包。 profiles：里面有apparmor和seccomp两个目录。用于内核访问控制。 project：项目管理的一些说明文档。 reference：处理docker store中镜像的reference。 registry：docker registry的实现。 restartmanager：处理重启后的动作。 runconfig：配置格式解码和校验。 vendor：各种依赖包。 volume：docker volume的实现。  下一篇将讲解docker的各个功能模块和原理。</description>
    </item>
    
    <item>
      <title>Contiv Ultimate-Docker17.03CE下思科docker网络插件contiv趟坑终极版</title>
      <link>http://rootsongjc.github.io/blogs/contiv-ultimate/</link>
      <pubDate>Fri, 17 Mar 2017 17:52:37 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-ultimate/</guid>
      <description>（题图：广州石牌桥 Aug 10,2014）
前几天写的几篇关于Contiv的文章已经把引入坑了😂
今天这篇文章将带领大家用正确的姿势编译和打包一个contiv netplugin。
 请一定要在Linux环境中编译。docker中编译也会报错，最好还是搞个虚拟🐔吧，最好还有VPN能翻墙。
 环境准备 我使用的是docker17.03-CE、安装了open vSwitch(这个包redhat的源里没有，需要自己的编译安装)，如果你懒得编译可以用我编译的rpm包，点这里下载。
编译 这一步是很容易失败的，有人提过issue-779
具体步骤
 创建一个link /go链接到你的GOPATH目录，下面编译的时候要用。 将源码的vender目录下的文件拷贝到$GOPATH/src目录。 执行编译  在netplugin目录下执行以下命令能够编译出二进制文件。
NET_CONTAINER_BUILD=1 make build  在你的/$GOPATH/bin目录下应该会有如下几个文件：
contivk8s github-release godep golint misspell modelgen netcontiv netctl netmaster netplugin  ⚠️编译过程中可能会遇到 有些包不存在或者需要翻墙下载。
打包 我们将其打包为docker plugin。
Makefile里用于创建plugin rootfs的命令是：
host-pluginfs-create: @echo dev: creating a docker v2plugin rootfs ... sh scripts/v2plugin_rootfs.sh  v2plugin_rootfs.sh这个脚本的内容：
#!/bin/bash # Script to create the docker v2 plugin # run this script from contiv/netplugin directory echo &amp;quot;Creating rootfs for v2plugin &amp;quot;, ${CONTIV_V2PLUGIN_NAME} cat install/v2plugin/config.</description>
    </item>
    
    <item>
      <title>Docker17.03-CE插件开发-举个🌰</title>
      <link>http://rootsongjc.github.io/blogs/docker-plugin-develop/</link>
      <pubDate>Wed, 15 Mar 2017 13:57:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-plugin-develop/</guid>
      <description>（题图：杭州吴山步道旁的墙壁 Oct 16,2016）
 当你看到这篇文章时，如果你也正在进行docker1.13+版本下的plugin开发，恭喜你也入坑了，如果你趟出坑，麻烦告诉你的方法，感恩不尽🙏
 看了文章后你可能会觉得，官网上的可能是个假🌰。虽然官网上的文档写的有点不对，不过你使用docker-ssh-volume的开源代码自己去构建plugin的还是可以成功的！
Docker plugin开发文档 首先docker官方给出了一个docker legacy plugin文档，这篇文章基本就是告诉你docker目前支持哪些插件，罗列了一系列连接，不过对不起，这些不是docker官方插件，有问题去找它们的开发者去吧😂
Docker plugin貌似开始使用了新的v2 plugin了，legacy版本的plugin可以能在后期被废弃。
从docker的源码plugin/store.go中可以看到：
/* allowV1PluginsFallback determines daemon&#39;s support for V1 plugins. * When the time comes to remove support for V1 plugins, flipping * this bool is all that will be needed. */ const allowV1PluginsFallback bool = true /* defaultAPIVersion is the version of the plugin API for volume, network, IPAM and authz. This is a very stable API.</description>
    </item>
    
    <item>
      <title>Docker 17.03-CE create plugin源码解析</title>
      <link>http://rootsongjc.github.io/blogs/docker-create-plugin/</link>
      <pubDate>Wed, 15 Mar 2017 12:09:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-create-plugin/</guid>
      <description>（题图：故宫 Apr 3,2016）
继续上一篇Docker17.03-CE插件开发的🌰，今天来看下docker create plugin的源码。
cli/command/plugin/create.go
Docker命令行docker plugin create调用的，使用的是cobra，这个命令行工具开发包很好用，推荐下。
执行这两个函数
func newCreateCommand(dockerCli *command.DockerCli) *cobra.Command //调用下面的函数，拼装成URL调用RESTful API接口 func runCreate(dockerCli *command.DockerCli, options pluginCreateOptions) error { ... if err = dockerCli.Client().PluginCreate(ctx, createCtx, createOptions); err != nil { return err } ... }  在api/server/router/plugin/plugin_routes.go中
func (pr *pluginRouter) createPlugin(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { ... if err := pr.backend.CreateFromContext(ctx, r.Body, options); err != nil { return err } ... }  createPlugin这个方法定义在api/server/route/plugin/backen.</description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part2</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</link>
      <pubDate>Fri, 10 Mar 2017 22:06:32 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</guid>
      <description> （题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)
本文是Docker v.s Kubernetes第二篇，续接上文Docker v.s Kuberntes Part1。
Kubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。
Kubernetes核心架构 master节点  apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类：  endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。   node节点  kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。  Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。
APIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。
Scheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。
工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。
工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。
工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。
Controller Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。
与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。
kubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。
kube-proxy kube-proxy提供两种功能:
 提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。  </description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part1</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</link>
      <pubDate>Fri, 10 Mar 2017 21:09:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</guid>
      <description> （题图：杭州西湖 Oct 16,2016）
前言 这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看Kunernetes权威指南（第二版）这本书（P.S感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。
此系列文章中所说的docker指的是*17.03-ce*版本。
概念性的差别 Kubernetes
了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：
 Pod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job  Docker
Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：
 Service Node Stack Docker  就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。
功能性差别  Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。 Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。 Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。 Kubernetes对象选择机制继续通过label selector，用于对象调度。 Kubernetes中有一个比较特别的镜像，叫做google_containers/pause，这个镜像是用来实现Pod概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。 Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。 持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用volume plugin。 多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。  总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。
 以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。
 </description>
    </item>
    
    <item>
      <title>Contiv入坑指南-v2plugin</title>
      <link>http://rootsongjc.github.io/blogs/contiv-v2plugin/</link>
      <pubDate>Fri, 10 Mar 2017 11:51:09 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-v2plugin/</guid>
      <description>(题图：上海交通大学 Oct 22,2016)
继续趟昨天挖的坑。
昨天的issue-776已经得到@gkvijay的回复，原来是因为没有安装contiv/v2plugin的缘故，所以create contiv network失败，我需要自己build一个docker plugin。
查看下这个commit里面有build v2plugin的脚本更改，所以直接调用以下命令就可以build自己的v2plugin。
前提你需要先build出netctl、netmaster、netplugin三个二进制文件并保存到bin目录下，如果你没自己build直接下载release里面的文件保存进去也行。
编译v2plugin插件 修改config.json插件配置文件
{ &amp;quot;manifestVersion&amp;quot;: &amp;quot;v0&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Contiv network plugin for Docker&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;https://contiv.github.io&amp;quot;, &amp;quot;entrypoint&amp;quot;: [&amp;quot;/startcontiv.sh&amp;quot;], &amp;quot;network&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;host&amp;quot; }, &amp;quot;env&amp;quot;: [ { &amp;quot;Description&amp;quot;: &amp;quot;To enable debug mode, set to &#39;-debug&#39;&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;dbg_flag&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;-debug&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;VLAN uplink interface used by OVS&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;iflist&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;Etcd or Consul cluster store url&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;cluster_store&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;etcd://172.</description>
    </item>
    
    <item>
      <title>Contiv入坑指南-试用全记录</title>
      <link>http://rootsongjc.github.io/blogs/contiv-tryout/</link>
      <pubDate>Thu, 09 Mar 2017 14:23:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-tryout/</guid>
      <description>(题图：山东荣成滨海风力发电场 Jan 31,2017）
关于contiv的介绍请看我的上一篇文章Contiv Intro。
开发环境使用Vagrant搭建，昨天试用了下，真不知道它们是怎么想的，即然是docker插件为啥不直接在docker中开发呢，我有篇文章介绍如何搭建docker开发环境，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像dockercore/docker:latest即可，不过有点大2.31G，使用阿里云的mirror下载倒是也划算，总比你自己部署一个开发环境节省时间。
Contiv概念解析 Contiv用于给容器创建和分配网路，可以创建策略管理容器的安全、带宽、优先级等，相当于一个SDN。
Group 按容器或Pod的功能给容器分配策略组，通常是按照容器/Pod的label来分组，应用组跟contiv的network不是一一对应的，可以很多应用组属于同一个network或IP subnet。
Polices 用来限定group的行为，contiv支持两种类型的policy：
 Bandwidth 限定应用组的资源使用上限 Isolation 资源组的访问权限  Group可以同时应用一个或多个policy，当有容器调度到该group里就会适用该group的policy。
Network IPv4或IPv6网络，可以配置subnet和gateway。
Contiv中的网络
在contiv中可以配置两种类型的网络
 application network：容器使用的网络 infrastructure network：host namespace的虚拟网络，比如基础设施监控网络  网络封装
Contiv中有两种类型的网络封装
 Routed：overlay topology和L3-routed BGP topology Bridged：layer2 VLAN  Tenant Tenant提供contiv中的namespace隔离。一个tenant可以有很多个network，每个network都有个subnet。该tenant中的用户可以使用它的任意network和subnet的IP。
物理网络中的tenant称作虚拟路由转发(VRF)。Contiv使用VLAN和VXLAN ID来实现外部网络访问，这取决你使用的是layer2、layer3还是Cisco ACI。
Contiv下载 Contiv的编译安装比较复杂，我们直接下载github上的release-1.0.0-beta.3-03-08-2017.18-51-20.UTC文件解压获得二进制文件安装。
 https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。
 如果试用可以的话，我会后续写contiv开发环境搭建的文章。
这个release是2017年3月8日发布的，就在我写这篇文章的前一天。有个最重要的更新是支持docker1.13 swarm mode。
官方安装文档
下载解压后会得到如下几个文件：
 contivk8s k8s专用的 contrib 文件夹，里面有个netctl的bash脚本 netcontiv 这个命令就一个-version选项用来查看contiv的版本😓 netctl contiv命令行工具，用来配置网络、策略、服务负载均衡，使用说明 netmaster contiv的主节点服务 netplugin  下面的安装中用到的只有netctl、netmaster和netplugin这三个二进制文件。</description>
    </item>
    
    <item>
      <title>Contiv Intro</title>
      <link>http://rootsongjc.github.io/blogs/contiv-guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-guide/</guid>
      <description>(题图：北京蓝色港湾夜景 Feb 11,2017 元宵节)
Contiv是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。
容器网络插件 Calico 与 Contiv Netplugin深入比较
还有篇文章讲解了docker网络方案的改进
Contiv Netplugin 简介 Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。

 Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址 Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。 集群管理依赖 etcd/serf</description>
    </item>
    
    <item>
      <title>Packer Intro</title>
      <link>http://rootsongjc.github.io/blogs/packer-intro/</link>
      <pubDate>Thu, 09 Mar 2017 10:58:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/packer-intro/</guid>
      <description>昨天研究了下Vagrant，感觉它的虚拟机ruby格式定义很麻烦，经人指点还有一个叫做packer的东西，也是Hashicorp这家公司出品的，今天看了下。
Packer是一款开源轻量级的镜像定义工具，可以根据一份定义文件生成多个平台的镜像，支持的平台有：
 Amazon EC2 (AMI). Both EBS-backed and instance-store AMIs Azure DigitalOcean Docker Google Compute Engine OpenStack Parallels QEMU. Both KVM and Xen images. VirtualBox VMware  Packer创造的镜像也能转换成Vagrant boxes。
Packer的镜像创建需要一个json格式的定义文件，例如quick-start.json
{ &amp;quot;variables&amp;quot;: { &amp;quot;access_key&amp;quot;: &amp;quot;{{env `AWS_ACCESS_KEY_ID`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{env `AWS_SECRET_ACCESS_KEY`}}&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-af22d9b9&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;packer-example {{timestamp}}&amp;quot; }] }  使用packer build quick-start.json可以在AWS上build一个AIM镜像。
Packer的详细文档：https://www.packer.io/docs/</description>
    </item>
    
    <item>
      <title>Vagrant介绍-从使用到放弃完全指南</title>
      <link>http://rootsongjc.github.io/blogs/vagrant-intro/</link>
      <pubDate>Wed, 08 Mar 2017 20:40:08 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/vagrant-intro/</guid>
      <description>（题图：北京地铁13号线光熙家园夜景 Mar 5,2017）
起源 久闻Vagrant大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。
因为今天在看contiv正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的Vagrantfile文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。
➜ netplugin git:(master) ✗ vagrant status Current machine states: netplugin-node1 running (virtualbox) netplugin-node2 running (virtualbox) netplugin-node3 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`.  Vagrant是hashicorp这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform。他们的产品都是基于Open Source的Github地址。
用途 Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。
Vagrant提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。
跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个Ubuntu 12.04的镜像。
用法 你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。
Vagrantfile
Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。
看个例子，选自https://github.com/fenbox/Vagrantfile
# -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below.</description>
    </item>
    
    <item>
      <title>Docker源码编译和开发环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/docker-dev-env/</link>
      <pubDate>Mon, 06 Mar 2017 17:03:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-dev-env/</guid>
      <description>看了下网上其他人写的docker开发环境搭建，要么是在ubuntu下搭建，要么就是使用官方说明的build docker-dev镜像的方式一步步搭建的，甚是繁琐，docker hub上有一个docker官方推出的dockercore/docker镜像，其实这就是官网上所说的docker-dev镜像，不过以前的那个deprecated了，使用目前这个镜像搭建docker开发环境是最快捷的了。
想要修改docker源码和做docker定制开发的同学可以参考下。
官方指导文档：https://docs.docker.com/opensource/code/
设置docker开发环境：https://docs.docker.com/opensource/project/set-up-dev-env/
docker的编译实质上是在docker容器中运行docker。
因此在本地编译docker的前提是需要安装了docker，还需要用git把代码pull下来。
创建分支 为了方便以后给docker提交更改，我们从docker官方fork一个分支。
git clone https://github.com/rootsongjc/docker.git git config --local user.name &amp;quot;Jimmy Song&amp;quot; git config --local user.email &amp;quot;rootsongjc@gmail.com&amp;quot; git remote add upstream https://github.com/docker/docker.git git config --local -l git remote -v git checkout -b dry-run-test touch TEST.md vim TEST.md git status git add TEST.md git commit -am &amp;quot;Making a dry run test.&amp;quot; git push --set-upstream origin dry-run-test  然后就可以在dry-run-test这个分支下工作了。
配置docker开发环境 官网上说需要先清空自己电脑上已有的容器和镜像。
docker开发环境本质上是创建一个docker镜像，镜像里包含了docker的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。
在dry-run-test分支下执行
make BIND_DIR=.</description>
    </item>
    
    <item>
      <title>12因素法则</title>
      <link>http://rootsongjc.github.io/blogs/12-factor-app/</link>
      <pubDate>Mon, 27 Feb 2017 22:32:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/12-factor-app/</guid>
      <description>Twelve-factor App 简介 如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建如下的 SaaS 应用提供了方法论：
 使用标准化流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。 和操作系统之间尽可能的划清界限，在各个系统中提供最大的可移植性。 适合部署在现代的云计算平台，从而在服务器和系统管理方面节省资源。 将开发环境和生产环境的差异降至最低，并使用持续交付实施敏捷开发。 可以在工具、架构和开发流程不发生明显变化的前提下实现扩展。  这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。
背景 本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过 Heroku 平台间接见证了数十万应用程序的开发，运作以及扩展的过程。
本文综合了我们关于 SaaS 应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何 避免软件污染 。
我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于 Martin Fowler 的书籍： *Patterns of Enterprise Application Architecture* ， *Refactoring* 。
12-factors I. 基准代码 一份基准代码，多份部署 II. 依赖 显式声明依赖关系 III. 配置 在环境中存储配置 IV. 后端服务 把后端服务当作附加资源 V. 构建，发布，运行 严格分离构建和运行 VI. 进程 以一个或多个无状态进程运行应用 VII. 端口绑定 通过端口绑定提供服务 VIII. 并发 通过进程模型进行扩展 IX. 易处理 快速启动和优雅终止可最大化健壮性 X. 开发环境与线上环境等价 尽可能的保持开发，预发布，线上环境相同 XI. 日志 把日志当作事件流 XII.</description>
    </item>
    
    <item>
      <title>Docker Service Discovery</title>
      <link>http://rootsongjc.github.io/blogs/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-service-discovery/</guid>
      <description>Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of service discovery backend. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.

Thanks to Docker 1.12 Swarm Mode, we don’t have to depend upon these external tools and complex configurations. Docker Engine 1.12 runs it’s own internal DNS service to route services by name.</description>
    </item>
    
    <item>
      <title>Docker内置DNS</title>
      <link>http://rootsongjc.github.io/blogs/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-embedded-dns/</guid>
      <description>本文主要介绍了Docker容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。
Configure container DNS DNS in default bridge network    Options Description     -h HOSTNAME or –hostname=HOSTNAME 在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。   –link=CONTAINER_NAME or ID:ALIAS 在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？   –dns=IP_ADDRESS… 在该容器启动时，将nameserver IP_ADDRESS添加到容器内的/etc/resolv.conf中。可以配置多个。   –dns-search=DOMAIN… 在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。   –dns-opt=OPTION… 在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。     说明：
 如果docker run时不含--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。 如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：  如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.</description>
    </item>
    
    <item>
      <title>Raft一致性算法</title>
      <link>http://rootsongjc.github.io/blogs/raft/</link>
      <pubDate>Mon, 27 Feb 2017 10:47:14 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/raft/</guid>
      <description>这是一个动画演示版的Raft一致性算法的说明，很直观，推荐观看。 http://thesecretlivesofdata.com/raft/
P.S Raft一致性算法在很多软件中都有应用，如Docker（Swarm Mode）、Ectd等，Hadoop生态圈里的Zookeeper用的是艰深的Paxos算法。</description>
    </item>
    
  </channel>
</rss>