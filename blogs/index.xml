<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/blogs/index.xml</link>
    <description>Recent content in Blogs on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Apr 2017 22:14:49 +0800</lastBuildDate>
    <atom:link href="http://rootsongjc.github.io/blogs/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubernetes网络和集群性能测试</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</link>
      <pubDate>Tue, 25 Apr 2017 22:14:49 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160618045.jpg&#34; alt=&#34;Img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：无题@安贞门 Jun 18,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;该测试是为了测试在不同的场景下，访问kubernetes的延迟以及kubernetes的性能。进行以下测试前，你需要有一个部署好的kubernetes集群，关于如何部署kuberentes1.6集群，请参考&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;测试环境&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在以下几种环境下进行测试：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes集群node节点上通过Cluster IP方式访问&lt;/li&gt;
&lt;li&gt;Kubernetes集群内部通过service访问&lt;/li&gt;
&lt;li&gt;Kubernetes集群外部通过traefik ingress暴露的地址访问&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;测试地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cluster IP: 10.254.149.31&lt;/p&gt;

&lt;p&gt;Service Port：8000&lt;/p&gt;

&lt;p&gt;Ingress Host：traefik.sample-webapp.io&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;测试工具&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://locust.io&#34;&gt;Locust&lt;/a&gt;：一个简单易用的用户负载测试工具，用来测试web或其他系统能够同时处理的并发用户数。&lt;/li&gt;
&lt;li&gt;curl&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/test/e2e&#34;&gt;kubemark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;测试程序：sample-webapp，源码见Github &lt;a href=&#34;https://github.com/rootsongjc/distributed-load-testing-using-kubernetes&#34;&gt;kubernetes的分布式负载测试&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;测试说明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过向&lt;code&gt;sample-webapp&lt;/code&gt;发送curl请求获取响应时间，直接curl后的结果为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;$ curl &amp;quot;http://10.254.149.31:8000/&amp;quot;
Welcome to the &amp;quot;Distributed Load Testing Using Kubernetes&amp;quot; sample web app
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;网络延迟测试&#34;&gt;网络延迟测试&lt;/h2&gt;

&lt;h3 id=&#34;场景一-kubernetes集群node节点上通过cluster-ip访问&#34;&gt;场景一、 Kubernetes集群node节点上通过Cluster IP访问&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;测试命令&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://10.254.149.31:8000/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;10组测试结果&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;No&lt;/th&gt;
&lt;th&gt;time_connect&lt;/th&gt;
&lt;th&gt;time_starttransfer&lt;/th&gt;
&lt;th&gt;time_total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.000&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;平均响应时间：2ms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;时间指标说明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;单位：秒&lt;/p&gt;

&lt;p&gt;time_connect：建立到服务器的 TCP 连接所用的时间&lt;/p&gt;

&lt;p&gt;time_starttransfer：在发出请求之后，Web 服务器返回数据的第一个字节所用的时间&lt;/p&gt;

&lt;p&gt;time_total：完成请求所用的时间&lt;/p&gt;

&lt;h3 id=&#34;场景二-kubernetes集群内部通过service访问&#34;&gt;场景二、Kubernetes集群内部通过service访问&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;测试命令&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://sample-webapp:8000/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;10组测试结果&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;No&lt;/th&gt;
&lt;th&gt;time_connect&lt;/th&gt;
&lt;th&gt;time_starttransfer&lt;/th&gt;
&lt;th&gt;time_total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;平均响应时间：6ms&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;场景三-在公网上通过traefik-ingress访问&#34;&gt;场景三、在公网上通过traefik ingress访问&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;测试命令&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://traefik.sample-webapp.io&amp;quot; &amp;gt;&amp;gt;result
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;10组测试结果&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;No&lt;/th&gt;
&lt;th&gt;time_connect&lt;/th&gt;
&lt;th&gt;time_starttransfer&lt;/th&gt;
&lt;th&gt;time_total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.043&lt;/td&gt;
&lt;td&gt;0.085&lt;/td&gt;
&lt;td&gt;0.085&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.052&lt;/td&gt;
&lt;td&gt;0.093&lt;/td&gt;
&lt;td&gt;0.093&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.043&lt;/td&gt;
&lt;td&gt;0.082&lt;/td&gt;
&lt;td&gt;0.082&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.051&lt;/td&gt;
&lt;td&gt;0.093&lt;/td&gt;
&lt;td&gt;0.093&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.068&lt;/td&gt;
&lt;td&gt;0.188&lt;/td&gt;
&lt;td&gt;0.188&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0.049&lt;/td&gt;
&lt;td&gt;0.089&lt;/td&gt;
&lt;td&gt;0.089&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.051&lt;/td&gt;
&lt;td&gt;0.113&lt;/td&gt;
&lt;td&gt;0.113&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.055&lt;/td&gt;
&lt;td&gt;0.120&lt;/td&gt;
&lt;td&gt;0.120&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.065&lt;/td&gt;
&lt;td&gt;0.126&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.050&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;td&gt;0.111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;平均响应时间：110ms&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;测试结果&#34;&gt;测试结果&lt;/h3&gt;

&lt;p&gt;在这三种场景下的响应时间测试结果如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes集群node节点上通过Cluster IP方式访问：2ms&lt;/li&gt;
&lt;li&gt;Kubernetes集群内部通过service访问：6ms&lt;/li&gt;
&lt;li&gt;Kubernetes集群外部通过traefik ingress暴露的地址访问：110ms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注意：执行测试的node节点/Pod与serivce所在的pod的距离（是否在同一台主机上），对前两个场景可以能会有一定影响。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;网络性能测试&#34;&gt;网络性能测试&lt;/h2&gt;

&lt;p&gt;网络使用flannel的vxlan模式。&lt;/p&gt;

&lt;p&gt;使用iperf进行测试。&lt;/p&gt;

&lt;p&gt;服务端命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;iperf -s -p 12345 -i 1 -M
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;客户端命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;iperf -c ${server-ip} -p 12345 -i 1 -t 10 -w 20K
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;场景一-主机之间&#34;&gt;场景一、主机之间&lt;/h3&gt;

&lt;p&gt;[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   598 MBytes  5.02 Gbits/sec
[  3]  1.0- 2.0 sec   637 MBytes  5.35 Gbits/sec
[  3]  2.0- 3.0 sec   664 MBytes  5.57 Gbits/sec
[  3]  3.0- 4.0 sec   657 MBytes  5.51 Gbits/sec
[  3]  4.0- 5.0 sec   641 MBytes  5.38 Gbits/sec
[  3]  5.0- 6.0 sec   639 MBytes  5.36 Gbits/sec
[  3]  6.0- 7.0 sec   628 MBytes  5.26 Gbits/sec
[  3]  7.0- 8.0 sec   649 MBytes  5.44 Gbits/sec
[  3]  8.0- 9.0 sec   638 MBytes  5.35 Gbits/sec
[  3]  9.0-10.0 sec   652 MBytes  5.47 Gbits/sec
&lt;strong&gt;[  3]  0.0-10.0 sec  6.25 GBytes  5.37 Gbits/sec&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;场景二-不同主机的的pod之间&#34;&gt;场景二、不同主机的的Pod之间&lt;/h3&gt;

&lt;p&gt;[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   372 MBytes  3.12 Gbits/sec
[  3]  1.0- 2.0 sec   345 MBytes  2.89 Gbits/sec
[  3]  2.0- 3.0 sec   361 MBytes  3.03 Gbits/sec
[  3]  3.0- 4.0 sec   397 MBytes  3.33 Gbits/sec
[  3]  4.0- 5.0 sec   405 MBytes  3.40 Gbits/sec
[  3]  5.0- 6.0 sec   410 MBytes  3.44 Gbits/sec
[  3]  6.0- 7.0 sec   404 MBytes  3.39 Gbits/sec
[  3]  7.0- 8.0 sec   408 MBytes  3.42 Gbits/sec
[  3]  8.0- 9.0 sec   451 MBytes  3.78 Gbits/sec
[  3]  9.0-10.0 sec   387 MBytes  3.25 Gbits/sec
&lt;strong&gt;[  3]  0.0-10.0 sec  3.85 GBytes  3.30 Gbits/sec&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;场景三-node与非同主机的pod之间&#34;&gt;场景三、Node与非同主机的Pod之间&lt;/h3&gt;

&lt;p&gt;[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   372 MBytes  3.12 Gbits/sec
[  3]  1.0- 2.0 sec   420 MBytes  3.53 Gbits/sec
[  3]  2.0- 3.0 sec   434 MBytes  3.64 Gbits/sec
[  3]  3.0- 4.0 sec   409 MBytes  3.43 Gbits/sec
[  3]  4.0- 5.0 sec   382 MBytes  3.21 Gbits/sec
[  3]  5.0- 6.0 sec   408 MBytes  3.42 Gbits/sec
[  3]  6.0- 7.0 sec   403 MBytes  3.38 Gbits/sec
[  3]  7.0- 8.0 sec   423 MBytes  3.55 Gbits/sec
[  3]  8.0- 9.0 sec   376 MBytes  3.15 Gbits/sec
[  3]  9.0-10.0 sec   451 MBytes  3.78 Gbits/sec
&lt;strong&gt;[  3]  0.0-10.0 sec  3.98 GBytes  3.42 Gbits/sec&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;网络性能对比综述&#34;&gt;网络性能对比综述&lt;/h3&gt;

&lt;p&gt;使用Flannel的&lt;strong&gt;vxlan&lt;/strong&gt;模式实现每个pod一个IP的方式，会比宿主机直接互联的网络性能损耗30%～40%，符合网上流传的测试结论。&lt;/p&gt;

&lt;h2 id=&#34;kubernete的性能测试&#34;&gt;Kubernete的性能测试&lt;/h2&gt;

&lt;p&gt;参考&lt;a href=&#34;https://supereagle.github.io/2017/03/09/kubemark/&#34;&gt;Kubernetes集群性能测试&lt;/a&gt;中的步骤，对kubernetes的性能进行测试。&lt;/p&gt;

&lt;p&gt;我的集群版本是Kubernetes1.6.0，首先克隆代码，将kubernetes目录复制到&lt;code&gt;$GOPATH/src/k8s.io/&lt;/code&gt;下然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./hack/generate-bindata.sh
/usr/local/src/k8s.io/kubernetes /usr/local/src/k8s.io/kubernetes
Generated bindata file : test/e2e/generated/bindata.go has 13498 test/e2e/generated/bindata.go lines of lovely automated artifacts
No changes in generated bindata file: pkg/generated/bindata.go
/usr/local/src/k8s.io/kubernetes
$ make WHAT=&amp;quot;test/e2e/e2e.test&amp;quot;
...
+++ [0425 17:01:34] Generating bindata:
    test/e2e/generated/gobindata_util.go
/usr/local/src/k8s.io/kubernetes /usr/local/src/k8s.io/kubernetes/test/e2e/generated
/usr/local/src/k8s.io/kubernetes/test/e2e/generated
+++ [0425 17:01:34] Building go targets for linux/amd64:
    test/e2e/e2e.test
$ make ginkgo
+++ [0425 17:05:57] Building the toolchain targets:
    k8s.io/kubernetes/hack/cmd/teststale
    k8s.io/kubernetes/vendor/github.com/jteeuwen/go-bindata/go-bindata
+++ [0425 17:05:57] Generating bindata:
    test/e2e/generated/gobindata_util.go
/usr/local/src/k8s.io/kubernetes /usr/local/src/k8s.io/kubernetes/test/e2e/generated
/usr/local/src/k8s.io/kubernetes/test/e2e/generated
+++ [0425 17:05:58] Building go targets for linux/amd64:
    vendor/github.com/onsi/ginkgo/ginkgo

$ export KUBERNETES_PROVIDER=local
$ export KUBECTL_PATH=/usr/bin/kubectl
$ go run hack/e2e.go -v -test  --test_args=&amp;quot;--host=http://172.20.0.113:8080 --ginkgo.focus=\[Feature:Performance\]&amp;quot; &amp;gt;&amp;gt;log.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;测试结果&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Apr 25 18:27:31.461: INFO: API calls latencies: {
  &amp;quot;apicalls&amp;quot;: [
    {
      &amp;quot;resource&amp;quot;: &amp;quot;pods&amp;quot;,
      &amp;quot;verb&amp;quot;: &amp;quot;POST&amp;quot;,
      &amp;quot;latency&amp;quot;: {
        &amp;quot;Perc50&amp;quot;: 2148000,
        &amp;quot;Perc90&amp;quot;: 13772000,
        &amp;quot;Perc99&amp;quot;: 14436000,
        &amp;quot;Perc100&amp;quot;: 0
      }
    },
    {
      &amp;quot;resource&amp;quot;: &amp;quot;services&amp;quot;,
      &amp;quot;verb&amp;quot;: &amp;quot;DELETE&amp;quot;,
      &amp;quot;latency&amp;quot;: {
        &amp;quot;Perc50&amp;quot;: 9843000,
        &amp;quot;Perc90&amp;quot;: 11226000,
        &amp;quot;Perc99&amp;quot;: 12391000,
        &amp;quot;Perc100&amp;quot;: 0
      }
    },
    ...
Apr 25 18:27:31.461: INFO: [Result:Performance] {
  &amp;quot;version&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;dataItems&amp;quot;: [
    {
      &amp;quot;data&amp;quot;: {
        &amp;quot;Perc50&amp;quot;: 2.148,
        &amp;quot;Perc90&amp;quot;: 13.772,
        &amp;quot;Perc99&amp;quot;: 14.436
      },
      &amp;quot;unit&amp;quot;: &amp;quot;ms&amp;quot;,
      &amp;quot;labels&amp;quot;: {
        &amp;quot;Resource&amp;quot;: &amp;quot;pods&amp;quot;,
        &amp;quot;Verb&amp;quot;: &amp;quot;POST&amp;quot;
      }
    },
...
2.857: INFO: Running AfterSuite actions on all node
Apr 26 10:35:32.857: INFO: Running AfterSuite actions on node 1

Ran 2 of 606 Specs in 268.371 seconds
SUCCESS! -- 2 Passed | 0 Failed | 0 Pending | 604 Skipped PASS

Ginkgo ran 1 suite in 4m28.667870101s
Test Suite Passed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从kubemark输出的日志中可以看到&lt;strong&gt;API calls latencies&lt;/strong&gt;和&lt;strong&gt;Performance&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;另外还有一个将测试结果画图的工具&lt;a href=&#34;https://github.com/coreos/kscale/blob/dfe65f050cff5bebf83074e3b3e6b3c2d69a9222/logplot/main.go&#34;&gt;plot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubenetes-e2e-test.jpg&#34; alt=&#34;kubernetes-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;注意事项&#34;&gt;注意事项&lt;/h3&gt;

&lt;p&gt;测试过程中需要用到docker镜像存储在GCE中，需要翻墙下载，我没看到哪里配置这个镜像的地址。该镜像副本已上传时速云：&lt;/p&gt;

&lt;p&gt;用到的镜像有如下两个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcr.io/google_containers/pause-amd64:3.0&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/serve_hostname:v1.4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;时速云镜像地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/pause-amd64:3.0&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/serve_hostname:v1.4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将镜像pull到本地后重新打tag。&lt;/p&gt;

&lt;h2 id=&#34;locust测试&#34;&gt;Locust测试&lt;/h2&gt;

&lt;p&gt;请求统计&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;# requests&lt;/th&gt;
&lt;th&gt;# failures&lt;/th&gt;
&lt;th&gt;Median response time&lt;/th&gt;
&lt;th&gt;Average response time&lt;/th&gt;
&lt;th&gt;Min response time&lt;/th&gt;
&lt;th&gt;Max response time&lt;/th&gt;
&lt;th&gt;Average Content Size&lt;/th&gt;
&lt;th&gt;Requests/s&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;POST&lt;/td&gt;
&lt;td&gt;/login&lt;/td&gt;
&lt;td&gt;5070&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;td&gt;59000&lt;/td&gt;
&lt;td&gt;80551&lt;/td&gt;
&lt;td&gt;11218&lt;/td&gt;
&lt;td&gt;202140&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;1.17&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;POST&lt;/td&gt;
&lt;td&gt;/metrics&lt;/td&gt;
&lt;td&gt;5114232&lt;/td&gt;
&lt;td&gt;85879&lt;/td&gt;
&lt;td&gt;63000&lt;/td&gt;
&lt;td&gt;82280&lt;/td&gt;
&lt;td&gt;29518&lt;/td&gt;
&lt;td&gt;331330&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;1178.77&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;5119302&lt;/td&gt;
&lt;td&gt;85957&lt;/td&gt;
&lt;td&gt;63000&lt;/td&gt;
&lt;td&gt;82279&lt;/td&gt;
&lt;td&gt;11218&lt;/td&gt;
&lt;td&gt;331330&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;1179.94&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;响应时间分布&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;# requests&lt;/th&gt;
&lt;th&gt;50%&lt;/th&gt;
&lt;th&gt;66%&lt;/th&gt;
&lt;th&gt;75%&lt;/th&gt;
&lt;th&gt;80%&lt;/th&gt;
&lt;th&gt;90%&lt;/th&gt;
&lt;th&gt;95%&lt;/th&gt;
&lt;th&gt;98%&lt;/th&gt;
&lt;th&gt;99%&lt;/th&gt;
&lt;th&gt;100%&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;POST /login&lt;/td&gt;
&lt;td&gt;5070&lt;/td&gt;
&lt;td&gt;59000&lt;/td&gt;
&lt;td&gt;125000&lt;/td&gt;
&lt;td&gt;140000&lt;/td&gt;
&lt;td&gt;148000&lt;/td&gt;
&lt;td&gt;160000&lt;/td&gt;
&lt;td&gt;166000&lt;/td&gt;
&lt;td&gt;174000&lt;/td&gt;
&lt;td&gt;176000&lt;/td&gt;
&lt;td&gt;202140&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;POST /metrics&lt;/td&gt;
&lt;td&gt;5114993&lt;/td&gt;
&lt;td&gt;63000&lt;/td&gt;
&lt;td&gt;127000&lt;/td&gt;
&lt;td&gt;142000&lt;/td&gt;
&lt;td&gt;149000&lt;/td&gt;
&lt;td&gt;160000&lt;/td&gt;
&lt;td&gt;166000&lt;/td&gt;
&lt;td&gt;172000&lt;/td&gt;
&lt;td&gt;176000&lt;/td&gt;
&lt;td&gt;331330&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;None Total&lt;/td&gt;
&lt;td&gt;5120063&lt;/td&gt;
&lt;td&gt;63000&lt;/td&gt;
&lt;td&gt;127000&lt;/td&gt;
&lt;td&gt;142000&lt;/td&gt;
&lt;td&gt;149000&lt;/td&gt;
&lt;td&gt;160000&lt;/td&gt;
&lt;td&gt;166000&lt;/td&gt;
&lt;td&gt;172000&lt;/td&gt;
&lt;td&gt;176000&lt;/td&gt;
&lt;td&gt;331330&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;以上两个表格都是瞬时值。请求失败率在2%左右。&lt;/p&gt;

&lt;p&gt;Sample-webapp起了48个pod。&lt;/p&gt;

&lt;p&gt;Locust模拟10万用户，每秒增长100个。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-locust-test.jpg&#34; alt=&#34;locust-test&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://testerhome.com/topics/4839&#34;&gt;基于 Python 的性能测试工具 locust (与 LR 的简单对比)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://docs.locust.io/en/latest/what-is-locust.html&#34;&gt;Locust docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://timd.cn/2015/09/17/locust/&#34;&gt;python用户负载测试工具：locust&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://supereagle.github.io/2017/03/09/kubemark/&#34;&gt;Kubernetes集群性能测试&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/1050&#34;&gt;CoreOS是如何将Kubernetes的性能提高10倍的&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.fleeto.us/translation/updates-performance-and-scalability-kubernetes-13-2000-node-60000-pod-clusters&#34;&gt;Kubernetes 1.3 的性能和弹性 —— 2000 节点，60,0000 Pod 的集群&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.csdn.net/article/2015-07-07/2825155&#34;&gt;运用Kubernetes进行分布式负载测试&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/kubemark-guide.md&#34;&gt;Kubemark User Guide&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>运用kubernetes进行分布式负载测试</title>
      <link>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 21:32:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160325002.jpg&#34; alt=&#34;kubrick&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：Kubrick Book Store  Mar 25,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;Github地址&lt;a href=&#34;https://github.com/rootsongjc/distributed-load-testing-using-kubernetes&#34;&gt;https://github.com/rootsongjc/distributed-load-testing-using-kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该教程描述如何在&lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt;中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看&lt;a href=&#34;http://cloud.google.com/solutions/distributed-load-testing-using-kubernetes&#34;&gt;Distributed Load Testing Using Kubernetes&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;不需要GCE及其他组件，你只需要有一个kubernetes集群即可。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果你还没有kubernetes集群，可以参考&lt;a href=&#34;https://www.gitbook.com/book/rootsongjc/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;部署一个。&lt;/p&gt;

&lt;h2 id=&#34;部署web应用&#34;&gt;部署Web应用&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;sample-webapp&lt;/code&gt; 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像&lt;code&gt;index.tenxcloud.com/jimmy/k8s-sample-webapp:latest&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;在kubernetes上部署sample-webapp。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd kubernetes-config
$ kubectl create -f sample-webapp-controller.yaml
$ kubectl create -f kubectl create -f sample-webapp-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;部署locust的controller和service&#34;&gt;部署Locust的Controller和Service&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;locust-master&lt;/code&gt;和&lt;code&gt;locust-work&lt;/code&gt;使用同样的docker镜像，修改cotnroller中&lt;code&gt;spec.template.spec.containers.env&lt;/code&gt;字段中的value为你&lt;code&gt;sample-webapp&lt;/code&gt; service的名字。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: TARGET_HOST
  value: http://sample-webapp:8000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;创建controller-docker镜像-可选&#34;&gt;创建Controller Docker镜像（可选）&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;locust-master&lt;/code&gt;和&lt;code&gt;locust-work&lt;/code&gt; controller使用的都是&lt;code&gt;locust-tasks&lt;/code&gt; docker镜像。你可以直接下载&lt;code&gt;gcr.io/cloud-solutions-http://olz1di9xf.bkt.clouddn.com/locust-tasks&lt;/code&gt;，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest .
$ docker push index.tenxcloud.com/jimmy/locust-tasks:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：我使用的是时速云的镜像仓库。&lt;/p&gt;

&lt;p&gt;每个controller的yaml的&lt;code&gt;spec.template.spec.containers.image&lt;/code&gt; 字段指定的是我的镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;image: index.tenxcloud.com/jimmy/locust-tasks:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;部署locust-master&#34;&gt;部署locust-master&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f locust-master-controller.yaml
$ kubectl create -f locust-master-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;部署locust-worker&#34;&gt;部署locust-worker&lt;/h3&gt;

&lt;p&gt;Now deploy &lt;code&gt;locust-worker-controller&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f locust-worker-controller.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以很轻易的给work扩容，通过命令行方式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl scale --replicas=20 replicationcontrollers locust-worker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然你也可以通过WebUI：Dashboard - Workloads - Replication Controllers - &lt;strong&gt;ServiceName&lt;/strong&gt; - Scale来扩容。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/dashbaord-scale.jpg&#34; alt=&#34;dashboard-scale&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置traefik&#34;&gt;配置Traefik&lt;/h3&gt;

&lt;p&gt;参考&lt;a href=&#34;http://rootsongjc.github.io/blogs/traefik-ingress-installation/&#34;&gt;kubernetes的traefik ingress安装&lt;/a&gt;，在&lt;code&gt;ingress.yaml&lt;/code&gt;中加入如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;  - host: traefik.locust.io
    http:
      paths:
      - path: /
        backend:
          serviceName: locust-master
          servicePort: 8089
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行&lt;code&gt;kubectl replace -f ingress.yaml&lt;/code&gt;即可更新traefik。&lt;/p&gt;

&lt;p&gt;通过Traefik的dashboard就可以看到刚增加的&lt;code&gt;traefik.locust.io&lt;/code&gt;节点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/traefik-dashboard-locust.jpg&#34; alt=&#34;traefik-dashboard-locust&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;执行测试&#34;&gt;执行测试&lt;/h2&gt;

&lt;p&gt;打开&lt;code&gt;http://traefik.locust.io&lt;/code&gt;页面，点击&lt;code&gt;Edit&lt;/code&gt;输入伪造的用户数和用户每秒发送的请求个数，点击&lt;code&gt;Start Swarming&lt;/code&gt;就可以开始测试了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/locust-start-swarming.jpg&#34; alt=&#34;locust-start-swarming&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在测试过程中调整&lt;code&gt;sample-webapp&lt;/code&gt;的pod个数（默认设置了1个pod），观察pod的负载变化情况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/sample-webapp-rc.jpg&#34; alt=&#34;sample-webapp-rc&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从一段时间的观察中可以看到负载被平均分配给了3个pod。&lt;/p&gt;

&lt;p&gt;在locust的页面中可以实时观察也可以下载测试结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/locust-dashboard.jpg&#34; alt=&#34;locust-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/solutions/distributed-load-testing-using-kubernetes&#34;&gt;Distributed Load Testing Using Kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.csdn.net/article/2015-07-07/2825155&#34;&gt;运用Kubernetes进行分布式负载测试&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes中的IP和服务发现体系</title>
      <link>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 16:11:16 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20151108011.jpg&#34; alt=&#34;朝阳公园&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：路边的野花@朝阳公园 Nov 8,2015）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;cluster-ip&#34;&gt;Cluster IP&lt;/h2&gt;

&lt;p&gt;即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Service Name才是对外暴露服务的关键。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在kubeapi的配置中指定该地址范围。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;默认配置&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--service-cluster-ip-range=10.254.0.0/16
--service-node-port-range=30000-32767
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pod-ip&#34;&gt;Pod IP&lt;/h2&gt;

&lt;p&gt;通过配置flannel的&lt;code&gt;network&lt;/code&gt;和&lt;code&gt;subnet&lt;/code&gt;来实现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;默认配置&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.46.1/24
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod的IP地址&lt;u&gt;不固定&lt;/u&gt;，当pod重启时IP地址会变化。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;该IP地址也是用户无需关心的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。&lt;/p&gt;

&lt;h2 id=&#34;主机-node-ip&#34;&gt;主机/Node IP&lt;/h2&gt;

&lt;p&gt;物理机的IP地址，即kubernetes管理的物理机的IP地址。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
NAME           STATUS    AGE       VERSION
172.20.0.113   Ready     12d       v1.6.0
172.20.0.114   Ready     12d       v1.6.0
172.20.0.115   Ready     12d       v1.6.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;集群内部的服务发现&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。&lt;/p&gt;

&lt;p&gt;通过&lt;code&gt;sericename:port&lt;/code&gt;即可调用服务。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;服务外部的服务发现&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过Ingress来实现，我们是用的&lt;strong&gt;Traefik&lt;/strong&gt;来实现。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/&#34;&gt;Ingress解析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://rootsongjc.github.io/blogs/traefik-ingress-installation/&#34;&gt;Kubernetes Traefik Ingress安装试用&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes中的RBAC支持</title>
      <link>http://rootsongjc.github.io/blogs/RBAC-support-in-kubernetes/</link>
      <pubDate>Fri, 21 Apr 2017 19:52:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/RBAC-support-in-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160402017.jpg&#34; alt=&#34;无题&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：无题 Apr 2,2016）&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;在Kubernetes1.6版本中新增角色访问控制机制（Role-Based Access，RBAC）让集群管理员可以针对特定使用者或服务账号的角色，进行更精确的资源访问控制。在RBAC中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。在一个组织中，角色是为了完成各种工作而创造，用户则依据它的责任和资格来被指派相应的角色，用户可以很容易地从一个角色被指派到另一个角色。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;前言&#34;&gt;前言&lt;/h3&gt;

&lt;p&gt;本文翻译自&lt;a href=&#34;http://blog.kubernetes.io/2017/04/rbac-support-in-kubernetes.html&#34;&gt;RBAC Support in Kubernetes&lt;/a&gt;，转载自&lt;a href=&#34;https://www.kubernetes.org.cn/1879.html&#34;&gt;kubernetes中文社区&lt;/a&gt;，译者催总，&lt;a href=&#34;http://rootsongjc.github.com/about&#34;&gt;Jimmy Song&lt;/a&gt;做了稍许修改。该文章是&lt;a href=&#34;http://blog.kubernetes.io/2017/03/five-days-of-kubernetes-1.6.html&#34;&gt;5天内了解Kubernetes1.6新特性&lt;/a&gt;的系列文章之一。&lt;/p&gt;

&lt;p&gt;One of the highlights of the &lt;a href=&#34;http://blog.kubernetes.io/2017/03/kubernetes-1.6-multi-user-multi-workloads-at-scale.html&#34;&gt;Kubernetes 1.6&lt;/a&gt;中的一个亮点时RBAC访问控制机制升级到了beta版本。RBAC，基于角色的访问控制机制，是用来管理kubernetes集群中资源访问权限的机制。使用RBAC可以很方便的更新访问授权策略而不用重启集群。&lt;/p&gt;

&lt;p&gt;本文主要关注新特性和最佳实践。&lt;/p&gt;

&lt;h3 id=&#34;rbac-vs-abac&#34;&gt;RBAC vs ABAC&lt;/h3&gt;

&lt;p&gt;目前kubernetes中已经有一系列l &lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/&#34;&gt;鉴权机制&lt;/a&gt;。鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。&lt;/p&gt;

&lt;p&gt;ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。&lt;/p&gt;

&lt;p&gt;而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。&lt;strong&gt;RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。&lt;/strong&gt;RBAC 在 Kubernetes 中被映射为 API 资源和操作。&lt;/p&gt;

&lt;p&gt;因为 Kubernetes 社区的投入和偏好，相对于 ABAC 而言，RBAC 是更好的选择。&lt;/p&gt;

&lt;h3 id=&#34;基础概念&#34;&gt;基础概念&lt;/h3&gt;

&lt;p&gt;需要理解 RBAC 一些基础的概念和思路，RBAC 是让用户能够访问 &lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.6/&#34;&gt;Kubernetes API 资源&lt;/a&gt;的授权方式。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://1.bp.blogspot.com/-v6KLs1tT_xI/WOa0anGP4sI/AAAAAAAABBo/KIgYfp8PjusuykUVTfgu9-2uKj_wXo4lwCLcB/s1600/rbac1.png&#34;&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-v6KLs1tT_xI/WOa0anGP4sI/AAAAAAAABBo/KIgYfp8PjusuykUVTfgu9-2uKj_wXo4lwCLcB/s400/rbac1.png&#34; alt=&#34;img&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在 RBAC 中定义了两个对象，用于描述在用户和资源之间的连接权限。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;role&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;角色是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限， ClusterRole 跟 Role 类似，但是可以在集群中到处使用（ Role 是 namespace 一级的）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;role binding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RoleBinding 把角色映射到用户，从而让这些用户继承角色在 namespace 中的权限。ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://1.bp.blogspot.com/-ixDe91-cnqw/WOa0auxC0mI/AAAAAAAABBs/4LxVsr6shEgTYqUapt5QPISUeuTuztVwwCEw/s1600/rbac2.png&#34;&gt;&lt;img src=&#34;https://1.bp.blogspot.com/-ixDe91-cnqw/WOa0auxC0mI/AAAAAAAABBs/4LxVsr6shEgTYqUapt5QPISUeuTuztVwwCEw/s640/rbac2.png&#34; alt=&#34;img&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;另外还要考虑cluster roles和cluster role binding。cluster role和cluster role binding方法跟role和role binding一样，出了它们有更广的scope。详细差别请访问 &lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#rolebinding-and-clusterrolebinding&#34;&gt;role binding与clsuter role binding&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;kubernetes中的rbac&#34;&gt;Kubernetes中的RBAC&lt;/h3&gt;

&lt;p&gt;RBAC 现在被 Kubernetes 深度集成，并使用他给系统组件进行授权。&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#default-roles-and-role-bindings&#34;&gt;System Roles&lt;/a&gt; 一般具有前缀&lt;code&gt;system:&lt;/code&gt;，很容易识别：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get clusterroles --namespace=kube-system
NAME                                           AGE
admin                                          10d
cluster-admin                                  10d
edit                                           10d
system:auth-delegator                          10d
system:basic-user                              10d
system:controller:attachdetach-controller      10d
system:controller:certificate-controller       10d
system:controller:cronjob-controller           10d
system:controller:daemon-set-controller        10d
system:controller:deployment-controller        10d
system:controller:disruption-controller        10d
system:controller:endpoint-controller          10d
system:controller:generic-garbage-collector    10d
system:controller:horizontal-pod-autoscaler    10d
system:controller:job-controller               10d
system:controller:namespace-controller         10d
system:controller:node-controller              10d
system:controller:persistent-volume-binder     10d
system:controller:pod-garbage-collector        10d
system:controller:replicaset-controller        10d
system:controller:replication-controller       10d
system:controller:resourcequota-controller     10d
system:controller:route-controller             10d
system:controller:service-account-controller   10d
system:controller:service-controller           10d
system:controller:statefulset-controller       10d
system:controller:ttl-controller               10d
system:discovery                               10d
system:heapster                                10d
system:kube-aggregator                         10d
system:kube-controller-manager                 10d
system:kube-dns                                10d
system:kube-scheduler                          10d
system:node                                    10d
system:node-bootstrapper                       10d
system:node-problem-detector                   10d
system:node-proxier                            10d
system:persistent-volume-provisioner           10d
view                                           10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RBAC 系统角色已经完成足够的覆盖，让集群可以完全在 RBAC 的管理下运行。&lt;/p&gt;

&lt;p&gt;在 ABAC 到 RBAC 进行迁移的过程中，有些在 ABAC 集群中缺省开放的权限，在 RBAC 中会被视为不必要的授权，会对其进行&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#upgrading-from-15&#34;&gt;降级&lt;/a&gt;。这种情况会影响到使用 Service Account 的负载。ABAC 配置中，从 Pod 中发出的请求会使用 Pod Token，API Server 会为其授予较高权限。例如下面的命令在 ABAC 集群中会返回 JSON 结果，而在 RBAC 的情况下则会返回错误。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl run nginx --image=nginx:latest
$ kubectl exec -it $(kubectl get pods -o jsonpath=&#39;{.items[0].metadata.name}&#39;) bash
$ apt-get update &amp;amp;&amp;amp; apt-get install -y curl
$ curl -ik \
  -H &amp;quot;Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&amp;quot; \
  https://kubernetes/api/v1/namespaces/default/pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有在 Kubernetes 集群中运行的应用，一旦和 API Server 进行通信，都会有可能受到迁移的影响。&lt;/p&gt;

&lt;p&gt;要平滑的从 ABAC 升级到 RBAC，在创建 1.6 集群的时候，可以同时启用&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#parallel-authorizers&#34;&gt;ABAC 和 RBAC&lt;/a&gt;。当他们同时启用的时候，对一个资源的权限请求，在任何一方获得放行都会获得批准。然而在这种配置下的权限太过粗放，很可能无法在单纯的 RBAC 环境下工作。&lt;/p&gt;

&lt;p&gt;目前RBAC已经足够了，ABAC可能会被弃用。在可见的未来ABAC依然会保留在kubernetes中，不过开发的重心已经转移到了RBAC。&lt;/p&gt;

&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/&#34;&gt;RBAC documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Cd4JU7qzYbE#t=8m01s&#34;&gt;Google Cloud Next talks 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=18P7cFc6nTU#t=41m06s&#34;&gt;Google Cloud Next talks 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/&#34;&gt;在Kubernetes Pod中使用Service Account访问API Server&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes traefik ingress安装试用</title>
      <link>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</link>
      <pubDate>Thu, 20 Apr 2017 22:38:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160915046.jpg&#34; alt=&#34;fish&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：🐟@鱼缸 Sep 15,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;昨天翻了下&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/&#34;&gt;Ingress解析&lt;/a&gt;，然后安装试用了下&lt;a href=&#34;https://traefik.io&#34;&gt;traefik&lt;/a&gt;，过程已同步到&lt;a href=&#34;https://www.gitbook.com/book/rootsongjc/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;上，Github地址&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;https://github.com/rootsongjc/kubernetes-handbook&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;ingress简介&#34;&gt;Ingress简介&lt;/h2&gt;

&lt;p&gt;如果你还不了解，ingress是什么，可以先看下我翻译的Kubernetes官网上ingress的介绍&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/&#34;&gt;Kubernetes Ingress解析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;理解Ingress&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers&#34;&gt;Ingress controller&lt;/a&gt;来提供。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;理解Ingress Controller&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。&lt;/p&gt;

&lt;h2 id=&#34;部署traefik&#34;&gt;部署Traefik&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;介绍traefik&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://traefik.io/&#34;&gt;Traefik&lt;/a&gt;是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。&lt;/p&gt;

&lt;p&gt;以下配置文件可以在&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;GitHub仓库中的&lt;a href=&#34;manifests/traefik-ingress/&#34;&gt;manifests/traefik-ingress/&lt;/a&gt;目录下找到。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建ingress-rbac.yaml&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将用于service account验证。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: ingress
subjects:
  - kind: ServiceAccount
    name: ingress
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建名为&lt;code&gt;traefik-ingress&lt;/code&gt;的ingress&lt;/strong&gt;，文件名traefik.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-ingress
spec:
  rules:
  - host: traefik.nginx.io
    http:
      paths:
      - path: /
        backend:
          serviceName: my-nginx
          servicePort: 80
  - host: traefik.frontend.io
    http:
      paths:
      - path: /
        backend:
          serviceName: frontend
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这其中的&lt;code&gt;backend&lt;/code&gt;中要配置default namespace中启动的service名字。&lt;code&gt;path&lt;/code&gt;就是URL地址后的路径，如traefik.frontend.io/path，service将会接受path这个路径，host最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。&lt;/p&gt;

&lt;p&gt;根据你自己环境中部署的service的名字和端口自行修改，有新service增加时，修改该文件后可以使用&lt;code&gt;kubectl replace -f traefik.yaml&lt;/code&gt;来更新。&lt;/p&gt;

&lt;p&gt;我们现在集群中已经有两个service了，一个是nginx，另一个是官方的&lt;code&gt;guestbook&lt;/code&gt;例子。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建Depeloyment&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: traefik-ingress-lb
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      hostNetwork: true
      restartPolicy: Always
      serviceAccountName: ingress
      containers:
      - image: traefik
        name: traefik-ingress-lb
        resources:
          limits:
            cpu: 200m
            memory: 30Mi
          requests:
            cpu: 100m
            memory: 20Mi
        ports:
        - name: http
          containerPort: 80
          hostPort: 80
        - name: admin
          containerPort: 8580
          hostPort: 8580
        args:
        - --web
        - --web.address=:8580
        - --kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意我们这里用的是Deploy类型，没有限定该pod运行在哪个主机上。Traefik的端口是8580。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Traefik UI&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8580
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  rules:
  - host: traefik-ui.local
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置完成后就可以启动treafik ingress了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我查看到traefik的pod在&lt;code&gt;172.20.0.115&lt;/code&gt;这台节点上启动了。&lt;/p&gt;

&lt;p&gt;访问该地址&lt;code&gt;http://172.20.0.115:8580/&lt;/code&gt;将可以看到dashboard。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/traefik-dashboard.jpg&#34; alt=&#34;kubernetes-dashboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左侧黄色部分部分列出的是所有的rule，右侧绿色部分是所有的backend。&lt;/p&gt;

&lt;h2 id=&#34;测试&#34;&gt;测试&lt;/h2&gt;

&lt;p&gt;在集群的任意一个节点上执行。假如现在我要访问nginx的&amp;rdquo;/&amp;ldquo;路径。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -H Host:traefik.nginx.io http://172.20.0.115/
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你需要在kubernetes集群以外访问就需要设置DNS，或者修改本机的hosts文件。&lt;/p&gt;

&lt;p&gt;在其中加入：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;172.20.0.115 traefik.nginx.io
172.20.0.115 traefik.frontend.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所有访问这些地址的流量都会发送给172.20.0.115这台主机，就是我们启动traefik的主机。&lt;/p&gt;

&lt;p&gt;Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。&lt;/p&gt;

&lt;p&gt;修改hosts后就就可以在kubernetes集群外访问以上两个service，如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/traefik-nginx.jpg&#34; alt=&#34;traefik-nginx&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/traefik-guestbook.jpg&#34; alt=&#34;traefik-guestbook&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.colabug.com/thread-1703745-1-1.html&#34;&gt;Traefik-kubernetes 初试&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.tuicool.com/articles/ZnuEfay&#34;&gt;Traefik简介&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook&#34;&gt;Guestbook example&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes ingress解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</link>
      <pubDate>Wed, 19 Apr 2017 21:05:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160131054.jpg&#34; alt=&#34;银河soho&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：朝阳门银河SOHO Jan 31,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是kubernete官方文档中&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress Resource&lt;/a&gt;的翻译，因为最近工作中用到，文章也不长，也很好理解，索性翻译一下，也便于自己加深理解，同时造福&lt;a href=&#34;https://www.kubernetes.org.cn/&#34;&gt;kubernetes中文社区&lt;/a&gt;。后续准备使用&lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt;来做Ingress controller，文章末尾给出了几个相关链接，实际使用案例正在摸索中，届时相关安装文档和配置说明将同步更新到&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;术语&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;节点：Kubernetes集群中的一台物理机或者虚拟机。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群网络：一组逻辑或物理链接，可根据Kubernetes&lt;a href=&#34;https://kubernetes.io/docs/admin/networking/&#34;&gt;网络模型&lt;/a&gt;实现群集内的通信。 集群网络的实现包括Overlay模型的 &lt;a href=&#34;https://github.com/coreos/flannel#flannel&#34;&gt;flannel&lt;/a&gt; 和基于SDN的&lt;a href=&#34;https://kubernetes.io/docs/admin/ovs-networking/&#34;&gt;OVS&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;服务：使用标签选择器标识一组pod成为的Kubernetes&lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/&#34;&gt;服务&lt;/a&gt;。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;什么是ingress&#34;&gt;什么是Ingress？&lt;/h2&gt;

&lt;p&gt;通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    internet
        |
  ------------
  [ Services ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ingress是授权入站连接到达集群服务的规则集合。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers&#34;&gt;Ingress controller&lt;/a&gt;负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。&lt;/p&gt;

&lt;h2 id=&#34;先决条件&#34;&gt;先决条件&lt;/h2&gt;

&lt;p&gt;在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个&lt;code&gt;Ingress Controller&lt;/code&gt;来实现&lt;code&gt;Ingress&lt;/code&gt;，单纯的创建一个&lt;code&gt;Ingress&lt;/code&gt;没有任何意义。&lt;/p&gt;

&lt;p&gt;GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers/nginx#running-multiple-ingress-controllers&#34;&gt;运行多个ingress controller&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/gce/BETA_LIMITATIONS.md#disabling-glbc&#34;&gt;关闭glbc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;确定你已经阅读了Ingress controller的&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/gce/BETA_LIMITATIONS.md&#34;&gt;beta版本限制&lt;/a&gt;。在非GCE/GKE的环境中，你需要在pod中&lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers&#34;&gt;部署一个controller&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;ingress-resource&#34;&gt;Ingress Resource&lt;/h2&gt;

&lt;p&gt;最简化的Ingress配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;1: apiVersion: extensions/v1beta1
2: kind: Ingress
3: metadata:
4:   name: test-ingress
5: spec:
6:   rules:
7:   - http:
8:       paths:
9:       - path: /testpath
10:        backend:
11:           serviceName: test
12:           servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;如果你没有配置Ingress controller就将其POST到API server不会有任何用处&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;配置说明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1-4行&lt;/strong&gt;：跟Kubernetes的其他配置一样，ingress的配置也需要&lt;code&gt;apiVersion&lt;/code&gt;，&lt;code&gt;kind&lt;/code&gt;和&lt;code&gt;metadata&lt;/code&gt;字段。配置文件的详细说明请查看&lt;a href=&#34;https://kubernetes.io/docs/user-guide/deploying-applications&#34;&gt;部署应用&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/user-guide/configuring-containers&#34;&gt;配置容器&lt;/a&gt;和 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/working-with-resources&#34;&gt;使用resources&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5-7行&lt;/strong&gt;: Ingress &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#spec-and-status&#34;&gt;spec&lt;/a&gt; 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8-9行&lt;/strong&gt;：每条http规则包含以下信息：一个&lt;code&gt;host&lt;/code&gt;配置项（比如for.bar.com，在这个例子中默认是*），&lt;code&gt;path&lt;/code&gt;列表（比如：/testpath），每个path都关联一个&lt;code&gt;backend&lt;/code&gt;(比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;10-12行&lt;/strong&gt;：正如 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services&#34;&gt;services doc&lt;/a&gt;中描述的那样，backend是一个&lt;code&gt;service:port&lt;/code&gt;的组合。Ingress的流量被转发到它所匹配的backend。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;全局参数&lt;/strong&gt;：为了简单起见，Ingress示例中没有全局参数，请参阅资源完整定义的&lt;a href=&#34;https://releases.k8s.io/master/pkg/apis/extensions/v1beta1/types.go&#34;&gt;api参考&lt;/a&gt;。 在所有请求都不能跟spec中的path匹配的情况下，请求被发送到Ingress controller的默认后端，可以指定全局缺省backend。&lt;/p&gt;

&lt;h2 id=&#34;ingress-controllers&#34;&gt;Ingress controllers&lt;/h2&gt;

&lt;p&gt;为了使Ingress正常工作，集群中必须运行Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为&lt;code&gt;kube-controller-manager&lt;/code&gt;二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的Ingress controller或者自己实现一个。 示例和说明可以在&lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers&#34;&gt;这里&lt;/a&gt;找到。&lt;/p&gt;

&lt;h2 id=&#34;在你开始前&#34;&gt;在你开始前&lt;/h2&gt;

&lt;p&gt;以下文档描述了Ingress资源中公开的一组跨平台功能。 理想情况下，所有的Ingress controller都应该符合这个规范，但是我们还没有实现。 GCE和nginx控制器的文档分别在&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md&#34;&gt;这里&lt;/a&gt;和&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md&#34;&gt;这里&lt;/a&gt;。&lt;strong&gt;确保您查看控制器特定的文档，以便您了解每个文档的注意事项。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;ingress类型&#34;&gt;Ingress类型&lt;/h2&gt;

&lt;h3 id=&#34;单service-ingress&#34;&gt;单Service Ingress&lt;/h3&gt;

&lt;p&gt;Kubernetes中已经存在一些概念可以暴露单个service（查看&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives&#34;&gt;替代方案&lt;/a&gt;），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。&lt;/p&gt;

&lt;p&gt;ingress.yaml定义文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;code&gt;kubectl create -f&lt;/code&gt;命令创建，然后查看ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get ing
NAME                RULE          BACKEND        ADDRESS
test-ingress        -             testsvc:80     107.178.254.228
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;107.178.254.228&lt;/code&gt;就是Ingress controller为了实现Ingress而分配的IP地址。&lt;code&gt;RULE&lt;/code&gt;列表示所有发送给该IP的流量都被转发到了&lt;code&gt;BACKEND&lt;/code&gt;所列的Kubernetes service上。&lt;/p&gt;

&lt;h3 id=&#34;简单展开&#34;&gt;简单展开&lt;/h3&gt;

&lt;p&gt;如前面描述的那样，kubernete pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，假如你想要创建这样的一个设置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foo.bar.com -&amp;gt; 178.91.123.132 -&amp;gt; / foo    s1:80
                                 / bar    s2:80

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你需要一个这样的ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: s1
          servicePort: 80
      - path: /bar
        backend:
          serviceName: s2
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;code&gt;kubectl create -f&lt;/code&gt;创建完ingress后：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -
          foo.bar.com
          /foo          s1:80
          /bar          s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只要服务（s1，s2）存在，Ingress controller就会将提供一个满足该Ingress的特定loadbalancer实现。 这一步完成后，您将在Ingress的最后一列看到loadbalancer的地址。&lt;/p&gt;

&lt;h3 id=&#34;基于名称的虚拟主机&#34;&gt;基于名称的虚拟主机&lt;/h3&gt;

&lt;p&gt;Name-based的虚拟主机在同一个IP地址下拥有多个主机名。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com s1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&amp;gt; bar.foo.com s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面这个ingress说明基于&lt;a href=&#34;https://tools.ietf.org/html/rfc7230#section-5.4&#34;&gt;Host header&lt;/a&gt;的后端loadbalancer的路由请求：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;默认backend&lt;/strong&gt;：一个没有rule的ingress，如前面章节中所示，所有流量都将发送到一个默认backend。你可以用该技巧通知loadbalancer如何找到你网站的404页面，通过制定一些列rule和一个默认backend的方式。如果请求header中的host不能跟ingress中的host匹配，并且/或请求的URL不能与任何一个path匹配，则流量将路由到你的默认backend。&lt;/p&gt;

&lt;h3 id=&#34;tls&#34;&gt;TLS&lt;/h3&gt;

&lt;p&gt;你可以通过指定包含TLS私钥和证书的&lt;a href=&#34;https://kubernetes.io/docs/user-guide/secrets&#34;&gt;secret&lt;/a&gt;来加密Ingress。 目前，Ingress仅支持单个TLS端口443，并假定TLS termination。 如果Ingress中的TLS配置部分指定了不同的主机，则它们将根据通过SNI TLS扩展指定的主机名（假如Ingress controller支持SNI）在多个相同端口上进行复用。 TLS secret中必须包含名为&lt;code&gt;tls.crt&lt;/code&gt;和&lt;code&gt;tls.key&lt;/code&gt;的密钥，这里面包含了用于TLS的证书和私钥，例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: testsecret
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Ingress中引用这个secret将通知Ingress controller使用TLS加密从将客户端到loadbalancer的channel：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: no-rules-map
spec:
  tls:
    - secretName: testsecret
  backend:
    serviceName: s1
    servicePort: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请注意，各种Ingress controller支持的TLS功能之间存在差距。 请参阅有关&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md#https&#34;&gt;nginx&lt;/a&gt;，&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#tls&#34;&gt;GCE&lt;/a&gt;或任何其他平台特定Ingress controller的文档，以了解TLS在你的环境中的工作原理。&lt;/p&gt;

&lt;p&gt;Ingress controller启动时附带一些适用于所有Ingress的负载平衡策略设置，例如负载均衡算法，后端权重方案等。更高级的负载平衡概念（例如持久会话，动态权重）尚未在Ingress中公开。 你仍然可以通过&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/service-loadbalancer&#34;&gt;service loadbalancer&lt;/a&gt;获取这些功能。 随着时间的推移，我们计划将适用于跨平台的负载平衡模式加入到Ingress资源中。&lt;/p&gt;

&lt;p&gt;还值得注意的是，尽管健康检查不直接通过Ingress公开，但Kubernetes中存在并行概念，例如&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34;&gt;准备探查&lt;/a&gt;，可以使你达成相同的最终结果。 请查看特定控制器的文档，以了解他们如何处理健康检查（&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md&#34;&gt;nginx&lt;/a&gt;，&lt;a href=&#34;https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#health-checks&#34;&gt;GCE&lt;/a&gt;）。&lt;/p&gt;

&lt;h2 id=&#34;更新ingress&#34;&gt;更新Ingress&lt;/h2&gt;

&lt;p&gt;假如你想要向已有的ingress中增加一个新的Host，你可以编辑和更新该ingress：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
$ kubectl edit ing test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这会弹出一个包含已有的yaml文件的编辑器，修改它，增加新的Host配置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: s1
          servicePort: 80
        path: /foo
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: s2
          servicePort: 80
        path: /foo
..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存它会更新API server中的资源，这会触发ingress controller重新配置loadbalancer。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
test      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
          bar.baz.com
          /foo          s2:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在一个修改过的ingress yaml文件上调用&lt;code&gt;kubectl replace -f&lt;/code&gt;命令一样可以达到同样的效果。&lt;/p&gt;

&lt;h2 id=&#34;跨可用域故障&#34;&gt;跨可用域故障&lt;/h2&gt;

&lt;p&gt;在不通云供应商之间，跨故障域的流量传播技术有所不同。 有关详细信息，请查看相关Ingress controller的文档。 有关在federation集群中部署Ingress的详细信息，请参阅&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34;&gt;federation文档&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;未来计划&#34;&gt;未来计划&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;多样化的HTTPS/TLS模型支持（如SNI，re-encryption）&lt;/li&gt;
&lt;li&gt;通过声明来请求IP或者主机名&lt;/li&gt;
&lt;li&gt;结合L4和L7 Ingress&lt;/li&gt;
&lt;li&gt;更多的Ingress controller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;请跟踪&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/12827&#34;&gt;L7和Ingress的proposal&lt;/a&gt;，了解有关资源演进的更多细节，以及&lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master&#34;&gt;Ingress repository&lt;/a&gt;，了解有关各种Ingress controller演进的更多详细信息。&lt;/p&gt;

&lt;h2 id=&#34;替代方案&#34;&gt;替代方案&lt;/h2&gt;

&lt;p&gt;你可以通过很多种方式暴露service而不必直接使用ingress：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用&lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/#type-loadbalancer&#34;&gt;Service.Type=LoadBalancer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用&lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/#type-nodeport&#34;&gt;Service.Type=NodePort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/for-demos/proxy-to-service&#34;&gt;Port Proxy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;部署一个&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/service-loadbalancer&#34;&gt;Service loadbalancer&lt;/a&gt; 这允许你在多个service之间共享单个IP，并通过Service Annotations实现更高级的负载平衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Kubernetes Ingress Resource&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/957&#34;&gt;使用NGINX Plus负载均衡Kubernetes服务&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/276815076/p/6407101.html&#34;&gt;使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html&#34;&gt;Kubernetes : Ingress Controller with Træfɪk and Let&amp;rsquo;s Encrypt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.osones.com/en/kubernetes-traefik-and-lets-encrypt-at-scale.html&#34;&gt;Kubernetes : Træfɪk and Let&amp;rsquo;s Encrypt at scale&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.traefik.io/user-guide/kubernetes/&#34;&gt;Kubernetes Ingress Controller-Træfɪk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.kubernetes.io/2016/03/Kubernetes-1.2-and-simplifying-advanced-networking-with-Ingress.html&#34;&gt;Kubernetes 1.2 and simplifying advanced networking with Ingress&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装EFK</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</link>
      <pubDate>Thu, 13 Apr 2017 12:28:10 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016061706.jpg&#34; alt=&#34;簋街&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：簋街 Jun 17,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目(fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;)中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，在开启了TLS验证的集群中部署EFK日志收集监控插件。&lt;/p&gt;

&lt;h1 id=&#34;配置和安装-efk&#34;&gt;配置和安装 EFK&lt;/h1&gt;

&lt;p&gt;官方文件目录：&lt;code&gt;cluster/addons/fluentd-elasticsearch&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ls *.yaml
es-controller.yaml  es-service.yaml  fluentd-es-ds.yaml  kibana-controller.yaml  kibana-service.yaml efk-rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同样EFK服务也需要一个&lt;code&gt;efk-rbac.yaml&lt;/code&gt;文件，配置serviceaccount为&lt;code&gt;efk&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;已经修改好的 yaml 文件见：&lt;a href=&#34;./manifests/EFK&#34;&gt;EFK&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;配置-es-controller-yaml&#34;&gt;配置 es-controller.yaml&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff es-controller.yaml.orig es-controller.yaml
24c24
&amp;lt;       - image: gcr.io/google_containers/elasticsearch:v2.4.1-2
---
&amp;gt;       - image: sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置-es-service-yaml&#34;&gt;配置 es-service.yaml&lt;/h2&gt;

&lt;p&gt;无需配置；&lt;/p&gt;

&lt;h2 id=&#34;配置-fluentd-es-ds-yaml&#34;&gt;配置 fluentd-es-ds.yaml&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff fluentd-es-ds.yaml.orig fluentd-es-ds.yaml
26c26
&amp;lt;         image: gcr.io/google_containers/fluentd-elasticsearch:1.22
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置-kibana-controller-yaml&#34;&gt;配置 kibana-controller.yaml&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff kibana-controller.yaml.orig kibana-controller.yaml
22c22
&amp;lt;         image: gcr.io/google_containers/kibana:v4.6.1-1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/kibana:v4.6.1-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;给-node-设置标签&#34;&gt;给 Node 设置标签&lt;/h2&gt;

&lt;p&gt;定义 DaemonSet &lt;code&gt;fluentd-es-v1.22&lt;/code&gt; 时设置了 nodeSelector &lt;code&gt;beta.kubernetes.io/fluentd-ds-ready=true&lt;/code&gt; ，所以需要在期望运行 fluentd 的 Node 上设置该标签；&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get nodes
NAME        STATUS    AGE       VERSION
172.20.0.113   Ready     1d        v1.6.0

$ kubectl label nodes 172.20.0.113 beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;172.20.0.113&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给其他两台node打上同样的标签。&lt;/p&gt;

&lt;h2 id=&#34;执行定义文件&#34;&gt;执行定义文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f .
serviceaccount &amp;quot;efk&amp;quot; created
clusterrolebinding &amp;quot;efk&amp;quot; created
replicationcontroller &amp;quot;elasticsearch-logging-v1&amp;quot; created
service &amp;quot;elasticsearch-logging&amp;quot; created
daemonset &amp;quot;fluentd-es-v1.22&amp;quot; created
deployment &amp;quot;kibana-logging&amp;quot; created
service &amp;quot;kibana-logging&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;检查执行结果&#34;&gt;检查执行结果&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get deployment -n kube-system|grep kibana
kibana-logging         1         1         1            1           2m

$ kubectl get pods -n kube-system|grep -E &#39;elasticsearch|fluentd|kibana&#39;
elasticsearch-logging-v1-mlstp          1/1       Running   0          1m
elasticsearch-logging-v1-nfbbf          1/1       Running   0          1m
fluentd-es-v1.22-31sm0                  1/1       Running   0          1m
fluentd-es-v1.22-bpgqs                  1/1       Running   0          1m
fluentd-es-v1.22-qmn7h                  1/1       Running   0          1m
kibana-logging-1432287342-0gdng         1/1       Running   0          1m

$ kubectl get service  -n kube-system|grep -E &#39;elasticsearch|kibana&#39;
elasticsearch-logging   10.254.77.62    &amp;lt;none&amp;gt;        9200/TCP                        2m
kibana-logging          10.254.8.113    &amp;lt;none&amp;gt;        5601/TCP                        2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kibana Pod 第一次启动时会用&lt;strong&gt;较长时间(10-20分钟)&lt;/strong&gt;来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl logs kibana-logging-1432287342-0gdng -n kube-system -f
ELASTICSEARCH_URL=http://elasticsearch-logging:9200
server.basePath: /api/v1/proxy/namespaces/kube-system/services/kibana-logging
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:08:06Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;message&amp;quot;:&amp;quot;Optimizing and caching bundles for kibana and statusPage. This may take a few minutes&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:17Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;info&amp;quot;,&amp;quot;optimize&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;message&amp;quot;:&amp;quot;Optimization of bundles for kibana and statusPage complete in 610.40 seconds&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:17Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kibana@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:18Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to yellow - Waiting for Elasticsearch&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:kbn_vislib_vis_types@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:markdown_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:metric_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:spyModes@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:statusPage@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:table_vis@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from uninitialized to green - Ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;uninitialized&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;uninitialized&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:19Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;listening&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;message&amp;quot;:&amp;quot;Server running at http://0.0.0.0:5601&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:24Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to yellow - No existing Kibana index found&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;Waiting for Elasticsearch&amp;quot;}
{&amp;quot;type&amp;quot;:&amp;quot;log&amp;quot;,&amp;quot;@timestamp&amp;quot;:&amp;quot;2017-04-12T13:18:29Z&amp;quot;,&amp;quot;tags&amp;quot;:[&amp;quot;status&amp;quot;,&amp;quot;plugin:elasticsearch@1.0.0&amp;quot;,&amp;quot;info&amp;quot;],&amp;quot;pid&amp;quot;:7,&amp;quot;state&amp;quot;:&amp;quot;green&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Status changed from yellow to green - Kibana index ready&amp;quot;,&amp;quot;prevState&amp;quot;:&amp;quot;yellow&amp;quot;,&amp;quot;prevMsg&amp;quot;:&amp;quot;No existing Kibana index found&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;访问-kibana&#34;&gt;访问 kibana&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;通过 kube-apiserver 访问：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;获取 monitoring-grafana 服务 URL&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   $ kubectl cluster-info
   Kubernetes master is running at https://172.20.0.113:6443
   Elasticsearch is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
   Heapster is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/heapster
   Kibana is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging
   KubeDNS is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
   kubernetes-dashboard is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
   monitoring-grafana is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
   monitoring-influxdb is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 URL： &lt;code&gt;https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通过 kubectl proxy 访问：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;创建代理&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   $ kubectl proxy --address=&#39;172.20.0.113&#39; --port=8086 --accept-hosts=&#39;^*$&#39;
   Starting to serve on 172.20.0.113:8086
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 URL：&lt;code&gt;http://172.20.0.113:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在 Settings -&amp;gt; Indices 页面创建一个 index（相当于 mysql 中的一个 database），选中 &lt;code&gt;Index contains time-based events&lt;/code&gt;，使用默认的 &lt;code&gt;logstash-*&lt;/code&gt; pattern，点击 &lt;code&gt;Create&lt;/code&gt; ;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;可能遇到的问题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果你在这里发现Create按钮是灰色的无法点击，且Time-filed name中没有选项，fluentd要读取&lt;code&gt;/var/log/containers/&lt;/code&gt;目录下的log日志，这些日志是从&lt;code&gt;/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log&lt;/code&gt;链接过来的，查看你的docker配置，&lt;code&gt;—log-dirver&lt;/code&gt;需要设置为&lt;strong&gt;json-file&lt;/strong&gt;格式，默认的可能是&lt;strong&gt;journald&lt;/strong&gt;，参考&lt;a href=&#34;[https://docs.docker.com/engine/admin/logging/overview/#examples](https://docs.docker.com/engine/admin/logging/overview/#examples)&#34;&gt;docker logging&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-es-setting.png&#34; alt=&#34;es-setting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;创建Index后，可以在 &lt;code&gt;Discover&lt;/code&gt; 下看到 ElasticSearch logging 中汇聚的日志；&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-efk-kibana.jpg&#34; alt=&#34;es-home&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此Kubernetes的所有环境都安装完成。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装heapster</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 20:20:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016080801.jpg&#34; alt=&#34;大喵&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：大喵 Aug 8,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目(fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;)中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，在开启了TLS验证的集群中部署heapster，包括influxdb、grafana等组件。&lt;/p&gt;

&lt;h2 id=&#34;配置和安装heapster&#34;&gt;配置和安装Heapster&lt;/h2&gt;

&lt;p&gt;到 &lt;a href=&#34;https://github.com/kubernetes/heapster/releases&#34;&gt;heapster release 页面&lt;/a&gt; 下载最新版本的 heapster。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip
$ unzip v1.3.0.zip
$ mv v1.3.0.zip heapster-1.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件目录： &lt;code&gt;heapster-1.3.0/deploy/kube-config/influxdb&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd heapster-1.3.0/deploy/kube-config/influxdb
$ ls *.yaml
grafana-deployment.yaml  grafana-service.yaml  heapster-deployment.yaml  heapster-service.yaml  influxdb-deployment.yaml  influxdb-service.yaml heapster-rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们自己创建了heapster的rbac配置&lt;code&gt;heapster-rbac.yaml&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;已经修改好的 yaml 文件见：&lt;a href=&#34;./manifests/heapster&#34;&gt;heapster&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;配置-grafana-deployment&#34;&gt;配置 grafana-deployment&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff grafana-deployment.yaml.orig grafana-deployment.yaml
16c16
&amp;lt;         image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2
40,41c40,41
&amp;lt;           # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;lt;           value: /
---
&amp;gt;           value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
&amp;gt;           #value: /
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 &lt;code&gt;GF_SERVER_ROOT_URL&lt;/code&gt; 设置为 &lt;code&gt;/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/&lt;/code&gt;，否则后续访问grafana时访问时提示找不到&lt;code&gt;http://10.64.3.7:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home&lt;/code&gt; 页面；&lt;/p&gt;

&lt;h2 id=&#34;配置-heapster-deployment&#34;&gt;配置 heapster-deployment&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff heapster-deployment.yaml.orig heapster-deployment.yaml
16c16
&amp;lt;         image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置-influxdb-deployment&#34;&gt;配置 influxdb-deployment&lt;/h2&gt;

&lt;p&gt;influxdb 官方建议使用命令行或 HTTP API 接口来查询数据库，从 v1.1.0 版本开始默认关闭 admin UI，将在后续版本中移除 admin UI 插件。&lt;/p&gt;

&lt;p&gt;开启镜像中 admin UI的办法如下：先导出镜像中的 influxdb 配置文件，开启 admin 插件后，再将配置文件内容写入 ConfigMap，最后挂载到镜像中，达到覆盖原始配置的目的：&lt;/p&gt;

&lt;p&gt;注意：manifests 目录已经提供了 &lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/heapster/influxdb-cm.yaml&#34;&gt;修改后的 ConfigMap 定义文件&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ # 导出镜像中的 influxdb 配置文件
$ docker run --rm --entrypoint &#39;cat&#39;  -ti lvanneo/heapster-influxdb-amd64:v1.1.1 /etc/config.toml &amp;gt;config.toml.orig
$ cp config.toml.orig config.toml
$ # 修改：启用 admin 接口
$ vim config.toml
$ diff config.toml.orig config.toml
35c35
&amp;lt;   enabled = false
---
&amp;gt;   enabled = true
$ # 将修改后的配置写入到 ConfigMap 对象中
$ kubectl create configmap influxdb-config --from-file=config.toml  -n kube-system
configmap &amp;quot;influxdb-config&amp;quot; created
$ # 将 ConfigMap 中的配置文件挂载到 Pod 中，达到覆盖原始配置的目的
$ diff influxdb-deployment.yaml.orig influxdb-deployment.yaml
16c16
&amp;lt;         image: grc.io/google_containers/heapster-influxdb-amd64:v1.1.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1
19a20,21
&amp;gt;         - mountPath: /etc/
&amp;gt;           name: influxdb-config
22a25,27
&amp;gt;       - name: influxdb-config
&amp;gt;         configMap:
&amp;gt;           name: influxdb-config
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置-monitoring-influxdb-service&#34;&gt;配置 monitoring-influxdb Service&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ diff influxdb-service.yaml.orig influxdb-service.yaml
12a13
&amp;gt;   type: NodePort
15a17,20
&amp;gt;     name: http
&amp;gt;   - port: 8083
&amp;gt;     targetPort: 8083
&amp;gt;     name: admin
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;定义端口类型为 NodePort，额外增加了 admin 端口映射，用于后续浏览器访问 influxdb 的 admin UI 界面；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;执行所有定义文件&#34;&gt;执行所有定义文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pwd
/root/heapster-1.3.0/deploy/kube-config/influxdb
$ ls *.yaml
grafana-service.yaml      heapster-rbac.yaml     influxdb-cm.yaml          influxdb-service.yaml
grafana-deployment.yaml  heapster-deployment.yaml  heapster-service.yaml  influxdb-deployment.yaml
$ kubectl create -f  .
deployment &amp;quot;monitoring-grafana&amp;quot; created
service &amp;quot;monitoring-grafana&amp;quot; created
deployment &amp;quot;heapster&amp;quot; created
serviceaccount &amp;quot;heapster&amp;quot; created
clusterrolebinding &amp;quot;heapster&amp;quot; created
service &amp;quot;heapster&amp;quot; created
configmap &amp;quot;influxdb-config&amp;quot; created
deployment &amp;quot;monitoring-influxdb&amp;quot; created
service &amp;quot;monitoring-influxdb&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;检查执行结果&#34;&gt;检查执行结果&lt;/h2&gt;

&lt;p&gt;检查 Deployment&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get deployments -n kube-system | grep -E &#39;heapster|monitoring&#39;
heapster               1         1         1            1           2m
monitoring-grafana     1         1         1            1           2m
monitoring-influxdb    1         1         1            1           2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查 Pods&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -n kube-system | grep -E &#39;heapster|monitoring&#39;
heapster-110704576-gpg8v                1/1       Running   0          2m
monitoring-grafana-2861879979-9z89f     1/1       Running   0          2m
monitoring-influxdb-1411048194-lzrpc    1/1       Running   0          2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图；&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-dashboard-with-heapster.jpg&#34; alt=&#34;dashboard-heapster&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;访问-grafana&#34;&gt;访问 grafana&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;通过 kube-apiserver 访问：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;获取 monitoring-grafana 服务 URL&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   $ kubectl cluster-info
   Kubernetes master is running at https://172.20.0.113:6443
   Heapster is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/heapster
   KubeDNS is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
   kubernetes-dashboard is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
   monitoring-grafana is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
   monitoring-influxdb is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

   To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 URL： &lt;code&gt;http://172.20.0.113:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;通过 kubectl proxy 访问：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;创建代理&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   $ kubectl proxy --address=&#39;172.20.0.113&#39; --port=8086 --accept-hosts=&#39;^*$&#39;
   Starting to serve on 172.20.0.113:8086
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 URL：&lt;code&gt;http://172.20.0.113:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-heapster-grafana.jpg&#34; alt=&#34;grafana&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;访问-influxdb-admin-ui&#34;&gt;访问 influxdb admin UI&lt;/h2&gt;

&lt;p&gt;获取 influxdb http 8086 映射的 NodePort&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc -n kube-system|grep influxdb
monitoring-influxdb    10.254.22.46    &amp;lt;nodes&amp;gt;       8086:32299/TCP,8083:30269/TCP   9m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 kube-apiserver 的&lt;strong&gt;非安全端口&lt;/strong&gt;访问 influxdb 的 admin UI 界面： &lt;code&gt;http://172.20.0.113:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;在页面的 “Connection Settings” 的 Host 中输入 node IP， Port 中输入 8086 映射的 nodePort 如上面的 32299，点击 “Save” 即可（我的集群中的地址是172.20.0.113:32299）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-influxdb-heapster.jpg&#34; alt=&#34;kubernetes-influxdb-heapster&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到此Heapster已经部署完成。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装dashboard</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 15:53:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016082001.jpg&#34; alt=&#34;东直门&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：东直门桥 Aug 20,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目(fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;)中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，在开启了TLS验证的集群中部署dashboard。&lt;/p&gt;

&lt;p&gt;感谢&lt;a href=&#34;github.com/opsnull&#34;&gt;opsnull&lt;/a&gt;和&lt;a href=&#34;github.com/ipchy&#34;&gt;ipchy&lt;/a&gt;的细心解答。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装环境配置信息&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CentOS 7.2.1511&lt;/li&gt;
&lt;li&gt;Docker 1.12.5&lt;/li&gt;
&lt;li&gt;Flannel 0.7&lt;/li&gt;
&lt;li&gt;Kubernetes 1.6.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;配置和安装-dashboard&#34;&gt;配置和安装 dashboard&lt;/h2&gt;

&lt;p&gt;官方文件目录：&lt;code&gt;kubernetes/cluster/addons/dashboard&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;我们使用的文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ls *.yaml
dashboard-controller.yaml  dashboard-service.yaml dashboard-rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;已经修改好的 yaml 文件见：&lt;a href=&#34;./manifests/dashboard&#34;&gt;dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;由于 &lt;code&gt;kube-apiserver&lt;/code&gt; 启用了 &lt;code&gt;RBAC&lt;/code&gt; 授权，而官方源码目录的 &lt;code&gt;dashboard-controller.yaml&lt;/code&gt; 没有定义授权的 ServiceAccount，所以后续访问 &lt;code&gt;kube-apiserver&lt;/code&gt; 的 API 时会被拒绝，web中提示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Forbidden (403)

User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增加了一个&lt;code&gt;dashboard-rbac.yaml&lt;/code&gt;文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。&lt;/p&gt;

&lt;h2 id=&#34;配置dashboard-service&#34;&gt;配置dashboard-service&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff dashboard-service.yaml.orig dashboard-service.yaml
10a11
&amp;gt;   type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;配置dashboard-controller&#34;&gt;配置dashboard-controller&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff dashboard-controller.yaml.orig dashboard-controller.yaml
23c23
&amp;lt;         image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/kubernetes-dashboard-amd64:v1.6.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;执行所有定义文件&#34;&gt;执行所有定义文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pwd
/root/kubernetes/cluster/addons/dashboard
$ ls *.yaml
dashboard-controller.yaml  dashboard-service.yaml
$ kubectl create -f  .
service &amp;quot;kubernetes-dashboard&amp;quot; created
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;检查执行结果&#34;&gt;检查执行结果&lt;/h2&gt;

&lt;p&gt;查看分配的 NodePort&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get services kubernetes-dashboard -n kube-system
NAME                   CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.254.224.130   &amp;lt;nodes&amp;gt;       80:30312/TCP   25s
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;NodePort 30312映射到 dashboard pod 80端口；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;检查 controller&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           3m
$ kubectl get pods  -n kube-system | grep dashboard
kubernetes-dashboard-1339745653-pmn6z   1/1       Running   0          4m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;访问dashboard&#34;&gt;访问dashboard&lt;/h2&gt;

&lt;p&gt;有以下三种方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubernetes-dashboard 服务暴露了 NodePort，可以使用 &lt;code&gt;http://NodeIP:nodePort&lt;/code&gt; 地址访问 dashboard；&lt;/li&gt;
&lt;li&gt;通过 kube-apiserver 访问 dashboard（https 6443端口和http 8080端口方式）；&lt;/li&gt;
&lt;li&gt;通过 kubectl proxy 访问 dashboard：&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;通过-kubectl-proxy-访问-dashboard&#34;&gt;通过 kubectl proxy 访问 dashboard&lt;/h3&gt;

&lt;p&gt;启动代理&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl proxy --address=&#39;172.20.0.113&#39; --port=8086 --accept-hosts=&#39;^*$&#39;
Starting to serve on 172.20.0.113:8086
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;需要指定 &lt;code&gt;--accept-hosts&lt;/code&gt; 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;浏览器访问 URL：&lt;code&gt;http://172.20.0.113:8086/ui&lt;/code&gt;
自动跳转到：&lt;code&gt;http://172.20.0.113:8086/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/workload?namespace=default&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;通过-kube-apiserver-访问dashboard&#34;&gt;通过 kube-apiserver 访问dashboard&lt;/h3&gt;

&lt;p&gt;获取集群服务地址列表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl cluster-info
Kubernetes master is running at https://172.20.0.113:6443
KubeDNS is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问 URL：&lt;code&gt;https://172.20.0.113:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&lt;/code&gt;（浏览器会提示证书验证，因为通过加密通道，以改方式访问的话，需要提前导入证书到你的计算机中）。这是我当时在这遇到的坑：&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/5&#34;&gt;通过 kube-apiserver 访问dashboard，提示User &amp;ldquo;system:anonymous&amp;rdquo; cannot proxy services in the namespace &amp;ldquo;kube-system&amp;rdquo;. #5&lt;/a&gt;，已经解决。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;导入证书&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将生成的admin.pem证书转换格式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl pkcs12 -export -in admin.pem  -out admin.p12 -inkey admin-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将生成的&lt;code&gt;admin.p12&lt;/code&gt;证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。&lt;/p&gt;

&lt;p&gt;如果你不想使用&lt;strong&gt;https&lt;/strong&gt;的话，可以直接访问insecure port 8080端口:&lt;code&gt;http://172.20.0.113:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-dashboard-raw.jpg&#34; alt=&#34;kubernetes-dashboard&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等 metric 图形。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes安装之kubedns配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</link>
      <pubDate>Wed, 12 Apr 2017 13:04:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016082701.jpg&#34; alt=&#34;东三环&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：雨过天晴@北京定福庄 Aug 27,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目(fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;)中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，使用yaml文件部署&lt;strong&gt;kubedns&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装环境配置信息&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CentOS 7.2.1511&lt;/li&gt;
&lt;li&gt;Docker 1.12.5&lt;/li&gt;
&lt;li&gt;Flannel 0.7&lt;/li&gt;
&lt;li&gt;Kubernetes 1.6.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装和配置-kubedns-插件&#34;&gt;安装和配置 kubedns 插件&lt;/h2&gt;

&lt;p&gt;官方的yaml文件目录：&lt;code&gt;kubernetes/cluster/addons/dns&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;该插件直接使用kubernetes部署，官方的配置文件中包含以下镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我clone了上述镜像，上传到我的私有镜像仓库：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:1.14.1
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:1.14.1
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时上传了一份到时速云备份：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下yaml配置文件中使用的是私有镜像仓库中的镜像。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubedns-cm.yaml  
kubedns-sa.yaml  
kubedns-controller.yaml  
kubedns-svc.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;已经修改好的 yaml 文件见：&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;github项目中的manifest/kubedns/目录&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;系统预定义的-rolebinding&#34;&gt;系统预定义的 RoleBinding&lt;/h2&gt;

&lt;p&gt;预定义的 RoleBinding &lt;code&gt;system:kube-dns&lt;/code&gt; 将 kube-system 命名空间的 &lt;code&gt;kube-dns&lt;/code&gt; ServiceAccount 与 &lt;code&gt;system:kube-dns&lt;/code&gt; Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;$ kubectl get clusterrolebindings system:kube-dns -o yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &amp;quot;true&amp;quot;
  creationTimestamp: 2017-04-11T11:20:42Z
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-dns
  resourceVersion: &amp;quot;58&amp;quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindingssystem%3Akube-dns
  uid: e61f4d92-1ea8-11e7-8cd7-f4e9d49f8ed0
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-dns
subjects:
- kind: ServiceAccount
  name: kube-dns
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;kubedns-controller.yaml&lt;/code&gt; 中定义的 Pods 时使用了 &lt;code&gt;kubedns-sa.yaml&lt;/code&gt; 文件定义的 &lt;code&gt;kube-dns&lt;/code&gt; ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限。&lt;/p&gt;

&lt;h2 id=&#34;配置-kube-dns-serviceaccount&#34;&gt;配置 kube-dns ServiceAccount&lt;/h2&gt;

&lt;p&gt;无需修改。&lt;/p&gt;

&lt;h2 id=&#34;配置-kube-dns-服务&#34;&gt;配置 &lt;code&gt;kube-dns&lt;/code&gt; 服务&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff kubedns-svc.yaml.base kubedns-svc.yaml
30c30
&amp;lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&amp;gt;   clusterIP: 10.254.0.2
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;spec.clusterIP = 10.254.0.2，即明确指定了 kube-dns Service IP，这个 IP 需要和 kubelet 的 &lt;code&gt;--cluster-dns&lt;/code&gt; 参数值一致；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;配置-kube-dns-deployment&#34;&gt;配置 &lt;code&gt;kube-dns&lt;/code&gt; Deployment&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ diff kubedns-controller.yaml.base kubedns-controller.yaml
58c58
&amp;lt;         image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:v1.14.1
88c88
&amp;lt;         - --domain=__PILLAR__DNS__DOMAIN__.
---
&amp;gt;         - --domain=cluster.local.
92c92
&amp;lt;         __PILLAR__FEDERATIONS__DOMAIN__MAP__
---
&amp;gt;         #__PILLAR__FEDERATIONS__DOMAIN__MAP__
110c110
&amp;lt;         image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:v1.14.1
129c129
&amp;lt;         - --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053
---
&amp;gt;         - --server=/cluster.local./127.0.0.1#10053
148c148
&amp;lt;         image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
---
&amp;gt;         image: sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:v1.14.1
161,162c161,162
&amp;lt;         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,A
&amp;lt;         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,A
---
&amp;gt;         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A
&amp;gt;         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;使用系统已经做了 RoleBinding 的 &lt;code&gt;kube-dns&lt;/code&gt; ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;执行所有定义文件&#34;&gt;执行所有定义文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pwd
/root/kubedns
$ ls *.yaml
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml
$ kubectl create -f .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;检查-kubedns-功能&#34;&gt;检查 kubedns 功能&lt;/h2&gt;

&lt;p&gt;新建一个 Deployment&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat  my-nginx.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: sz-pg-oam-docker-hub-001.tendcloud.com/library/nginx:1.9
        ports:
        - containerPort: 80
$ kubectl create -f my-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Export 该 Deployment, 生成 &lt;code&gt;my-nginx&lt;/code&gt; 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl expose deploy my-nginx
$ kubectl get services --all-namespaces |grep my-nginx
default       my-nginx     10.254.179.239   &amp;lt;none&amp;gt;        80/TCP          42m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建另一个 Pod，查看 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 是否包含 &lt;code&gt;kubelet&lt;/code&gt; 配置的 &lt;code&gt;--cluster-dns&lt;/code&gt; 和 &lt;code&gt;--cluster-domain&lt;/code&gt;，是否能够将服务 &lt;code&gt;my-nginx&lt;/code&gt; 解析到 Cluster IP &lt;code&gt;10.254.179.239&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f nginx-pod.yaml
$ kubectl exec  nginx -i -t -- /bin/bash
root@nginx:/# cat /etc/resolv.conf
nameserver 10.254.0.2
search default.svc.cluster.local. svc.cluster.local. cluster.local. tendcloud.com
options ndots:5

root@nginx:/# ping my-nginx
PING my-nginx.default.svc.cluster.local (10.254.179.239): 56 data bytes
76 bytes from 119.147.223.109: Destination Net Unreachable
^C--- my-nginx.default.svc.cluster.local ping statistics ---

root@nginx:/# ping kubernetes
PING kubernetes.default.svc.cluster.local (10.254.0.1): 56 data bytes
^C--- kubernetes.default.svc.cluster.local ping statistics ---
11 packets transmitted, 0 packets received, 100% packet loss

root@nginx:/# ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 56 data bytes
^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---
6 packets transmitted, 0 packets received, 100% packet loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从结果来看，service名称可以正常解析。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes node节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</link>
      <pubDate>Tue, 11 Apr 2017 22:20:31 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016121101.jpg&#34; alt=&#34;东三环&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：太阳宫桥@北京东北三环 Dec 11,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目(fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;)中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，部署node节点上的&lt;code&gt;kube-proxy&lt;/code&gt;和&lt;code&gt;kubelet&lt;/code&gt;，同时对之前部署的flannel改造。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;安装环境配置信息&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CentOS7.2.1511&lt;/li&gt;
&lt;li&gt;Docker 1.12.5&lt;/li&gt;
&lt;li&gt;Flannel 0.7&lt;/li&gt;
&lt;li&gt;Kubernetes 1.6.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;部署kubernetes-node节点&#34;&gt;部署kubernetes node节点&lt;/h2&gt;

&lt;p&gt;kubernetes node 节点包含如下组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Flanneld：参考我之前写的文章&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;Kubernetes基于Flannel的网络配置&lt;/a&gt;，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。&lt;/li&gt;
&lt;li&gt;Docker1.12.5：docker的安装很简单，这里也不说了。&lt;/li&gt;
&lt;li&gt;kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面着重讲&lt;code&gt;kubelet&lt;/code&gt;和&lt;code&gt;kube-proxy&lt;/code&gt;的安装，同时还要将之前安装的flannel集成TLS验证。&lt;/p&gt;

&lt;h2 id=&#34;目录和文件&#34;&gt;目录和文件&lt;/h2&gt;

&lt;p&gt;我们再检查一下三个节点上，经过前几步操作生成的配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
$ ls /etc/kubernetes/
apiserver  bootstrap.kubeconfig  config  controller-manager  kubelet  kube-proxy.kubeconfig  proxy  scheduler  ssl  token.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置flanneld&#34;&gt;配置Flanneld&lt;/h2&gt;

&lt;p&gt;参考我之前写的文章&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;Kubernetes基于Flannel的网络配置&lt;/a&gt;，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。&lt;/p&gt;

&lt;p&gt;service配置文件&lt;code&gt;/usr/lib/systemd/system/flanneld.service&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/etc/sysconfig/flanneld&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;https://172.20.0.113:2379,https://172.20.0.114:2379,https://172.20.0.115:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
FLANNEL_OPTIONS=&amp;quot;-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在FLANNEL_OPTIONS中增加TLS的配置。&lt;/p&gt;

&lt;h2 id=&#34;安装和配置-kubelet&#34;&gt;安装和配置 kubelet&lt;/h2&gt;

&lt;p&gt;kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)，
然后 kubelet 才能有权限创建认证请求(certificate signing requests)：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd /etc/kubernetes
$ kubectl create clusterrolebinding kubelet-bootstrap \
  --clusterrole=system:node-bootstrapper \
  --user=kubelet-bootstrap
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--user=kubelet-bootstrap&lt;/code&gt; 是在 &lt;code&gt;/etc/kubernetes/token.csv&lt;/code&gt; 文件中指定的用户名，同时也写入了 &lt;code&gt;/etc/kubernetes/bootstrap.kubeconfig&lt;/code&gt; 文件；&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;下载最新的-kubelet-和-kube-proxy-二进制文件&#34;&gt;下载最新的 kubelet 和 kube-proxy 二进制文件&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
$ tar -xzvf kubernetes-server-linux-amd64.tar.gz
$ cd kubernetes
$ tar -xzvf  kubernetes-src.tar.gz
$ cp -r ./server/bin/{kube-proxy,kubelet} /usr/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;创建-kubelet-的service配置文件&#34;&gt;创建 kubelet 的service配置文件&lt;/h3&gt;

&lt;p&gt;文件位置&lt;code&gt;/usr/lib/systemd/system/kubelet.serivce&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBELET_API_SERVER \
            $KUBELET_ADDRESS \
            $KUBELET_PORT \
            $KUBELET_HOSTNAME \
            $KUBE_ALLOW_PRIV \
            $KUBELET_POD_INFRA_CONTAINER \
            $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubelet的配置文件&lt;code&gt;/etc/kubernetes/kubelet&lt;/code&gt;。其中的IP地址更改为你的每台node节点的IP地址。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=172.20.0.113&amp;quot;
#
## The port for the info server to serve on
#KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=172.20.0.113&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://172.20.0.113:8080&amp;quot;
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;--cgroup-driver=systemd --cluster_dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster_domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--address&lt;/code&gt; 不能设置为 &lt;code&gt;127.0.0.1&lt;/code&gt;，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 &lt;code&gt;127.0.0.1&lt;/code&gt; 指向自己而不是 kubelet；&lt;/li&gt;
&lt;li&gt;如果设置了 &lt;code&gt;--hostname-override&lt;/code&gt; 选项，则 &lt;code&gt;kube-proxy&lt;/code&gt; 也需要设置该选项，否则会出现找不到 Node 的情况；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--experimental-bootstrap-kubeconfig&lt;/code&gt; 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；&lt;/li&gt;
&lt;li&gt;管理员通过了 CSR 请求后，kubelet 自动在 &lt;code&gt;--cert-dir&lt;/code&gt; 目录创建证书和私钥文件(&lt;code&gt;kubelet-client.crt&lt;/code&gt; 和 &lt;code&gt;kubelet-client.key&lt;/code&gt;)，然后写入 &lt;code&gt;--kubeconfig&lt;/code&gt; 文件；&lt;/li&gt;
&lt;li&gt;建议在 &lt;code&gt;--kubeconfig&lt;/code&gt; 配置文件中指定 &lt;code&gt;kube-apiserver&lt;/code&gt; 地址，如果未指定 &lt;code&gt;--api-servers&lt;/code&gt; 选项，则必须指定 &lt;code&gt;--require-kubeconfig&lt;/code&gt; 选项后才从配置文件中读取 kube-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），&lt;code&gt;kubectl get nodes&lt;/code&gt; 不会返回对应的 Node 信息;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cluster_dns&lt;/code&gt; 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，&lt;code&gt;--cluster_domain&lt;/code&gt; 指定域名后缀，这两个参数同时指定后才会生效；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整 unit 见 &lt;a href=&#34;./systemd/kubelet.service&#34;&gt;kubelet.service&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动kublet&#34;&gt;启动kublet&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kubelet
$ systemctl start kubelet
$ systemctl status kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;通过-kublet-的-tls-证书请求&#34;&gt;通过 kublet 的 TLS 证书请求&lt;/h3&gt;

&lt;p&gt;kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。&lt;/p&gt;

&lt;p&gt;查看未授权的 CSR 请求&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get csr
NAME        AGE       REQUESTOR           CONDITION
csr-2b308   4m        kubelet-bootstrap   Pending
$ kubectl get nodes
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 CSR 请求&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl certificate approve csr-2b308
certificatesigningrequest &amp;quot;csr-2b308&amp;quot; approved
$ kubectl get nodes
NAME        STATUS    AGE       VERSION
10.64.3.7   Ready     49m       v1.6.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;自动生成了 kubelet kubeconfig 文件和公私钥&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2284 Apr  7 02:07 /etc/kubernetes/kubelet.kubeconfig
$ ls -l /etc/kubernetes/ssl/kubelet*
-rw-r--r-- 1 root root 1046 Apr  7 02:07 /etc/kubernetes/ssl/kubelet-client.crt
-rw------- 1 root root  227 Apr  7 02:04 /etc/kubernetes/ssl/kubelet-client.key
-rw-r--r-- 1 root root 1103 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.crt
-rw------- 1 root root 1675 Apr  7 02:07 /etc/kubernetes/ssl/kubelet.key
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置-kube-proxy&#34;&gt;配置 kube-proxy&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;创建 kube-proxy 的service配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文件路径&lt;code&gt;/usr/lib/systemd/system/kube-proxy.service&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kube-proxy配置文件&lt;code&gt;/etc/kubernetes/proxy&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;--bind-address=172.20.0.113 --hostname-override=172.20.0.113 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--hostname-override&lt;/code&gt; 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；&lt;/li&gt;
&lt;li&gt;kube-proxy 根据 &lt;code&gt;--cluster-cidr&lt;/code&gt; 判断集群内部和外部流量，指定 &lt;code&gt;--cluster-cidr&lt;/code&gt; 或 &lt;code&gt;--masquerade-all&lt;/code&gt; 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--kubeconfig&lt;/code&gt; 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；&lt;/li&gt;
&lt;li&gt;预定义的 RoleBinding &lt;code&gt;cluster-admin&lt;/code&gt; 将User &lt;code&gt;system:kube-proxy&lt;/code&gt; 与 Role &lt;code&gt;system:node-proxier&lt;/code&gt; 绑定，该 Role 授予了调用 &lt;code&gt;kube-apiserver&lt;/code&gt; Proxy 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整 unit 见 &lt;a href=&#34;./systemd/kube-proxy.service&#34;&gt;kube-proxy.service&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-kube-proxy&#34;&gt;启动 kube-proxy&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kube-proxy
$ systemctl start kube-proxy
$ systemctl status kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;验证测试&#34;&gt;验证测试&lt;/h2&gt;

&lt;p&gt;我们创建一个niginx的service试一下集群是否可用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/nginx:1.9  --port=80
deployment &amp;quot;nginx&amp;quot; created
$ kubectl expose deployment nginx --type=NodePort --name=example-service
service &amp;quot;example-service&amp;quot; exposed
$ kubectl describe svc example-service
Name:			example-service
Namespace:		default
Labels:			run=load-balancer-example
Annotations:		&amp;lt;none&amp;gt;
Selector:		run=load-balancer-example
Type:			NodePort
IP:			10.254.62.207
Port:			&amp;lt;unset&amp;gt;	80/TCP
NodePort:		&amp;lt;unset&amp;gt;	32724/TCP
Endpoints:		172.30.60.2:80,172.30.94.2:80
Session Affinity:	None
Events:			&amp;lt;none&amp;gt;
$ curl &amp;quot;10.254.62.207:80&amp;quot;
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;访问&lt;code&gt;172.20.0.113:32724&lt;/code&gt;或&lt;code&gt;172.20.0.114:32724&lt;/code&gt;或者&lt;code&gt;172.20.0.115:32724&lt;/code&gt;都可以得到nginx的页面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-installation-test-nginx.png&#34; alt=&#34;welcome-nginx&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes高可用master节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</link>
      <pubDate>Tue, 11 Apr 2017 19:55:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2015091402.jpg&#34; alt=&#34;北京西山&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：鬼见愁@北京西山 Sep 14,2015）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目((fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;))中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，部署master节点的&lt;code&gt;kube-apiserver&lt;/code&gt;、&lt;code&gt;kube-controller-manager&lt;/code&gt;和&lt;code&gt;kube-scheduler&lt;/code&gt;的过程。&lt;/p&gt;

&lt;h2 id=&#34;高可用kubernetes-master节点安装&#34;&gt;高可用kubernetes master节点安装&lt;/h2&gt;

&lt;p&gt;kubernetes master 节点包含的组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前这三个组件需要部署在同一台机器上。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kube-scheduler&lt;/code&gt;、&lt;code&gt;kube-controller-manager&lt;/code&gt; 和 &lt;code&gt;kube-apiserver&lt;/code&gt; 三者的功能紧密相关；&lt;/li&gt;
&lt;li&gt;同时只能有一个 &lt;code&gt;kube-scheduler&lt;/code&gt;、&lt;code&gt;kube-controller-manager&lt;/code&gt; 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）&lt;/p&gt;

&lt;h2 id=&#34;tls-证书文件&#34;&gt;TLS 证书文件&lt;/h2&gt;

&lt;p&gt;pem和token.csv证书文件我们在&lt;a href=&#34;./01-TLS证书和秘钥.md&#34;&gt;TLS证书和秘钥&lt;/a&gt;这一步中已经创建过了。我们再检查一下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ls /etc/kubernetes/ssl
admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;下载最新版本的二进制文件&#34;&gt;下载最新版本的二进制文件&lt;/h2&gt;

&lt;p&gt;有两种下载方式&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方式一&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases&#34;&gt;github release 页面&lt;/a&gt; 下载发布版 tarball，解压后再执行下载脚本&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
$ tar -xzvf kubernetes.tar.gz
...
$ cd kubernetes
$ ./cluster/get-kube-binaries.sh
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;方式二&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md&#34;&gt;&lt;code&gt;CHANGELOG&lt;/code&gt;页面&lt;/a&gt; 下载 &lt;code&gt;client&lt;/code&gt; 或 &lt;code&gt;server&lt;/code&gt; tarball 文件&lt;/p&gt;

&lt;p&gt;&lt;code&gt;server&lt;/code&gt; 的 tarball &lt;code&gt;kubernetes-server-linux-amd64.tar.gz&lt;/code&gt; 已经包含了 &lt;code&gt;client&lt;/code&gt;(&lt;code&gt;kubectl&lt;/code&gt;) 二进制文件，所以不用单独下载&lt;code&gt;kubernetes-client-linux-amd64.tar.gz&lt;/code&gt;文件；&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # wget https://dl.k8s.io/v1.6.0/kubernetes-client-linux-amd64.tar.gz
$ wget https://dl.k8s.io/v1.6.0/kubernetes-server-linux-amd64.tar.gz
$ tar -xzvf kubernetes-server-linux-amd64.tar.gz
...
$ cd kubernetes
$ tar -xzvf  kubernetes-src.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将二进制文件拷贝到指定路径&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /root/local/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置和启动-kube-apiserver&#34;&gt;配置和启动 kube-apiserver&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;创建 kube-apiserver的service配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;serivce配置文件&lt;code&gt;/usr/lib/systemd/system/kube-apiserver.service&lt;/code&gt;内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_ETCD_SERVERS \
	    $KUBE_API_ADDRESS \
	    $KUBE_API_PORT \
	    $KUBELET_PORT \
	    $KUBE_ALLOW_PRIV \
	    $KUBE_SERVICE_ADDRESSES \
	    $KUBE_ADMISSION_CONTROL \
	    $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/etc/kubernetes/config&lt;/code&gt;文件的内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=true&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
#KUBE_MASTER=&amp;quot;--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
KUBE_MASTER=&amp;quot;--master=http://172.20.0.113:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用。&lt;/p&gt;

&lt;p&gt;apiserver配置文件&lt;code&gt;/etc/kubernetes/apiserver&lt;/code&gt;内容为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS=&amp;quot;--insecure-bind-address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
KUBE_API_ADDRESS=&amp;quot;--advertise-address=172.20.0.113 --bind-address=172.20.0.113 --insecure-bind-address=172.20.0.113&amp;quot;
#
## The port on the local server to listen on.
#KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
#KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=https://172.20.0.113:2379,172.20.0.114:2379,172.20.0.115:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota&amp;quot;
#
## Add your own!
KUBE_API_ARGS=&amp;quot;--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--authorization-mode=RBAC&lt;/code&gt; 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；&lt;/li&gt;
&lt;li&gt;kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用&lt;strong&gt;非安全端口&lt;/strong&gt;和 kube-apiserver通信;&lt;/li&gt;
&lt;li&gt;kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过&lt;strong&gt;安全端口&lt;/strong&gt;访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；&lt;/li&gt;
&lt;li&gt;kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；&lt;/li&gt;
&lt;li&gt;如果使用了 kubelet TLS Boostrap 机制，则不能再指定 &lt;code&gt;--kubelet-certificate-authority&lt;/code&gt;、&lt;code&gt;--kubelet-client-certificate&lt;/code&gt; 和 &lt;code&gt;--kubelet-client-key&lt;/code&gt; 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--admission-control&lt;/code&gt; 值必须包含 &lt;code&gt;ServiceAccount&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--bind-address&lt;/code&gt; 不能为 &lt;code&gt;127.0.0.1&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;runtime-config&lt;/code&gt;配置为&lt;code&gt;rbac.authorization.k8s.io/v1beta1&lt;/code&gt;，表示运行时的apiVersion；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--service-cluster-ip-range&lt;/code&gt; 指定 Service Cluster IP 地址段，该地址段不能路由可达；&lt;/li&gt;
&lt;li&gt;缺省情况下 kubernetes 对象保存在 etcd &lt;code&gt;/registry&lt;/code&gt; 路径下，可以通过 &lt;code&gt;--etcd-prefix&lt;/code&gt; 参数进行调整；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整 unit 见 &lt;a href=&#34;./systemd/kube-apiserver.service&#34;&gt;kube-apiserver.service&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动kube-apiserver&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kube-apiserver
$ systemctl start kube-apiserver
$ systemctl status kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置和启动-kube-controller-manager&#34;&gt;配置和启动 kube-controller-manager&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;创建 kube-controller-manager的serivce配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文件路径&lt;code&gt;/usr/lib/systemd/system/kube-controller-manager.service&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件&lt;code&gt;/etc/kubernetes/controller-manager&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;--service-cluster-ip-range&lt;/code&gt; 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--cluster-signing-*&lt;/code&gt; 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--root-ca-file&lt;/code&gt; 用来对 kube-apiserver 证书进行校验，&lt;strong&gt;指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件&lt;/strong&gt;；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--address&lt;/code&gt; 值必须为 &lt;code&gt;127.0.0.1&lt;/code&gt;，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器，否则：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  $ kubectl get componentstatuses
  NAME                 STATUS      MESSAGE                                                                                        ERROR
  scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
  controller-manager   Healthy     ok                                                                                             
  etcd-2               Unhealthy   Get http://172.20.0.113:2379/health: malformed HTTP response &amp;quot;\x15\x03\x01\x00\x02\x02&amp;quot;        
  etcd-0               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}                                                                             
  etcd-1               Healthy     {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考：&lt;a href=&#34;https://github.com/kubernetes-incubator/bootkube/issues/64&#34;&gt;https://github.com/kubernetes-incubator/bootkube/issues/64&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;完整 unit 见 &lt;a href=&#34;./systemd/kube-controller-manager.service&#34;&gt;kube-controller-manager.service&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-kube-controller-manager&#34;&gt;启动 kube-controller-manager&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kube-controller-manager
$ systemctl start kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置和启动-kube-scheduler&#34;&gt;配置和启动 kube-scheduler&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;创建 kube-scheduler的serivce配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;文件路径&lt;code&gt;/usr/lib/systemd/system/kube-scheduler.serivce&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler \
            $KUBE_LOGTOSTDERR \
            $KUBE_LOG_LEVEL \
            $KUBE_MASTER \
            $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置文件&lt;code&gt;/etc/kubernetes/scheduler&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;--leader-elect=true --address=127.0.0.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--address&lt;/code&gt; 值必须为 &lt;code&gt;127.0.0.1&lt;/code&gt;，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整 unit 见 &lt;a href=&#34;./systemd/kube-scheduler.service&#34;&gt;kube-scheduler.service&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-kube-scheduler&#34;&gt;启动 kube-scheduler&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kube-scheduler
$ systemctl start kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;验证-master-节点功能&#34;&gt;验证 master 节点功能&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-1               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
etcd-2               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;当时在配置过程中遇到了问题&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/4&#34;&gt;TLS认证相关的问题&lt;/a&gt;，其实就是因为配置apiserver时候etcd的协议写成了http导致的，应该是用https。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;Opsnull&lt;/a&gt;写的kubernetes高可用master集群部署过程中似乎并没有包括&lt;strong&gt;高可用的配置&lt;/strong&gt;，才云科技的唐继元分享过&lt;a href=&#34;https://segmentfault.com/a/1190000005832319&#34;&gt;Kubernetes Master High Availability 高级实践&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;究竟如何实现kubernetes master的高可用还需要继续探索。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes安装之etcd高可用配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</link>
      <pubDate>Tue, 11 Apr 2017 15:21:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2015091401.jpg&#34; alt=&#34;西山俯瞰北京夜景&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：北京夜景@西山）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目((fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;))中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，生成&lt;strong&gt;kubeconfig&lt;/strong&gt;文件的过程。&lt;/p&gt;

&lt;h2 id=&#34;创建高可用-etcd-集群&#34;&gt;创建高可用 etcd 集群&lt;/h2&gt;

&lt;p&gt;kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为&lt;code&gt;sz-pg-oam-docker-test-001.tendcloud.com&lt;/code&gt;、&lt;code&gt;sz-pg-oam-docker-test-002.tendcloud.com&lt;/code&gt;、&lt;code&gt;sz-pg-oam-docker-test-003.tendcloud.com&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tls-认证文件&#34;&gt;TLS 认证文件&lt;/h2&gt;

&lt;p&gt;需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;kubernetes 证书的 &lt;code&gt;hosts&lt;/code&gt; 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;下载二进制文件&#34;&gt;下载二进制文件&lt;/h2&gt;

&lt;p&gt;到 &lt;code&gt;https://github.com/coreos/etcd/releases&lt;/code&gt; 页面下载最新版本的二进制文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz
$ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz
$ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建-etcd-的-systemd-unit-文件&#34;&gt;创建 etcd 的 systemd unit 文件&lt;/h2&gt;

&lt;p&gt;注意替换 &lt;code&gt;ETCD_NAME&lt;/code&gt; 和 &lt;code&gt;INTERNAL_IP&lt;/code&gt; 变量的值；&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export ETCD_NAME=sz-pg-oam-docker-test-001.tendcloud.com
$ export INTERNAL_IP=172.20.0.113
$ sudo mkdir -p /var/lib/etcd /var/lib/etcd
$ cat &amp;gt; etcd.service &amp;lt;&amp;lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/root/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster sz-pg-oam-docker-test-001.tendcloud.com=https://172.20.0.113:2380,sz-pg-oam-docker-test-002.tendcloud.com=https://172.20.0.114:2380,sz-pg-oam-docker-test-003.tendcloud.com=https://172.20.0.115:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;指定 &lt;code&gt;etcd&lt;/code&gt; 的工作目录为 &lt;code&gt;/var/lib/etcd&lt;/code&gt;，数据目录为 &lt;code&gt;/var/lib/etcd&lt;/code&gt;，需在启动服务前创建这两个目录；&lt;/li&gt;
&lt;li&gt;为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；&lt;/li&gt;
&lt;li&gt;创建 &lt;code&gt;kubernetes.pem&lt;/code&gt; 证书时使用的 &lt;code&gt;kubernetes-csr.json&lt;/code&gt; 文件的 &lt;code&gt;hosts&lt;/code&gt; 字段&lt;strong&gt;包含所有 etcd 节点的 INTERNAL_IP&lt;/strong&gt;，否则证书校验会出错；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--initial-cluster-state&lt;/code&gt; 值为 &lt;code&gt;new&lt;/code&gt; 时，&lt;code&gt;--name&lt;/code&gt; 的参数值必须位于 &lt;code&gt;--initial-cluster&lt;/code&gt; 列表中；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整 unit 文件见：&lt;a href=&#34;./systemd/etcd.service&#34;&gt;etcd.service&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;启动-etcd-服务&#34;&gt;启动 etcd 服务&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo mv etcd.service /etc/systemd/system/
$ sudo systemctl daemon-reload
$ sudo systemctl enable etcd
$ sudo systemctl start etcd
$ systemctl status etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在所有的 kubernetes master 节点重复上面的步骤，直到所有机器的 etcd 服务都已启动。&lt;/p&gt;

&lt;h2 id=&#34;验证服务&#34;&gt;验证服务&lt;/h2&gt;

&lt;p&gt;在任一 kubernetes master 机器上执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ etcdctl \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  cluster-health
2017-04-11 15:17:09.082250 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
2017-04-11 15:17:09.083681 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated
member 9a2ec640d25672e5 is healthy: got healthy result from https://172.20.0.115:2379
member bc6f27ae3be34308 is healthy: got healthy result from https://172.20.0.114:2379
member e5c92ea26c4edba0 is healthy: got healthy result from https://172.20.0.113:2379
cluster is healthy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果最后一行为 &lt;code&gt;cluster is healthy&lt;/code&gt; 时表示集群服务正常。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes安装之创建kubeconfig文件</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</link>
      <pubDate>Tue, 11 Apr 2017 14:34:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2016050801.jpg&#34; alt=&#34;北海公园&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北海公园 May 8,2016)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;这是&lt;a href=&#34;https://github.com/rootsongjc/follow-me-install-kubernetes-cluster&#34;&gt;和我一步步部署kubernetes集群&lt;/a&gt;项目((fork自&lt;a href=&#34;https://github.com/opsnull/follow-me-install-kubernetes-cluster&#34;&gt;opsnull&lt;/a&gt;))中的一篇文章，下文是结合我&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;之前部署kubernetes的过程&lt;/a&gt;产生的kuberentes环境，生成&lt;strong&gt;kubeconfig&lt;/strong&gt;文件的过程。
&lt;code&gt;kubelet&lt;/code&gt;、&lt;code&gt;kube-proxy&lt;/code&gt; 等 Node 机器上的进程与 Master 机器的 &lt;code&gt;kube-apiserver&lt;/code&gt; 进程通信时需要认证和授权；
kubernetes 1.4 开始支持由 &lt;code&gt;kube-apiserver&lt;/code&gt; 为客户端生成 TLS 证书的 &lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/&#34;&gt;TLS Bootstrapping&lt;/a&gt; 功能，这样就不需要为每个客户端生成证书了；该功能&lt;strong&gt;当前仅支持为 &lt;code&gt;kubelet&lt;/code&gt;&lt;/strong&gt; 生成证书。&lt;/p&gt;

&lt;h2 id=&#34;创建-tls-bootstrapping-token&#34;&gt;创建 TLS Bootstrapping Token&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Token auth file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;)
cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;后三行是一句，直接复制上面的脚本运行即可。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;将token.csv发到所有机器（Master 和 Node）的 &lt;code&gt;/etc/kubernetes/&lt;/code&gt; 目录。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$cp token.csv /etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;创建-kubelet-bootstrapping-kubeconfig-文件&#34;&gt;创建 kubelet bootstrapping kubeconfig 文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd /etc/kubernetes
$ export KUBE_APISERVER=&amp;quot;https://172.20.0.113:6443&amp;quot;
$ # 设置集群参数
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig
$ # 设置客户端认证参数
$ kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
$ # 设置上下文参数
$ kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
$ # 设置默认上下文
$ kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--embed-certs&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt; 时表示将 &lt;code&gt;certificate-authority&lt;/code&gt; 证书写入到生成的 &lt;code&gt;bootstrap.kubeconfig&lt;/code&gt; 文件中；&lt;/li&gt;
&lt;li&gt;设置客户端认证参数时&lt;strong&gt;没有&lt;/strong&gt;指定秘钥和证书，后续由 &lt;code&gt;kube-apiserver&lt;/code&gt; 自动生成；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;创建-kube-proxy-kubeconfig-文件&#34;&gt;创建 kube-proxy kubeconfig 文件&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export KUBE_APISERVER=&amp;quot;https://172.20.0.113:6443&amp;quot;
$ # 设置集群参数
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
$ # 设置客户端认证参数
$ kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
$ # 设置上下文参数
$ kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
$ # 设置默认上下文
$ kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;设置集群参数和客户端认证参数时 &lt;code&gt;--embed-certs&lt;/code&gt; 都为 &lt;code&gt;true&lt;/code&gt;，这会将 &lt;code&gt;certificate-authority&lt;/code&gt;、&lt;code&gt;client-certificate&lt;/code&gt; 和 &lt;code&gt;client-key&lt;/code&gt; 指向的证书文件内容写入到生成的 &lt;code&gt;kube-proxy.kubeconfig&lt;/code&gt; 文件中；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-proxy.pem&lt;/code&gt; 证书中 CN 为 &lt;code&gt;system:kube-proxy&lt;/code&gt;，&lt;code&gt;kube-apiserver&lt;/code&gt; 预定义的 RoleBinding &lt;code&gt;cluster-admin&lt;/code&gt; 将User &lt;code&gt;system:kube-proxy&lt;/code&gt; 与 Role &lt;code&gt;system:node-proxier&lt;/code&gt; 绑定，该 Role 授予了调用 &lt;code&gt;kube-apiserver&lt;/code&gt; Proxy 相关 API 的权限；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;分发-kubeconfig-文件&#34;&gt;分发 kubeconfig 文件&lt;/h2&gt;

&lt;p&gt;将两个 kubeconfig 文件分发到所有 Node 机器的 &lt;code&gt;/etc/kubernetes/&lt;/code&gt; 目录&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>开源微服务管理平台fabric8简介</title>
      <link>http://rootsongjc.github.io/blogs/fabric8-introduction/</link>
      <pubDate>Mon, 10 Apr 2017 21:39:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/fabric8-introduction/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://fabric8.io/images/fabric8_logo.svg&#34; alt=&#34;fabric8&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;无意中发现&lt;a href=&#34;https://github.com/fabric8io/fabric8&#34;&gt;Fabric8&lt;/a&gt;这个&lt;strong&gt;对于Java友好的开源微服务管理平台&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;其实这在这里发现的&lt;a href=&#34;http://blog.sonatype.com/achieving-ci/cd-with-kubernetes&#34;&gt;Achieving CI/CD with Kubernetes&lt;/a&gt;（by Ramit Surana,on February 17, 2017），其实是先在&lt;a href=&#34;https://www.slideshare.net/ramitsurana/achieving-cicd-with-kubernetes&#34;&gt;slideshare&lt;/a&gt;上看到的，pdf可以&lt;a href=&#34;http://olz1di9xf.bkt.clouddn.com/achiveving-ci-cd-with-kubernetes-ramit-surana.pdf&#34;&gt;在此下载&lt;/a&gt;，大小2.04M。&lt;/p&gt;

&lt;p&gt;大家可能以前听过一个叫做&lt;a href=&#34;https://github.com/fabric/fabric/&#34;&gt;fabric&lt;/a&gt;的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把fabric8跟fabric搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，&lt;strong&gt;fabric8不是fabric的一个版本&lt;/strong&gt;。Fabric是用python开发的，fabric8是java开发的。&lt;/p&gt;

&lt;p&gt;如果你想了解简化Fabric可以看它的&lt;a href=&#34;http://fabric-docs-cn.readthedocs.io/zh_CN/latest/tutorial.html&#34;&gt;中文官方文档&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;fabric8简介&#34;&gt;Fabric8简介&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;fabric8&lt;/strong&gt;是一个开源&lt;strong&gt;集成开发平台&lt;/strong&gt;，为基于&lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;和&lt;a href=&#34;https://jenkins.io/&#34;&gt;Jenkins&lt;/a&gt;的微服务提供&lt;a href=&#34;http://fabric8.io/guide/cdelivery.html&#34;&gt;持续发布&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;使用fabric可以很方便的通过&lt;a href=&#34;http://fabric8.io/guide/cdelivery.html&#34;&gt;Continuous Delivery pipelines&lt;/a&gt;创建、编译、部署和测试微服务，然后通过Continuous Improvement和&lt;a href=&#34;http://fabric8.io/guide/chat.html&#34;&gt;ChatOps&lt;/a&gt;运行和管理他们。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://fabric8.io/guide/fabric8DevOps.html&#34;&gt;Fabric8微服务平台&lt;/a&gt;提供：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/console.html&#34;&gt;Developer Console&lt;/a&gt;，是一个&lt;a href=&#34;http://www.infoq.com/cn/news/2014/11/seven-principles-rich-web-app&#34;&gt;富web应用&lt;/a&gt;，提供一个单页面来创建、编辑、编译、部署和测试微服务。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/cdelivery.html&#34;&gt;Continuous Integration and Continous Delivery&lt;/a&gt;，使用 &lt;a href=&#34;https://jenkins.io/&#34;&gt;Jenkins&lt;/a&gt; with a &lt;a href=&#34;http://fabric8.io/guide/jenkinsWorkflowLibrary.html&#34;&gt;Jenkins Workflow Library&lt;/a&gt;更快和更可靠的交付软件。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/management.html&#34;&gt;Management&lt;/a&gt;，集中式管理&lt;a href=&#34;http://fabric8.io/guide/logging.html&#34;&gt;Logging&lt;/a&gt;、&lt;a href=&#34;http://fabric8.io/guide/metrics.html&#34;&gt;Metrics&lt;/a&gt;, &lt;a href=&#34;http://fabric8.io/guide/chat.html&#34;&gt;ChatOps&lt;/a&gt;、&lt;a href=&#34;http://fabric8.io/guide/chaosMonkey.html&#34;&gt;Chaos Monkey&lt;/a&gt;，使用&lt;a href=&#34;http://hawt.io/&#34;&gt;Hawtio&lt;/a&gt;和&lt;a href=&#34;http://jolokia.org/&#34;&gt;Jolokia&lt;/a&gt;管理Java Containers。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/ipaas.html&#34;&gt;Integration&lt;/a&gt; &lt;u&gt;&lt;em&gt;Integration Platform As A Service&lt;/em&gt;&lt;/u&gt; with &lt;a href=&#34;http://fabric8.io/guide/console.html&#34;&gt;deep visualisation&lt;/a&gt; of your &lt;a href=&#34;http://camel.apache.org/&#34;&gt;Apache Camel&lt;/a&gt; integration services, an &lt;a href=&#34;http://fabric8.io/guide/apiRegistry.html&#34;&gt;API Registry&lt;/a&gt; to view of all your RESTful and SOAP APIs and &lt;a href=&#34;http://fabric8.io/guide/fabric8MQ.html&#34;&gt;Fabric8 MQ&lt;/a&gt; provides &lt;u&gt;&lt;em&gt;Messaging As A Service&lt;/em&gt;&lt;/u&gt; based on &lt;a href=&#34;http://activemq.apache.org/&#34;&gt;Apache ActiveMQ&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/tools.html&#34;&gt;Java Tools&lt;/a&gt; 帮助Java应用使用&lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/mavenPlugin.html&#34;&gt;Maven Plugin&lt;/a&gt; for working with &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; ，这真是极好的&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/testing.html&#34;&gt;Integration and System Testing&lt;/a&gt; of &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; resources easily inside &lt;a href=&#34;http://junit.org/&#34;&gt;JUnit&lt;/a&gt; with &lt;a href=&#34;http://arquillian.org/&#34;&gt;Arquillian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/javaLibraries.html&#34;&gt;Java Libraries&lt;/a&gt; and support for &lt;a href=&#34;http://fabric8.io/guide/cdi.html&#34;&gt;CDI&lt;/a&gt; extensions for working with &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;fabric8微服务平台&#34;&gt;Fabric8微服务平台&lt;/h2&gt;

&lt;p&gt;Fabric8提供了一个完全集成的开源微服务平台，可在任何的&lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;和&lt;a href=&#34;http://www.openshift.org/&#34;&gt;OpenShift&lt;/a&gt;环境中开箱即用。&lt;/p&gt;

&lt;p&gt;整个平台是基于微服务而且是模块化的，你可以按照微服务的方式来使用它。&lt;/p&gt;

&lt;p&gt;微服务平台提供的服务有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开发者控制台，这是一个富Web应用程序，它提供了一个单一的页面来创建、编辑、编译、部署和测试微服务。&lt;/li&gt;
&lt;li&gt;持续集成和持续交付，帮助团队以更快更可靠的方式交付软件，可以使用以下开源软件：

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jenkins.io/&#34;&gt;Jenkins&lt;/a&gt;：CI／CD pipeline&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sonatype.org/nexus/&#34;&gt;Nexus&lt;/a&gt;： 组件库&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://gogs.io/&#34;&gt;Gogs&lt;/a&gt;：git代码库&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sonarqube.org/&#34;&gt;SonarQube&lt;/a&gt;：代码质量维护平台&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/jenkinsWorkflowLibrary.html&#34;&gt;Jenkins Workflow Library&lt;/a&gt;：在不同的项目中复用&lt;a href=&#34;https://github.com/fabric8io/jenkins-workflow-library&#34;&gt;Jenkins Workflow scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/fabric8YmlFile.html&#34;&gt;Fabric8.yml&lt;/a&gt;：为每个项目、存储库、聊天室、工作流脚本和问题跟踪器提供一个配置文件&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/chat.html&#34;&gt;ChatOps&lt;/a&gt;：通过使用&lt;a href=&#34;https://hubot.github.com/&#34;&gt;hubot&lt;/a&gt;来开发和管理，能够让你的团队拥抱DevOps，通过聊天和系统通知的方式来&lt;a href=&#34;https://github.com/fabric8io/fabric8-jenkins-workflow-steps#hubotapprove&#34;&gt;approval of release promotion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/chaosMonkey.html&#34;&gt;Chaos Monkey&lt;/a&gt;：通过干掉&lt;a href=&#34;http://fabric8.io/guide/pods.html&#34;&gt;pods&lt;/a&gt;来测试系统健壮性和可靠性&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/management.html&#34;&gt;管理&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/logging.html&#34;&gt;日志&lt;/a&gt; 统一集群日志和可视化查看状态&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fabric8.io/guide/metrics.html&#34;&gt;metris&lt;/a&gt; 可查看历史metrics和可视化&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://hao.jobbole.com/fabric8/&#34;&gt;fabric8：容器集成平台——伯乐在线&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://mp.weixin.qq.com/s?__biz=MzI0NjI4MDg5MQ==&amp;amp;mid=2715290731&amp;amp;idx=1&amp;amp;sn=f1fcacb9aa4f1f3037918f03c29c0465&amp;amp;chksm=cd6d0bbffa1a82a978ccc0405afa295bd9265bd9f89f2217c80f48e1c497b25d1f24090108af&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0410RTk3PKkxlFlLbCVlOKMK#rd&#34;&gt;Kubernetes部署微服务速成指南——&lt;em&gt;2017-03-09&lt;/em&gt; &lt;em&gt;徐薛彪&lt;/em&gt; 容器时代微信公众号&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;上面那篇文章是翻译的，英文原文地址：&lt;a href=&#34;http://www.eclipse.org/community/eclipse_newsletter/2017/january/article2.php&#34;&gt;Quick Guide to Developing Microservices on Kubernetes and Docker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://fabric8.io/&#34;&gt;fabric8官网&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://fabric8.io/guide/getStarted/gofabric8.html&#34;&gt;fabric8 get started&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;&lt;del&gt;我在自己笔记本上装了个minikube，试玩感受将在后续发表。&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;试玩时需要科学上网。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$gofabric8 start
using the executable /usr/local/bin/minikube
minikube already running
using the executable /usr/local/bin/kubectl
Switched to context &amp;quot;minikube&amp;quot;.
Deploying fabric8 to your Kubernetes installation at https://192.168.99.100:8443 for domain  in namespace default

Loading fabric8 releases from maven repository:https://repo1.maven.org/maven2/
Deploying package: platform version: 2.4.24

Now about to install package https://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml
Processing resource kind: Namespace in namespace default name user-secrets-source-admin
Found namespace on kind Secret of user-secrets-source-adminProcessing resource kind: Secret in namespace user-secrets-source-admin name default-gogs-git
Processing resource kind: Secret in namespace default name jenkins-docker-cfg
Processing resource kind: Secret in namespace default name jenkins-git-ssh
Processing resource kind: Secret in namespace default name jenkins-hub-api-token
Processing resource kind: Secret in namespace default name jenkins-master-ssh
Processing resource kind: Secret in namespace default name jenkins-maven-settings
Processing resource kind: Secret in namespace default name jenkins-release-gpg
Processing resource kind: Secret in namespace default name jenkins-ssh-config
Processing resource kind: ServiceAccount in namespace default name configmapcontroller
Processing resource kind: ServiceAccount in namespace default name exposecontroller
Processing resource kind: ServiceAccount in namespace default name fabric8
Processing resource kind: ServiceAccount in namespace default name gogs
Processing resource kind: ServiceAccount in namespace default name jenkins
Processing resource kind: Service in namespace default name fabric8
Processing resource kind: Service in namespace default name fabric8-docker-registry
Processing resource kind: Service in namespace default name fabric8-forge
Processing resource kind: Service in namespace default name gogs
...
-------------------------

Default GOGS admin username/password = gogsadmin/RedHat$1

Checking if PersistentVolumeClaims bind to a PersistentVolume ....
Downloading images and waiting to open the fabric8 console...

-------------------------
.....................................................
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动了半天一直是这种状态：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Waiting, endpoint for service is not ready yet...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我一看下载下来的&lt;code&gt;https://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml&lt;/code&gt;文件，真是&lt;strong&gt;蔚为壮观&lt;/strong&gt;啊，足足有&lt;strong&gt;24712行&lt;/strong&gt;(这里面都是实际配置，没有配置充行数)，使用了如下这些docker镜像，足足有&lt;strong&gt;53个docker镜像&lt;/strong&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fabric8/alpine-caddy:2.2.311
fabric8/apiman-gateway:2.2.168
fabric8/apiman:2.2.168
fabric8/chaos-monkey:2.2.311
fabric8/configmapcontroller:2.3.5
fabric8/eclipse-orion:2.2.311
fabric8/elasticsearch-k8s:2.3.4
fabric8/elasticsearch-logstash-template:2.2.311
fabric8/elasticsearch-v1:2.2.168
fabric8/exposecontroller:2.3.2
fabric8/fabric8-console:2.2.199
fabric8/fabric8-forge:2.3.88
fabric8/fabric8-kiwiirc:2.2.311
fabric8/fluentd-kubernetes:v1.19
fabric8/gerrit:2.2.311
fabric8/git-collector:2.2.311
fabric8/gogs:v0.9.97
fabric8/grafana:2.6.1
fabric8/hubot-irc:2.2.311
fabric8/hubot-letschat:v1.0.0
fabric8/hubot-notifier:2.2.311
fabric8/hubot-slack:2.2.311
fabric8/jenkins-docker:2.2.311
fabric8/jenkinshift:2.2.199
fabric8/kafka:2.2.153
fabric8/kibana-config:2.2.311
fabric8/kibana4:v4.5.3
fabric8/lets-chat:2.2.311
fabric8/maven-builder:2.2.311
fabric8/message-broker:2.2.168
fabric8/message-gateway:2.2.168
fabric8/nexus:2.2.311
fabric8/taiga-back:2.2.311
fabric8/taiga-front:2.2.311
fabric8/turbine-server:1.0.28
fabric8/zookeeper:2.2.153
fabric8/zookeeper:2.2.168
funktion/funktion-nodejs-runtime:1.0.3
funktion/funktion:1.0.9
gitlab/gitlab-ce
jboss/keycloak:2.2.0.Final
jfrog-docker-registry.bintray.io/artifactory/artifactory-oss
jimmidyson/configmap-reload:v0.1
manageiq/manageiq:latest
mongo
mysql:5.7
nginxdemos/nginx-ingress:0.3.1
openzipkin/zipkin:1.13.0
postgres
prom/blackbox-exporter:master
prom/node-exporter
prom/prometheus:v1.3.1
registry:2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你们感受下吧，我果断放弃了在自己笔记本上安装的念头。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>