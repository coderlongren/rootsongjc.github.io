<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Jimmy&#39;s blog</title>
    <link>http://rootsongjc.github.io/blogs/index.xml</link>
    <description>Recent content in Blogs on Jimmy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Mar 2017 14:23:04 +0800</lastBuildDate>
    <atom:link href="http://rootsongjc.github.io/blogs/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Contiv人坑指南-试用全记录</title>
      <link>http://rootsongjc.github.io/blogs/contiv-tryout/</link>
      <pubDate>Thu, 09 Mar 2017 14:23:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-tryout/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017013129.jpg&#34; alt=&#34;黄昏&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北纬37度黄海之滨风力发电场，冬天的大风持续给人类提供清洁的能源）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;关于contiv的介绍请看我的上一篇文章&lt;a href=&#34;http://rootsongjc.github.io/post/contiv_guide/&#34;&gt;Contiv Intro&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;开发环境使用&lt;strong&gt;Vagrant&lt;/strong&gt;搭建，昨天试用了下，真不知道它们是怎么想的，即然是docker插件为啥不直接在docker中开发呢，我有篇文章介绍&lt;a href=&#34;http://rootsongjc.github.io/post/docker-dev-env/&#34;&gt;如何搭建docker开发环境&lt;/a&gt;，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像&lt;code&gt;dockercore/docker:latest&lt;/code&gt;即可，不过有点大2.31G，使用阿里云的mirror下载倒是也划算，总比你自己部署一个开发环境节省时间。&lt;/p&gt;

&lt;h3 id=&#34;contiv概念解析&#34;&gt;Contiv概念解析&lt;/h3&gt;

&lt;p&gt;Contiv用于给容器创建和分配网路，可以创建策略管理容器的安全、带宽、优先级等，相当于一个SDN。&lt;/p&gt;

&lt;h4 id=&#34;group&#34;&gt;Group&lt;/h4&gt;

&lt;p&gt;按容器或Pod的功能给容器分配策略组，通常是按照容器/Pod的&lt;code&gt;label&lt;/code&gt;来分组，应用组跟contiv的network不是一一对应的，可以很多应用组属于同一个network或IP subnet。&lt;/p&gt;

&lt;h4 id=&#34;polices&#34;&gt;Polices&lt;/h4&gt;

&lt;p&gt;用来限定group的行为，contiv支持两种类型的policy：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bandwidth 限定应用组的资源使用上限&lt;/li&gt;
&lt;li&gt;Isolation 资源组的访问权限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Group可以同时应用一个或多个policy，当有容器调度到该group里就会适用该group的policy。&lt;/p&gt;

&lt;h4 id=&#34;network&#34;&gt;Network&lt;/h4&gt;

&lt;p&gt;IPv4或IPv6网络，可以配置subnet和gateway。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contiv中的网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在contiv中可以配置两种类型的网络&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;application network：容器使用的网络&lt;/li&gt;
&lt;li&gt;infrastructure network：host namespace的虚拟网络，比如基础设施监控网络&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;网络封装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Contiv中有两种类型的网络封装&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Routed：overlay topology和L3-routed BGP topology&lt;/li&gt;
&lt;li&gt;Bridged：layer2 VLAN&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;tenant&#34;&gt;Tenant&lt;/h4&gt;

&lt;p&gt;Tenant提供contiv中的namespace隔离。一个tenant可以有很多个network，每个network都有个subnet。该tenant中的用户可以使用它的任意network和subnet的IP。&lt;/p&gt;

&lt;p&gt;物理网络中的tenant称作&lt;code&gt;虚拟路由转发(VRF)&lt;/code&gt;。Contiv使用VLAN和VXLAN ID来实现外部网络访问，这取决你使用的是layer2、layer3还是Cisco ACI。&lt;/p&gt;

&lt;h3 id=&#34;contiv下载&#34;&gt;Contiv下载&lt;/h3&gt;

&lt;p&gt;Contiv的编译安装比较复杂，我们直接下载github上的&lt;a href=&#34;[1.0.0-beta.3-03-08-2017.18-51-20.UTC](https://github.com/contiv/netplugin/releases/tag/1.0.0-beta.3-03-08-2017.18-51-20.UTC)&#34;&gt;release-1.0.0-beta.3-03-08-2017.18-51-20.UTC&lt;/a&gt;文件解压获得二进制文件安装。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。&#34;&gt;https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果试用可以的话，我会后续写contiv开发环境搭建的文章。&lt;/p&gt;

&lt;p&gt;这个release是2017年3月8日发布的，就在我写这篇文章的前一天。有个&lt;strong&gt;最重要的更新&lt;/strong&gt;是&lt;u&gt;支持docker1.13 swarm mode&lt;/u&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/contiv/netplugin/blob/master/install/HowtoSetupContiv.md&#34;&gt;官方安装文档&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下载解压后会得到如下几个文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;contivk8s  k8s专用的&lt;/li&gt;
&lt;li&gt;contrib  文件夹，里面有个&lt;code&gt;netctl&lt;/code&gt;的bash脚本&lt;/li&gt;
&lt;li&gt;netcontiv  这个命令就一个-version选项用来查看contiv的版本😓&lt;/li&gt;
&lt;li&gt;netctl  contiv命令行工具，用来配置网络、策略、服务负载均衡，&lt;a href=&#34;http://contiv.github.io/documents/reference/netctlcli.html&#34;&gt;使用说明&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;netmaster  contiv的主节点服务&lt;/li&gt;
&lt;li&gt;netplugin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面的安装中用到的只有netctl、netmaster和netplugin这三个二进制文件。&lt;/p&gt;

&lt;p&gt;我们将这三个文件都copy到/usr/bin目录下。&lt;/p&gt;

&lt;p&gt;我们在docker17.03-ce中安装contiv。&lt;/p&gt;

&lt;h3 id=&#34;contiv安装依赖&#34;&gt;Contiv安装依赖&lt;/h3&gt;

&lt;p&gt;Contiv依赖于consul或etcd，我们选择使用etcd，slack里的人说只支持2.3.x版本，可能不支持3.0+版本的吧，还没实际测过，先使用2.3.7。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;contiv master&lt;/code&gt;启动后自动向etcd中注册信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/contiv.io/oper
/contiv.io/oper/auto-vlan
/contiv.io/oper/auto-vlan/global
/contiv.io/oper/auto-vxlan
/contiv.io/oper/auto-vxlan/global
/contiv.io/oper/global
/contiv.io/oper/global/global
/contiv.io/oper/ovs-driver
/contiv.io/oper/ovs-driver/sz-pg-oam-docker-test-001.tendcloud.com
/contiv.io/master
/contiv.io/master/config
/contiv.io/master/config/global
/contiv.io/obj
/contiv.io/obj/modeldb
/contiv.io/obj/modeldb/global
/contiv.io/obj/modeldb/global/global
/contiv.io/obj/modeldb/tenant
/contiv.io/obj/modeldb/tenant/default
/contiv.io/lock
/contiv.io/lock/netmaster
/contiv.io/lock/netmaster/leader
/contiv.io/service
/contiv.io/service/netmaster
/contiv.io/service/netmaster/172.20.0.113:9999
/contiv.io/service/netmaster.rpc
/contiv.io/service/netmaster.rpc/172.20.0.113:9001
/contiv.io/state
/contiv.io/state/auto-vlan
/contiv.io/state/auto-vlan/global
/contiv.io/state/auto-vxlan
/contiv.io/state/auto-vxlan/global
/contiv.io/state/global
/contiv.io/state/global/global
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;contiv启动&#34;&gt;Contiv启动&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;启动netmaster&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$nohup netmaster -cluster-mode docker -cluster-store etcd://172.20.0.113:2379 -debug -listen-url 172.20.0.113:9999 -plugin-name netplugin &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了突出netmaster命令的使用，我把所有可以使用默认值的参数都明确的写出。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netmaster&lt;/code&gt;监听9999端口。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看已有的contiv网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl --netmaster http://172.20.0.113:9999 network ls
Tenant  Network  Nw Type  Encap type  Packet tag  Subnet   Gateway  IPv6Subnet  IPv6Gateway
------  -------  -------  ----------  ----------  -------  ------   ----------  -----------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了以后执行命令方便，不用来回输入&lt;code&gt;$NETMASTER&lt;/code&gt;地址，可以将其设置为环境变量&lt;/p&gt;

&lt;p&gt;&lt;code&gt;export NETMASTER=&amp;quot;http://172.20.0.113:9999&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;netplugin需要使用Open vSwitch，所以你需要先安装&lt;strong&gt;Open vSwitch&lt;/strong&gt;。否则你会遇到这个问题&lt;a href=&#34;https://github.com/contiv/netplugin/issues/760&#34;&gt;netplugin issue-760&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;open-vswitch安装&#34;&gt;Open vSwitch安装&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://supercomputing.caltech.edu/blog/index.php/2016/05/03/open-vswitch-installation-on-centos-7-2/&#34;&gt;Open vSwitch installation on CentOS7.2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考上面链接里的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash
yum -y install make gcc openssl-devel autoconf automake rpm-build redhat-rpm-config python-devel openssl-devel kernel-devel kernel-debug-devel libtool wget
mkdir -p ~/rpmbuild/SOURCES
cp openvswitch-2.5.1.tar.gz ~/rpmbuild/SOURCES/
tar xfz openvswitch-2.5.1.tar.gz
sed &#39;s/openvswitch-kmod, //g&#39; openvswitch-2.5.1/rhel/openvswitch.spec &amp;gt; openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec
rpmbuild -bb --nocheck ~/openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译好的rpm包在&lt;code&gt;~/rpmbuild/RPMS/x86_64/openvswitch-2.5.1-1.x86_64.rpm&lt;/code&gt;目录下。&lt;/p&gt;

&lt;p&gt;安装好Open vSwitch后就可以启动&lt;strong&gt;netplugin&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;创建contiv网络&#34;&gt;创建contiv网络&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;启动netplugin&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nohup netplugin -cluster-store etcd://172.20.0.113:2379 &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netctl --netmaster http://172.20.0.113:9999 network create --subnet=10.1.2.0/24 contiv-net&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;获得以下报错：&lt;/p&gt;

&lt;p&gt;ERRO[0000] Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&lt;/p&gt;

&lt;p&gt;但是执行第二次的时候居然成功了，不过当我查看docker network的时候根本就看不到刚刚创建的contiv-net网络。*这只是一场游戏一场梦。。。*😢&lt;/p&gt;

&lt;p&gt;Creating network default:contiv-net&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl network ls
Tenant   Network     Nw Type  Encap type  Packet tag  Subnet       Gateway  IPv6Subnet  IPv6Gateway
------   -------     -------  ----------  ----------  -------      ------   ----------  -----------
default  contiv-net  data     vxlan       0           10.1.2.0/24  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看刚创建的contiv-net网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl network inspect contiv-net
Inspeting network: contiv-net tenant: default
{
  &amp;quot;Config&amp;quot;: {
    &amp;quot;key&amp;quot;: &amp;quot;default:contiv-net&amp;quot;,
    &amp;quot;encap&amp;quot;: &amp;quot;vxlan&amp;quot;,
    &amp;quot;networkName&amp;quot;: &amp;quot;contiv-net&amp;quot;,
    &amp;quot;nwType&amp;quot;: &amp;quot;data&amp;quot;,
    &amp;quot;subnet&amp;quot;: &amp;quot;10.1.2.0/24&amp;quot;,
    &amp;quot;tenantName&amp;quot;: &amp;quot;default&amp;quot;,
    &amp;quot;link-sets&amp;quot;: {},
    &amp;quot;links&amp;quot;: {
      &amp;quot;Tenant&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;tenant&amp;quot;,
        &amp;quot;key&amp;quot;: &amp;quot;default&amp;quot;
      }
    }
  },
  &amp;quot;Oper&amp;quot;: {
    &amp;quot;availableIPAddresses&amp;quot;: &amp;quot;10.1.2.1-10.1.2.254&amp;quot;,
    &amp;quot;externalPktTag&amp;quot;: 1,
    &amp;quot;pktTag&amp;quot;: 1
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从&lt;strong&gt;netmaster&lt;/strong&gt;日志中可以看到如下报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time=&amp;quot;Mar  9 21:44:14.746627381&amp;quot; level=debug msg=&amp;quot;NwInfra type is default, no ACI&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.750278056&amp;quot; level=info msg=&amp;quot;Creating docker network: {CheckDuplicate:true Driver:netplugin EnableIPv6:false IPAM:0xc4204d8ea0 Internal:false Attachable:true Options:map[tenant:default encap:vxlan pkt-tag:1] Labels:map[]}&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752034749&amp;quot; level=error msg=&amp;quot;Error creating network contiv-net. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752067294&amp;quot; level=error msg=&amp;quot;Error creating network contiv-net.default in docker. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752102735&amp;quot; level=error msg=&amp;quot;Error creating network {&amp;amp;{Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752129195&amp;quot; level=error msg=&amp;quot;NetworkCreate retruned error for: &amp;amp;{Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752155973&amp;quot; level=error msg=&amp;quot;CreateNetwork error for: {Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752172138&amp;quot; level=error msg=&amp;quot;Handler for POST /api/v1/networks/default:contiv-net/ returned error: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;从日志中看到一个令人悲痛语句的话*legacy plugin netplugin of type NetworkDriver is not supported in swarm mode*，你们昨天不是刚发的版本说已经支持swarm mode吗？&lt;a href=&#34;https://github.com/contiv/netplugin/commit/8afd1b7718c8424a876760d18484124e0aad3557&#34;&gt;&lt;code&gt;commit-8afd1b7&lt;/code&gt;&lt;/a&gt;不是白纸黑字的写着吗？&lt;/p&gt;

&lt;p&gt;我提了个&lt;a href=&#34;https://github.com/contiv/netplugin/issues/776&#34;&gt;issue-776&lt;/a&gt;，看看怎样解决这个问题，另外netplugin命令怎么用，文档上没写啊？&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netplugin -h&lt;/code&gt;可以中有两个选项我不明白，不知道怎么设置，有知道的人请告诉我一声。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  -vlan-if value
    	VLAN uplink interface
  -vtep-ip string
    	My VTEP ip address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时我会继续关注contiv的slack和github &lt;a href=&#34;https://github.com/contiv/netplugin/issues/776&#34;&gt;Issue-776&lt;/a&gt;的进展。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contiv Intro</title>
      <link>http://rootsongjc.github.io/blogs/contiv-guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-guide/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017021162.jpg&#34; alt=&#34;蓝色港湾&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北京蓝色港湾夜景)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contiv&lt;/strong&gt;是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/1935&#34;&gt;容器网络插件 Calico 与 Contiv Netplugin深入比较&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有篇文章讲解了&lt;a href=&#34;http://blog.dataman-inc.com/shurenyun-docker-133/&#34;&gt;docker网络方案的改进&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contiv-netplugin-简介&#34;&gt;Contiv Netplugin 简介&lt;/h3&gt;

&lt;p&gt;Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34; alt=&#34;9260E9B7-43C0-48B8-B5C7-CF8B952959D2.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址&lt;/li&gt;
&lt;li&gt;Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。&lt;/li&gt;
&lt;li&gt;集群管理依赖 etcd/serf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34; alt=&#34;580469BC-468C-49C8-B29E-8B88143AFE0A.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;netplugin的优势&#34;&gt;Netplugin的优势&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等&lt;/li&gt;
&lt;li&gt;SDN能力，能够对容器的网络访问做更精细的控制&lt;/li&gt;
&lt;li&gt;多租户支持，具备未来向混合云/公有云迁移的潜力&lt;/li&gt;
&lt;li&gt;代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证&lt;/li&gt;
&lt;li&gt;&lt;u&gt;&lt;strong&gt;京东&lt;/strong&gt;基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。&lt;/u&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;next&#34;&gt;Next&lt;/h3&gt;

&lt;p&gt;后续文章会讲解contiv netplugin的环境配置和开发。目前还在1.0-beta版本。&lt;strong&gt;Docker store&lt;/strong&gt;上提供了contiv插件的&lt;a href=&#34;https://store.docker.com/plugins/803eecee-0780-401a-a454-e9523ccf86b3&#34;&gt;下载地址&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Packer Intro</title>
      <link>http://rootsongjc.github.io/blogs/packer-intro/</link>
      <pubDate>Thu, 09 Mar 2017 10:58:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/packer-intro/</guid>
      <description>&lt;p&gt;昨天研究了下&lt;a href=&#34;https://github.com/mitchellh/vagrant&#34;&gt;&lt;strong&gt;Vagrant&lt;/strong&gt;&lt;/a&gt;，感觉它的虚拟机ruby格式定义很麻烦，经人指点还有一个叫做&lt;a href=&#34;https://github.com/mitchellh/packer&#34;&gt;&lt;strong&gt;packer&lt;/strong&gt;&lt;/a&gt;的东西，也是Hashicorp这家公司出品的，今天看了下。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Packer&lt;/strong&gt;是一款开源轻量级的镜像定义工具，可以根据一份定义文件生成多个平台的镜像，支持的平台有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon EC2 (AMI). Both EBS-backed and instance-store AMIs&lt;/li&gt;
&lt;li&gt;Azure&lt;/li&gt;
&lt;li&gt;DigitalOcean&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Google Compute Engine&lt;/li&gt;
&lt;li&gt;OpenStack&lt;/li&gt;
&lt;li&gt;Parallels&lt;/li&gt;
&lt;li&gt;QEMU. Both KVM and Xen images.&lt;/li&gt;
&lt;li&gt;VirtualBox&lt;/li&gt;
&lt;li&gt;VMware&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Packer创造的镜像也能转换成&lt;strong&gt;Vagrant boxes&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Packer的镜像创建需要一个json格式的定义文件，例如&lt;code&gt;quick-start.json&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;access_key&amp;quot;: &amp;quot;{{env `AWS_ACCESS_KEY_ID`}}&amp;quot;,
    &amp;quot;secret_key&amp;quot;: &amp;quot;{{env `AWS_SECRET_ACCESS_KEY`}}&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [{
    &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;,
    &amp;quot;access_key&amp;quot;: &amp;quot;{{user `access_key`}}&amp;quot;,
    &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `secret_key`}}&amp;quot;,
    &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;,
    &amp;quot;source_ami&amp;quot;: &amp;quot;ami-af22d9b9&amp;quot;,
    &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;,
    &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;,
    &amp;quot;ami_name&amp;quot;: &amp;quot;packer-example {{timestamp}}&amp;quot;
  }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用&lt;code&gt;packer build quick-start.json&lt;/code&gt;可以在AWS上build一个AIM镜像。&lt;/p&gt;

&lt;p&gt;Packer的详细文档：&lt;a href=&#34;https://www.packer.io/docs/&#34;&gt;https://www.packer.io/docs/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vagrant介绍-从使用到放弃完全指南</title>
      <link>http://rootsongjc.github.io/blogs/vagrant-intro/</link>
      <pubDate>Wed, 08 Mar 2017 20:40:08 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/vagrant-intro/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017030513.jpg&#34; alt=&#34;光熙家园夜景&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：北京地铁13号线光熙家园夜景）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;起源&#34;&gt;起源&lt;/h2&gt;

&lt;p&gt;久闻&lt;strong&gt;Vagrant&lt;/strong&gt;大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。&lt;/p&gt;

&lt;p&gt;因为今天在看&lt;a href=&#34;https://github.com/contiv/netplugin&#34;&gt;contiv&lt;/a&gt;正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的&lt;a href=&#34;https://github.com/contiv/netplugin/blob/master/Vagrantfile&#34;&gt;Vagrantfile&lt;/a&gt;文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;➜  netplugin git:(master) ✗ vagrant status
Current machine states:

netplugin-node1           running (virtualbox)
netplugin-node2           running (virtualbox)
netplugin-node3           running (virtualbox)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Vagrant是&lt;a href=&#34;https://www.hashicorp.com/&#34;&gt;hashicorp&lt;/a&gt;这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有&lt;code&gt;Consul&lt;/code&gt;、&lt;code&gt;Vault&lt;/code&gt;、&lt;code&gt;Nomad&lt;/code&gt;、&lt;code&gt;Terraform&lt;/code&gt;。他们的产品都是基于&lt;strong&gt;Open Source&lt;/strong&gt;的&lt;a href=&#34;https://github.com/hashicorp&#34;&gt;Github地址&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;用途&#34;&gt;用途&lt;/h2&gt;

&lt;p&gt;Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是&lt;strong&gt;virtualbox&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Vagrant提供一个命令行工具&lt;code&gt;vagrant&lt;/code&gt;，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。&lt;/p&gt;

&lt;p&gt;跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用&lt;code&gt;vagrant init hashicorp/precise64&lt;/code&gt;就可以初始化一个Ubuntu 12.04的镜像。&lt;/p&gt;

&lt;h2 id=&#34;用法&#34;&gt;用法&lt;/h2&gt;

&lt;p&gt;你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vagrantfile&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。&lt;/p&gt;

&lt;p&gt;看个例子，选自&lt;a href=&#34;https://github.com/fenbox/Vagrantfile&#34;&gt;https://github.com/fenbox/Vagrantfile&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;# -*- mode: ruby -*-
# vi: set ft=ruby :

# All Vagrant configuration is done below. The &amp;quot;2&amp;quot; in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don&#39;t change it unless you know what
# you&#39;re doing.
Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Every Vagrant development environment requires a box. You can search for
  # boxes at https://atlas.hashicorp.com/search.
  config.vm.box = &amp;quot;ubuntu/trusty64&amp;quot;

  # Disable automatic box update checking. If you disable this, then
  # boxes will only be checked for updates when the user runs
  # `vagrant box outdated`. This is not recommended.
  # config.vm.box_check_update = false

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine. In the example below,
  # accessing &amp;quot;localhost:8080&amp;quot; will access port 80 on the guest machine.
  # config.vm.network &amp;quot;forwarded_port&amp;quot;, guest: 80, host: 8080

  # Create a private network, which allows host-only access to the machine
  # using a specific IP.
  config.vm.network &amp;quot;private_network&amp;quot;, ip: &amp;quot;192.168.33.10&amp;quot;

  # Create a public network, which generally matched to bridged network.
  # Bridged networks make the machine appear as another physical device on
  # your network.
  # config.vm.network &amp;quot;public_network&amp;quot;

  # Share an additional folder to the guest VM. The first argument is
  # the path on the host to the actual folder. The second argument is
  # the path on the guest to mount the folder. And the optional third
  # argument is a set of non-required options.
  # config.vm.synced_folder &amp;quot;../data&amp;quot;, &amp;quot;/vagrant_data&amp;quot;

  # Provider-specific configuration so you can fine-tune various
  # backing providers for Vagrant. These expose provider-specific options.
  # Example for VirtualBox:
  #
  # config.vm.provider &amp;quot;virtualbox&amp;quot; do |vb|
  #   # Display the VirtualBox GUI when booting the machine
  #   vb.gui = true
  #
  #   # Customize the amount of memory on the VM:
  #   vb.memory = &amp;quot;1024&amp;quot;
  # end
  #
  # View the documentation for the provider you are using for more
  # information on available options.

  # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
  # such as FTP and Heroku are also available. See the documentation at
  # https://docs.vagrantup.com/v2/push/atlas.html for more information.
  # config.push.define &amp;quot;atlas&amp;quot; do |push|
  #   push.app = &amp;quot;YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME&amp;quot;
  # end

  # Enable provisioning with a shell script. Additional provisioners such as
  # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the
  # documentation for more information about their specific syntax and use.
  # config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;lt;&amp;lt;-SHELL
  #   apt-get update
  #   apt-get install -y apache2
  # SHELL
  config.vm.provision :shell, path: &amp;quot;bootstrap.sh&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Boxes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vagrant的基础镜像，相当于docker images。可以在这些基础镜像的基础上制作自己的虚拟机镜像。&lt;/p&gt;

&lt;p&gt;添加一个box&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ vagrant box add hashicorp/precise64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Vagrantfile中指定box&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.box_version = &amp;quot;1.1.0&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;使用ssh进入vagrant&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vagrant up&lt;/code&gt;后就可以用&lt;code&gt;vagrant ssh $name&lt;/code&gt;进入虚拟机内，如果主机上就一个vagrant可以不指定名字。默认进入的用户是vagrant。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;文件同步&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vagrant up&lt;/code&gt;后在虚拟机中会有一个&lt;code&gt;/vagrant&lt;/code&gt;目录，这跟你定义&lt;code&gt;Vagrantfile&lt;/code&gt;是同一级目录。&lt;/p&gt;

&lt;p&gt;这个目录跟你宿主机上的目录文件是同步的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;软件安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在Vagrantfile中定义要安装的软件和操作。&lt;/p&gt;

&lt;p&gt;例如安装apache&lt;/p&gt;

&lt;p&gt;在与Vagrantfile同级的目录下创建一个&lt;code&gt;bootstrap.sh&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/usr/bin/env bash

apt-get update
apt-get install -y apache2
if ! [ -L /var/www ]; then
  rm -rf /var/www
  ln -fs /vagrant /var/www
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在Vagrantfile中使用它。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.box_version = &amp;quot;1.1.0&amp;quot;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;端口转发&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;quot;hashicorp/precise64&amp;quot;
  config.vm.provision :shell, path: &amp;quot;bootstrap.sh&amp;quot;
  config.vm.network :forwarded_port, guest: 80, host: 4567
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行&lt;code&gt;vagrant reload&lt;/code&gt;或者&lt;code&gt;vagrant up&lt;/code&gt;可以生效。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;分享&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;你自己做的vagrant是可以分享给别人的用的，只要你有一个hashicorp账号，&lt;code&gt;vagrant login&lt;/code&gt;后就可以执行&lt;code&gt;vagrant share&lt;/code&gt;分享，会生成一个URL，其它人也可以访问到你的vagrant里的服务。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中止&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vagrant suspend&lt;/li&gt;
&lt;li&gt;Vagrant halt&lt;/li&gt;
&lt;li&gt;Vagrant destroy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;重构&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;再次执行&lt;code&gt;vagrant up&lt;/code&gt;即可。&lt;/p&gt;

&lt;h2 id=&#34;分布式环境&#34;&gt;分布式环境&lt;/h2&gt;

&lt;p&gt;开发分布式环境下的应用时往往需要多个虚拟机用于测试，这时候才是vagrant显威力的时候。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义多个主机&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.provision &amp;quot;shell&amp;quot;, inline: &amp;quot;echo Hello&amp;quot;

  config.vm.define &amp;quot;web&amp;quot; do |web|
    web.vm.box = &amp;quot;apache&amp;quot;
  end

  config.vm.define &amp;quot;db&amp;quot; do |db|
    db.vm.box = &amp;quot;mysql&amp;quot;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个比较复杂，详见&lt;a href=&#34;https://www.vagrantup.com/docs/multi-machine/&#34;&gt;https://www.vagrantup.com/docs/multi-machine/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有一些其它功能，如push、plugins、providers按下不表。&lt;/p&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;总的来说说Vagrant没有Docker好用，但是对于协同开发，用它来定义分布式开发环境还可以，ruby的语法看着有点不习惯，好在也不复杂，如果是团队几个人开发，弄几个虚拟机大家互相拷贝一下也没那么复杂吧？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker源码编译和开发环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/docker-dev-env/</link>
      <pubDate>Mon, 06 Mar 2017 17:03:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-dev-env/</guid>
      <description>

&lt;p&gt;看了下网上其他人写的docker开发环境搭建，要么是在ubuntu下搭建，要么就是使用官方说明的build docker-dev镜像的方式一步步搭建的，甚是繁琐，docker hub上有一个docker官方推出的&lt;strong&gt;dockercore/docker&lt;/strong&gt;镜像，其实这就是官网上所说的docker-dev镜像，不过以前的那个deprecated了，使用目前这个镜像搭建docker开发环境是最快捷的了。&lt;/p&gt;

&lt;p&gt;想要修改docker源码和做docker定制开发的同学可以参考下。&lt;/p&gt;

&lt;p&gt;官方指导文档：&lt;a href=&#34;https://docs.docker.com/opensource/code/&#34;&gt;https://docs.docker.com/opensource/code/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;设置docker开发环境：&lt;a href=&#34;https://docs.docker.com/opensource/project/set-up-dev-env/&#34;&gt;https://docs.docker.com/opensource/project/set-up-dev-env/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;docker的编译实质上是在docker容器中运行docker。&lt;/p&gt;

&lt;p&gt;因此在本地编译docker的前提是需要安装了docker，还需要用git把代码pull下来。&lt;/p&gt;

&lt;h3 id=&#34;创建分支&#34;&gt;创建分支&lt;/h3&gt;

&lt;p&gt;为了方便以后给docker提交更改，我们从docker官方fork一个分支。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/rootsongjc/docker.git
git config --local user.name &amp;quot;Jimmy Song&amp;quot;
git config --local user.email &amp;quot;rootsongjc@gmail.com&amp;quot;
git remote add upstream https://github.com/docker/docker.git
git config --local -l
git remote -v
git checkout -b dry-run-test
touch TEST.md
vim TEST.md
git status
git add TEST.md
git commit -am &amp;quot;Making a dry run test.&amp;quot;
git push --set-upstream origin dry-run-test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后就可以在&lt;code&gt;dry-run-test&lt;/code&gt;这个分支下工作了。&lt;/p&gt;

&lt;h3 id=&#34;配置docker开发环境&#34;&gt;配置docker开发环境&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/opensource/project/set-up-dev-env/&#34;&gt;官网&lt;/a&gt;上说需要先清空自己电脑上已有的容器和镜像。&lt;/p&gt;

&lt;p&gt;docker开发环境本质上是创建一个docker镜像，镜像里包含了docker的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;dry-run-test&lt;/code&gt;分支下执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;make BIND_DIR=. shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令会自动编译一个docker镜像，From debian:jessie。这一步会上网下载很多依赖包，速度比较慢。如果翻不了墙的话肯定都会失败。因为需要下载的软件和安装包都是在国外服务器上，不翻墙根本就下载不下来，为了不用这么麻烦，推荐直接使用docker官方的dockercore/docker镜像，也不用以前的docker-dev镜像，那个造就废弃了。这个镜像大小有2.31G。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker pull dockercore/docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用方法见这里：&lt;a href=&#34;https://hub.docker.com/r/dockercore/docker/&#34;&gt;https://hub.docker.com/r/dockercore/docker/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后就可以进入到容器里&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run --rm -i --privileged -e BUILDFLAGS -e KEEPBUNDLE -e DOCKER_BUILD_GOGC -e DOCKER_BUILD_PKGS -e DOCKER_CLIENTONLY -e DOCKER_DEBUG -e DOCKER_EXPERIMENTAL -e DOCKER_GITCOMMIT -e DOCKER_GRAPHDRIVER=devicemapper -e DOCKER_INCREMENTAL_BINARY -e DOCKER_REMAP_ROOT -e DOCKER_STORAGE_OPTS -e DOCKER_USERLANDPROXY -e TESTDIRS -e TESTFLAGS -e TIMEOUT -v &amp;quot;/Users/jimmy/Workspace/github/rootsongjc/docker/bundles:/go/src/github.com/docker/docker/bundles&amp;quot; -t &amp;quot;dockercore/docker:latest&amp;quot; bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;按照官网的说明make会报错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker# ./hack/make.sh binary                          

error: .git directory missing and DOCKER_GITCOMMIT not specified
  Please either build with the .git directory accessible, or specify the
  exact (--short) commit hash you are building using DOCKER_GITCOMMIT for
  future accountability in diagnosing build issues.  Thanks!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是一个&lt;a href=&#34;https://github.com/docker/docker/issues/27581&#34;&gt;issue-27581&lt;/a&gt;，解决方式就是在make的时候手动指定&lt;code&gt;DOCKER_GITCOMMIT&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker# DOCKER_GITCOMMIT=3385658 ./hack/make.sh binary

---&amp;gt; Making bundle: binary (in bundles/17.04.0-dev/binary)
Building: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev
Created binary: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev
Building: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev
Created binary: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev
Copying nested executables into bundles/17.04.0-dev/binary-daemon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bundles目录下会生成如下文件结构&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── 17.04.0-dev
│   ├── binary-client
│   │   ├── docker -&amp;gt; docker-17.04.0-dev
│   │   ├── docker-17.04.0-dev
│   │   ├── docker-17.04.0-dev.md5
│   │   └── docker-17.04.0-dev.sha256
│   └── binary-daemon
│       ├── docker-containerd
│       ├── docker-containerd-ctr
│       ├── docker-containerd-ctr.md5
│       ├── docker-containerd-ctr.sha256
│       ├── docker-containerd-shim
│       ├── docker-containerd-shim.md5
│       ├── docker-containerd-shim.sha256
│       ├── docker-containerd.md5
│       ├── docker-containerd.sha256
│       ├── docker-init
│       ├── docker-init.md5
│       ├── docker-init.sha256
│       ├── docker-proxy
│       ├── docker-proxy.md5
│       ├── docker-proxy.sha256
│       ├── docker-runc
│       ├── docker-runc.md5
│       ├── docker-runc.sha256
│       ├── dockerd -&amp;gt; dockerd-17.04.0-dev
│       ├── dockerd-17.04.0-dev
│       ├── dockerd-17.04.0-dev.md5
│       └── dockerd-17.04.0-dev.sha256
└── latest -&amp;gt; 17.04.0-dev

4 directories, 26 files
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以将docker-daemon和docker-client目录下的docker可以执行文件复制到容器的/usr/bin/目录下了。&lt;/p&gt;

&lt;p&gt;启动docker deamon&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker daemon -D&amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查下docker是否可用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@f2753f78bb6d:/go/src/github.com/docker/docker/bundles/17.04.0-dev# docker version
DEBU[0048] Calling GET /_ping                           
DEBU[0048] Calling GET /v1.27/version                   
Client:
 Version:      17.04.0-dev
 API version:  1.27
 Go version:   go1.7.5
 Git commit:   3385658
 Built:        Mon Mar  6 08:39:06 2017
 OS/Arch:      linux/amd64

Server:
 Version:      17.04.0-dev
 API version:  1.27 (minimum version 1.12)
 Go version:   go1.7.5
 Git commit:   3385658
 Built:        Mon Mar  6 08:39:06 2017
 OS/Arch:      linux/amd64
 Experimental: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到此docker源码编译和开发环境都已经搭建好了。&lt;/p&gt;

&lt;p&gt;如果想要修改docker源码，只要在你的IDE、容器里或者你本机上修改docker代码后，再执行上面的hack/make.sh binary命令就可以生成新的docker二进制文件，再替换原来的/usr/bin/目录下的docker二进制文件即可。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践》读后感</title>
      <link>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</link>
      <pubDate>Wed, 01 Mar 2017 18:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</guid>
      <description>&lt;p&gt;最近友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，&lt;a href=&#34;http://olz1di9xf.bkt.clouddn.com/docs/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E8%B7%B5%E7%AC%AC2%E7%89%88.pdf&#34;&gt;点这里下载&lt;/a&gt;，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。&lt;/p&gt;

&lt;p&gt;后续我将会陆续分享这本书的读后感。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>12 factor app</title>
      <link>http://rootsongjc.github.io/blogs/12-factor-app/</link>
      <pubDate>Mon, 27 Feb 2017 22:32:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/12-factor-app/</guid>
      <description>

&lt;h1 id=&#34;twelve-factor-app&#34;&gt;Twelve-factor App&lt;/h1&gt;

&lt;h1 id=&#34;简介&#34;&gt;简介&lt;/h1&gt;

&lt;p&gt;如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建如下的 SaaS 应用提供了方法论：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用&lt;strong&gt;标准化&lt;/strong&gt;流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。&lt;/li&gt;
&lt;li&gt;和操作系统之间尽可能的&lt;strong&gt;划清界限&lt;/strong&gt;，在各个系统中提供&lt;strong&gt;最大的可移植性&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;适合&lt;strong&gt;部署&lt;/strong&gt;在现代的&lt;strong&gt;云计算平台&lt;/strong&gt;，从而在服务器和系统管理方面节省资源。&lt;/li&gt;
&lt;li&gt;将开发环境和生产环境的&lt;strong&gt;差异降至最低&lt;/strong&gt;，并使用&lt;strong&gt;持续交付&lt;/strong&gt;实施敏捷开发。&lt;/li&gt;
&lt;li&gt;可以在工具、架构和开发流程不发生明显变化的前提下实现&lt;strong&gt;扩展&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。&lt;/p&gt;

&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;

&lt;p&gt;本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过 &lt;a href=&#34;http://www.heroku.com/&#34;&gt;Heroku&lt;/a&gt; 平台间接见证了数十万应用程序的开发，运作以及扩展的过程。&lt;/p&gt;

&lt;p&gt;本文综合了我们关于 SaaS 应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何 &lt;a href=&#34;http://blog.heroku.com/archives/2011/6/28/the_new_heroku_4_erosion_resistance_explicit_contracts/&#34;&gt;避免软件污染&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于 Martin Fowler 的书籍： *Patterns of Enterprise Application Architecture* ， *Refactoring* 。&lt;/p&gt;

&lt;h1 id=&#34;12-factors&#34;&gt;12-factors&lt;/h1&gt;

&lt;h2 id=&#34;i-基准代码-https-12factor-net-zh-cn-codebase&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/codebase&#34;&gt;I. 基准代码&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;一份基准代码-多份部署&#34;&gt;一份基准代码，多份部署&lt;/h3&gt;

&lt;h2 id=&#34;ii-依赖-https-12factor-net-zh-cn-dependencies&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/dependencies&#34;&gt;II. 依赖&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;显式声明依赖关系&#34;&gt;显式声明依赖关系&lt;/h3&gt;

&lt;h2 id=&#34;iii-配置-https-12factor-net-zh-cn-config&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/config&#34;&gt;III. 配置&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;在环境中存储配置&#34;&gt;在环境中存储配置&lt;/h3&gt;

&lt;h2 id=&#34;iv-后端服务-https-12factor-net-zh-cn-backing-services&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/backing-services&#34;&gt;IV. 后端服务&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;把后端服务当作附加资源&#34;&gt;把后端服务当作附加资源&lt;/h3&gt;

&lt;h2 id=&#34;v-构建-发布-运行-https-12factor-net-zh-cn-build-release-run&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/build-release-run&#34;&gt;V. 构建，发布，运行&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;严格分离构建和运行&#34;&gt;严格分离构建和运行&lt;/h3&gt;

&lt;h2 id=&#34;vi-进程-https-12factor-net-zh-cn-processes&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/processes&#34;&gt;VI. 进程&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;以一个或多个无状态进程运行应用&#34;&gt;以一个或多个无状态进程运行应用&lt;/h3&gt;

&lt;h2 id=&#34;vii-端口绑定-https-12factor-net-zh-cn-port-binding&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/port-binding&#34;&gt;VII. 端口绑定&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;通过端口绑定提供服务&#34;&gt;通过端口绑定提供服务&lt;/h3&gt;

&lt;h2 id=&#34;viii-并发-https-12factor-net-zh-cn-concurrency&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/concurrency&#34;&gt;VIII. 并发&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;通过进程模型进行扩展&#34;&gt;通过进程模型进行扩展&lt;/h3&gt;

&lt;h2 id=&#34;ix-易处理-https-12factor-net-zh-cn-disposability&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/disposability&#34;&gt;IX. 易处理&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;快速启动和优雅终止可最大化健壮性&#34;&gt;快速启动和优雅终止可最大化健壮性&lt;/h3&gt;

&lt;h2 id=&#34;x-开发环境与线上环境等价-https-12factor-net-zh-cn-dev-prod-parity&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/dev-prod-parity&#34;&gt;X. 开发环境与线上环境等价&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;尽可能的保持开发-预发布-线上环境相同&#34;&gt;尽可能的保持开发，预发布，线上环境相同&lt;/h3&gt;

&lt;h2 id=&#34;xi-日志-https-12factor-net-zh-cn-logs&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/logs&#34;&gt;XI. 日志&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;把日志当作事件流&#34;&gt;把日志当作事件流&lt;/h3&gt;

&lt;h2 id=&#34;xii-管理进程-https-12factor-net-zh-cn-admin-processes&#34;&gt;&lt;a href=&#34;https://12factor.net/zh_cn/admin-processes&#34;&gt;XII. 管理进程&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&#34;后台管理任务当作一次性进程运行&#34;&gt;后台管理任务当作一次性进程运行&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>docker service discovery</title>
      <link>http://rootsongjc.github.io/blogs/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-service-discovery/</guid>
      <description>&lt;p&gt;Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of &lt;a href=&#34;https://docs.docker.com/v1.11/swarm/discovery/&#34;&gt;service discovery backend&lt;/a&gt;. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34; alt=&#34;pic-intro&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to Docker 1.12 Swarm Mode&lt;/strong&gt;, we don’t have to depend upon these external tools and complex configurations. &lt;a href=&#34;https://github.com/docker/docker/releases/tag/v1.12.0-rc5&#34;&gt;Docker Engine 1.12&lt;/a&gt; runs it’s own internal DNS service to route services by name.Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it help?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you create a service and provide a name for it, you can use just that name as a target hostname, and it’s going to be automatically resolved to the proper container IP of the service. In short, within the swarm, containers can simply reference other services via their names and the built-in DNS will be used to find the appropriate IP and port automatically. It is important to note that if the service has multiple replicas, &lt;strong&gt;the requests would be round-robin load-balanced&lt;/strong&gt;. This would still work if you didn’t forward any ports when you created your docker services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34; alt=&#34;Pic10&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Embedded DNS is not a new concept. It was first included under Docker 1.10 release. Please note that DNS lookup for containers connected to user-defined networks works differently compared to the containers connected to &lt;code&gt;default bridge&lt;/code&gt; network. As of Docker 1.10, the docker daemon implements an embedded DNS server which provides built-in service discovery for any container created with a valid &lt;code&gt;name&lt;/code&gt; or &lt;code&gt;net-alias&lt;/code&gt; or aliased by &lt;code&gt;link&lt;/code&gt;. Moreover,container name configured using &lt;code&gt;--name&lt;/code&gt; is used to discover a container within an user-defined docker network. The embedded DNS server maintains the mapping between the container name and its IP address (on the network the container is connected to).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does Embedded DNS resolve unqualified names?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34; alt=&#34;Pic22&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With Docker 1.12 release, a new API called “service” is being included which clearly talks about the functionality of service discovery.  It is important to note that Service discovery is scoped within the network. What it really means is –  If you have redis application and web client as two separate services , you combine into single application and put them into same network.If you try build your application in such a way that you are trying to reach to redis through name “redis”,it will always resolve to name “redis”. Reason – both of these services are part of the same network. You don’t need to be inside the application trying to resolve this service using FQDN. Reason – FQDN name is not going to be portable which in turn, makes your application non-portable.&lt;/p&gt;

&lt;p&gt;Internally, there is a listener opened inside the container itself. If we try to enter into the container which is providing a service discovery and look at /etc/resolv.conf, we will find that the nameserver entry holds something really different like 127.0.0.11.This is nothing but a loopback address. So, whenever resolver tried to resolve, it will resolve to 127.0.0.11 and this request is rightly trapped.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34; alt=&#34;Pic-12&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this request is trapped, it is sent to particular random UDP / TCP port currently being listened under the docker daemon. Consequently, the socket is to be created inside the namespace. When DNS server and daemon gets the request, it knows that this is coming from which specific network, hence gets aware of  the context of from where it is coming from.Once it knows the context, it can generate the appropriate DNS response.&lt;/p&gt;

&lt;p&gt;To demonstrate Service Discovery  under Docker 1.12, I have upgraded Docker 1.12.rc5 to 1.12.0 GA version. The swarm cluster look like:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34; alt=&#34;Pico01&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have created a network called “collabnet” for the new services as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34; alt=&#34;Pic-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create a service called “wordpressdb” under collabnet network :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34; alt=&#34;pico-mysql&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can list the running tasks(containers) and the node on which these containers are running on:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34; alt=&#34;Pic-4&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create another service called “wordpressapp” under the same network:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34; alt=&#34;pico-app&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, we can list out the number of services running on our swarm cluster as shown below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34; alt=&#34;pico-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have scaled out the number of wordpressapp and wordpressdb just for demonstration purpose.&lt;/p&gt;

&lt;p&gt;Let’s consider my master node where I have two of the containers running as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34; alt=&#34;Pico-1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can reach out one service(wordpressapp) from another service(wordpressapp) through just service-name as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34; alt=&#34;pico-last&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, I can reach out to particular container by its name from other container running different service but on the same network. As shown below, I can reach out to wordpressapp.3.6f8bthp container via wordpressdb.7.e62jl57qqu running wordpressdb.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34; alt=&#34;pico-tasktoo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The below picture depicts the Service Discovery in a nutshell:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic23.png&#34; alt=&#34;Pic23&#34; /&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every service has Virtual IP(VIP) associated which can be derived as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34; alt=&#34;pic-list&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As shown above, each service has an IP address and this IP address maps to multiple container IP address associated with that service. It is important to note that service IP associated with a service does not change even though containers associated with the service dies/ restarts.&lt;/p&gt;

&lt;p&gt;Few important points to remember:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VIP based services use Linux IPVS load balancing to route to the backend containers. This works only for TCP/UDP protocols. When you use DNS-RR mode services don’t have a VIP allocated. Instead service names resolves to one of the backend container IPs randomly.&lt;/li&gt;
&lt;li&gt;Ping not working for VIP is as designed. Technically, IPVS is a TCP/UDP load-balancer, while ping uses ICMP and hence IPVS is not going to load-balance the ping request.&lt;/li&gt;
&lt;li&gt;For VIP based services the reason ping works on the local node is because the VIP is added a 2nd IP address on the overlay network interface&lt;/li&gt;
&lt;li&gt;You can any of the tools like  dig, nslookup or wget -O- &lt;service name&gt; to demonstrate the service discovery functionality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below picture depicts that the network is the scope of service discoverability which means that when you have a service running on one network , it is scoped to that network and won’t be able to reach out to different service running on different network(unless it is part of that network).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34; alt=&#34;SD&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s dig little further introducing Load-balancing aspect too. To see what is basically enabling the load-balancing functionality, we can go into sandbox of each containers and see how it has been resolved.&lt;/p&gt;

&lt;p&gt;Let’s pick up the two containers running on the master node. We can see the sandbox running through the following command:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34; alt=&#34;pico-namespace&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Under /var/run/docker/netns, you will find various namespaces. The namespaces marked with x-{id} represents network namespace managed by the overlay network driver for its operation (such as creating a bridge, terminating vxlan tunnel, etc…). They don’t represent the container network namespace. Since it is managed by the driver, it is not recommended to manipulate anything within this namespace. But if you are curious on the deep dive, then you can use the “nsenter” tool to understand more about this internal namespace.&lt;/p&gt;

&lt;p&gt;We can enter into sandbox through the nsenter utility:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34; alt=&#34;pico-mangle&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case you faced an error stating “nsenter: reassociate to namespace ‘ns/net’ failed: Invalid argument”, I suggest to look at &lt;a href=&#34;http://tinyurl.com/gu5rsw9&#34;&gt;this&lt;/a&gt; workaround.&lt;/p&gt;

&lt;p&gt;10.0.3.4 service IP is marked 0x108 using iptables OUTPUT chain. ipvs uses this marking and load balances it to containers 10.0.3.5 and 10.0.3.6 as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34; alt=&#34;ipvs&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are key takeaways from this entire post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34; alt=&#34;Pic34&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&#34;http://collabnix.com/archives/1504&#34;&gt;http://collabnix.com/archives/1504&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker embedded dns</title>
      <link>http://rootsongjc.github.io/blogs/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-embedded-dns/</guid>
      <description>

&lt;p&gt;本文主要介绍了&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Docker&lt;/a&gt;容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。&lt;/p&gt;

&lt;h2 id=&#34;configure-container-dns&#34;&gt;Configure container DNS&lt;/h2&gt;

&lt;h3 id=&#34;dns-in-default-bridge-network&#34;&gt;DNS in default bridge network&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-h HOSTNAME or –hostname=HOSTNAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME or ID:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=IP_ADDRESS…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;添加到容器内的/etc/resolv.conf中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：

&lt;ul&gt;
&lt;li&gt;如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.conf内容更新容器内的/etc/resolv.conf.&lt;/li&gt;
&lt;li&gt;如果容器状态为running，则容器内的/etc/resolv.conf将不会改变，直到该容器状态变为stopped.&lt;/li&gt;
&lt;li&gt;如果容器启动后修改过容器内的/etc/resolv.conf，则不会对该容器进行处理，否则可能会丢失已经完成的修改，无论该容器为什么状态。 
如果容器启动时，用了–dns, –dns-search, or –dns-opt选项，其启动时已经修改了宿主机的/etc/resolv.conf过滤后的内容，因此docker daemon永远不会更新这种容器的/etc/resolv.conf。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;: docker daemon监控宿主机/etc/resolv.conf的这个file change notifier的实现是依赖linux内核的inotify特性，而inotfy特性不兼容overlay fs，因此使用overlay fs driver的docker deamon将无法使用该/etc/resolv.conf自动更新的功能。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;embedded-dns-in-user-defined-networks&#34;&gt;Embedded DNS in user-defined networks&lt;/h3&gt;

&lt;p&gt;在docker 1.10版本中，docker daemon实现了一个叫做&lt;code&gt;embedded DNS server&lt;/code&gt;的东西，用来当你创建的容器满足以下条件时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用自定义网络；&lt;/li&gt;
&lt;li&gt;容器创建时候通过&lt;code&gt;--name&lt;/code&gt;,&lt;code&gt;--network-alias&lt;/code&gt; or &lt;code&gt;--link&lt;/code&gt;提供了一个name；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;docker daemon就会利用embedded DNS server对整个自定义网络中所有容器进行名字解析（你可以理解为一个网络中的一种服务发现）。&lt;/p&gt;

&lt;p&gt;因此当你启动容器时候满足以上条件时，该容器的域名解析就不应该去考虑容器内的/etc/hosts, /etc/resolv.conf，应该保持其不变，甚至为空，将需要解析的域名都配置到对应embedded DNS server中。具体配置参数及说明如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;–name=CONTAINER-NAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将CONTAINER-NAME和该容器的IP配置到该容器连接到的自定义网络中的embedded DNS server中，由它提供该自定义网络范围内的域名解析&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–network-alias=ALIAS&lt;/td&gt;
&lt;td&gt;将容器的name-ip map配置到容器连接到的其他网络的embedded DNS server中。PS：一个容器可能连接到多个网络中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP配置到该容器连接到的自定义网络中的embedded DNS server中，但仅限于配置了该link的容器能解析这条rule。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=[IP_ADDRESS…]&lt;/td&gt;
&lt;td&gt;当embedded DNS server无法解析该容器的某个dns query时，会将请求foward到这些–dns配置的IP_ADDRESS DNS Server，由它们进一步进行域名解析。注意，这些–dns配置到&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;全部由对应的embedded DNS server管理，并不会更新到容器内的/etc/resolv.conf.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-search配置的DOMAIN们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-opt配置的OPTION们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;注意容器内/etc/resolv.conf中配置的DNS server，只有当the embedded DNS server无法解析某个name时，才会用到。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;embedded-dns-server源码分析&#34;&gt;embedded DNS server源码分析&lt;/h2&gt;

&lt;p&gt;所有embedded DNS server相关的代码都在libcontainer项目中，几个最主要的文件分别是&lt;code&gt;/libnetwork/resolver.Go&lt;/code&gt;,&lt;code&gt;/libnetwork/resolver_unix.go&lt;/code&gt;,&lt;code&gt;sandbox_dns_unix.go&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;OK, 先来看看embedded DNS server对象在docker中的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go

// resolver implements the Resolver interface
type resolver struct {
    sb         *sandbox
    extDNSList [maxExtDNS]extDNSEntry
    server     *dns.Server
    conn       *net.UDPConn
    tcpServer  *dns.Server
    tcpListen  *net.TCPListener
    err        error
    count      int32
    tStamp     time.Time
    queryLock  sync.Mutex
}

// Resolver represents the embedded DNS server in Docker. It operates
// by listening on container&#39;s loopback interface for DNS queries.
type Resolver interface {
    // Start starts the name server for the container
    Start() error
    // Stop stops the name server for the container. Stopped resolver
    // can be reused after running the SetupFunc again.
    Stop()
    // SetupFunc() provides the setup function that should be run
    // in the container&#39;s network namespace.
    SetupFunc() func()
    // NameServer() returns the IP of the DNS resolver for the
    // containers.
    NameServer() string
    // SetExtServers configures the external nameservers the resolver
    // should use to forward queries
    SetExtServers([]string)
    // ResolverOptions returns resolv.conf options that should be set
    ResolverOptions() []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，resolver就是embedded DNS server，每个resolver都bind一个sandbox，并定义了一个对应的dns.Server，还定义了外部DNS对象列表，但embedded DNS server无法解析某个name时，就会forward到那些外部DNS。&lt;/p&gt;

&lt;p&gt;Resolver Interface定义了embedded DNS server必须实现的接口，这里会重点关注SetupFunc()和Start()，见下文分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;dns.Server的实现，全部交给github.com/miekg/dns，限于篇幅，这里我将不会跟进去分析。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从整个&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Container&lt;/a&gt; create的流程上来看，docker daemon对embedded DNS server的处理是从endpoint Join a sandbox开始的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libnetwork/endpoint.go


func (ep *endpoint) Join(sbox Sandbox, options ...EndpointOption) error {
    ...

    return ep.sbJoin(sb, options...)
}


func (ep *endpoint) sbJoin(sb *sandbox, options ...EndpointOption) error {
    ...

    if err = sb.populateNetworkResources(ep); err != nil {
        return err
    }

    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox join a sandbox的流程中，会调用sandbox. populateNetworkResources做网络资源的设置，这其中就包括了embedded DNS server的启动。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox.go
func (sb *sandbox) populateNetworkResources(ep *endpoint) error {
    ...
    if ep.needResolver() {
        sb.startResolver(false)
    }
    ...
}


libnetwork/sandbox_dns_unix.go
func (sb *sandbox) startResolver(restore bool) {
    sb.resolverOnce.Do(func() {
        var err error
        sb.resolver = NewResolver(sb)
        defer func() {
            if err != nil {
                sb.resolver = nil
            }
        }()

        // In the case of live restore container is already running with
        // right resolv.conf contents created before. Just update the
        // external DNS servers from the restored sandbox for embedded
        // server to use.
        if !restore {
            err = sb.rebuildDNS()
            if err != nil {
                log.Errorf(&amp;quot;Updating resolv.conf failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
                return
            }
        }
        sb.resolver.SetExtServers(sb.extDNS)

        sb.osSbox.InvokeFunc(sb.resolver.SetupFunc())
        if err = sb.resolver.Start(); err != nil {
            log.Errorf(&amp;quot;Resolver Setup/Start failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox.startResolver是流程关键:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过sanbdox.rebuildDNS生成了container内的/etc/resolv.conf&lt;/li&gt;
&lt;li&gt;通过resolver.SetExtServers(sb.extDNS)设置embedded DNS server的forward DNS list&lt;/li&gt;
&lt;li&gt;通过resolver.SetupFunc()启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener&lt;/li&gt;
&lt;li&gt;通过resolver.Start()对容器内的iptable进行设置(见下)，并通过miekg/dns启动一个nameserver在53端口提供服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我将逐一介绍上面的各个步骤。&lt;/p&gt;

&lt;h3 id=&#34;sanbdox-rebuilddns&#34;&gt;sanbdox.rebuildDNS&lt;/h3&gt;

&lt;p&gt;sanbdox.rebuildDNS负责构建容器内的resolv.conf，构建规则就是第一节江参数配置时候提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save the external name servers in resolv.conf in the sandbox&lt;/li&gt;
&lt;li&gt;Add only the embedded server’s IP to container’s resolv.conf&lt;/li&gt;
&lt;li&gt;If the embedded server needs any resolv.conf options add it to the current list&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox_dns_unix.go

func (sb *sandbox) rebuildDNS() error {
    currRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)
    if err != nil {
        return err
    }

    // localhost entries have already been filtered out from the list
    // retain only the v4 servers in sb for forwarding the DNS queries
    sb.extDNS = resolvconf.GetNameservers(currRC.Content, types.IPv4)

    var (
        dnsList        = []string{sb.resolver.NameServer()}
        dnsOptionsList = resolvconf.GetOptions(currRC.Content)
        dnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)
    )

    dnsList = append(dnsList, resolvconf.GetNameservers(currRC.Content, types.IPv6)...)

    resOptions := sb.resolver.ResolverOptions()

dnsOpt:
    ...
    dnsOptionsList = append(dnsOptionsList, resOptions...)

    _, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setextservers&#34;&gt;resolver.SetExtServers&lt;/h3&gt;

&lt;p&gt;设置embedded DNS server的forward DNS list, 当embedded DNS server不能解析某name时，就会将请求forward到ExtServers。代码很简单，不多废话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go
func (r *resolver) SetExtServers(dns []string) {
    l := len(dns)
    if l &amp;gt; maxExtDNS {
        l = maxExtDNS
    }
    for i := 0; i &amp;lt; l; i++ {
        r.extDNSList[i].ipStr = dns[i]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setupfunc&#34;&gt;resolver.SetupFunc&lt;/h3&gt;

&lt;p&gt;启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;libnetwork/resolver.go

func (r *resolver) SetupFunc() func() {
    return (func() {
        var err error

        // DNS operates primarily on UDP
        addr := &amp;amp;net.UDPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.conn, err = net.ListenUDP(&amp;quot;udp&amp;quot;, addr)
        ...

        // Listen on a TCP as well
        tcpaddr := &amp;amp;net.TCPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.tcpListen, err = net.ListenTCP(&amp;quot;tcp&amp;quot;, tcpaddr)
        ...
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-start&#34;&gt;resolver.Start&lt;/h3&gt;

&lt;p&gt;resolver.Start中两个重要步骤，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;setupIPTable设置容器内的iptables&lt;/li&gt;
&lt;li&gt;启动dns nameserver在53端口开始提供域名解析服务&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;func (r *resolver) Start() error {
    ...
    if err := r.setupIPTable(); err != nil {
        return fmt.Errorf(&amp;quot;setting up IP table rules failed: %v&amp;quot;, err)
    }
    ...
    tcpServer := &amp;amp;dns.Server{Handler: r, Listener: r.tcpListen}
    r.tcpServer = tcpServer
    go func() {
        tcpServer.ActivateAndServe()
    }()
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先来看看怎么设置容器内的iptables的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (r *resolver) setupIPTable() error {
    ...
    // 获取setupFunc()时的两个本地随机监听端口
    laddr := r.conn.LocalAddr().String()
    ltcpaddr := r.tcpListen.Addr().String()

    cmd := &amp;amp;exec.Cmd{
        Path:   reexec.Self(),
        // 将这两个端口传给setup-resolver命令并启动执行
        Args:   append([]string{&amp;quot;setup-resolver&amp;quot;}, r.sb.Key(), laddr, ltcpaddr),
        Stdout: os.Stdout,
        Stderr: os.Stderr,
    }
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(&amp;quot;reexec failed: %v&amp;quot;, err)
    }
    return nil
}

// init时就注册setup-resolver对应的handler
func init() {
    reexec.Register(&amp;quot;setup-resolver&amp;quot;, reexecSetupResolver)
}

// setup-resolver对应的handler定义
func reexecSetupResolver() {
    ...
    // 封装iptables数据
    _, ipPort, _ := net.SplitHostPort(os.Args[2])
    _, tcpPort, _ := net.SplitHostPort(os.Args[3])
    rules := [][]string{
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[2]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--sport&amp;quot;, ipPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[3]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--sport&amp;quot;, tcpPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
    }
    ...

    // insert outputChain and postroutingchain
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在reexecSetupResolver()中清楚的定义了iptables添加outputChain 和postroutingchain，将到容器内的dns query请求重定向到embedded DNS server(127.0.0.11)上的udp/tcp两个随机可用端口，embedded DNS server(127.0.0.11)的返回数据则重定向到容器内的53端口，这样完成了整个dns query请求。&lt;/p&gt;

&lt;p&gt;模型图如下： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215440792?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;贴一张实例图： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215310369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20170105215322635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到这里，关于embedded DNS server的源码分析就结束了。当然，其中还有很多细节，就留给读者自己走读代码了。&lt;/p&gt;

&lt;h2 id=&#34;福利&#34;&gt;福利&lt;/h2&gt;

&lt;p&gt;从该时序图中看看embedded DNS server的操作在整个容器create流程中的位置。
&lt;img src=&#34;http://img.blog.csdn.net/20170105215401307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>raft consensus algorithm</title>
      <link>http://rootsongjc.github.io/blogs/raft/</link>
      <pubDate>Mon, 27 Feb 2017 10:47:14 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/raft/</guid>
      <description>&lt;p&gt;A wonderful raft consensus algorithm illustation
&lt;a href=&#34;http://thesecretlivesofdata.com/raft/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hello world</title>
      <link>http://rootsongjc.github.io/blogs/helloworld/</link>
      <pubDate>Mon, 20 Feb 2017 22:28:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/helloworld/</guid>
      <description>

&lt;h1 id=&#34;hello-world&#34;&gt;Hello world&lt;/h1&gt;

&lt;p&gt;This is my first post!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>