<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Network on Jimmy&#39;s blog</title>
    <link>http://rootsongjc.github.io/tags/network/index.xml</link>
    <description>Recent content in Network on Jimmy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://rootsongjc.github.io/tags/network/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Contiv人坑指南-试用全记录</title>
      <link>http://rootsongjc.github.io/post/contiv_tryout/</link>
      <pubDate>Thu, 09 Mar 2017 14:23:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/contiv_tryout/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017013129.jpg&#34; alt=&#34;黄昏&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北纬37度黄海之滨风力发电场，冬天的大风持续给人类提供清洁的能源）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;关于contiv的介绍请看我的上一篇文章&lt;a href=&#34;http://rootsongjc.github.io/post/contiv_guide/&#34;&gt;Contiv Intro&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;开发环境使用&lt;strong&gt;Vagrant&lt;/strong&gt;搭建，昨天试用了下，真不知道它们是怎么想的，即然是docker插件为啥不直接在docker中开发呢，我的这篇文章介绍如何搭建docker开发环境，&lt;a href=&#34;http://rootsongjc.github.io/post/docker-dev-env/，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像即可`dockercore/docker:latest`，不过有点大2.31G。&#34;&gt;http://rootsongjc.github.io/post/docker-dev-env/，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像即可`dockercore/docker:latest`，不过有点大2.31G。&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contiv概念解析&#34;&gt;Contiv概念解析&lt;/h3&gt;

&lt;p&gt;Contiv用于给容器创建和分配网路，可以创建策略管理容器的安全、带宽、优先级等，相当于一个SDN。&lt;/p&gt;

&lt;h4 id=&#34;group&#34;&gt;Group&lt;/h4&gt;

&lt;p&gt;按容器或Pod的功能给容器分配策略组，通常是按照容器/Pod的&lt;code&gt;label&lt;/code&gt;来分组，应用组跟contiv的network不是一一对应的，可以很多应用组属于同一个network或IP subnet。&lt;/p&gt;

&lt;h4 id=&#34;polices&#34;&gt;Polices&lt;/h4&gt;

&lt;p&gt;用来限定group的行为，contiv支持两种类型的policy：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bandwidth 限定应用组的资源使用上限&lt;/li&gt;
&lt;li&gt;Isolation 资源组的访问权限&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Group可以同时应用一个或多个policy，当有容器调度到该group里就会适用该group的policy。&lt;/p&gt;

&lt;h4 id=&#34;network&#34;&gt;Network&lt;/h4&gt;

&lt;p&gt;IPv4或IPv6网络，可以配置subnet和gateway。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contiv中的网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在contiv中可以配置两种类型的网络&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;application network：容器使用的网络&lt;/li&gt;
&lt;li&gt;infrastructure network：host namespace的虚拟网络，比如基础设施监控网络&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;网络封装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Contiv中有两种类型的网络封装&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Routed：overlay topology和L3-routed BGP topology&lt;/li&gt;
&lt;li&gt;Bridged：layer2 VLAN&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;tenant&#34;&gt;Tenant&lt;/h4&gt;

&lt;p&gt;Tenant提供contiv中的namespace隔离。一个tenant可以有很多个network，每个network都有个subnet。该tenant中的用户可以使用它的任意network和subnet的IP。&lt;/p&gt;

&lt;p&gt;物理网络中的tenant称作&lt;code&gt;虚拟路由转发(VRF)&lt;/code&gt;。Contiv使用VLAN和VXLAN ID来实现外部网络访问，这取决你使用的是layer2、layer3还是Cisco ACI。&lt;/p&gt;

&lt;h3 id=&#34;contiv下载&#34;&gt;Contiv下载&lt;/h3&gt;

&lt;p&gt;Contiv的编译安装比较复杂，我们直接下载github上的&lt;a href=&#34;[1.0.0-beta.3-03-08-2017.18-51-20.UTC](https://github.com/contiv/netplugin/releases/tag/1.0.0-beta.3-03-08-2017.18-51-20.UTC)&#34;&gt;release-1.0.0-beta.3-03-08-2017.18-51-20.UTC&lt;/a&gt;文件解压获得二进制文件安装。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。&#34;&gt;https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果试用可以的话，我会后续写contiv开发环境搭建的文章。&lt;/p&gt;

&lt;p&gt;这个release是2017年3月8日发布的，就在我写这篇文章的前一天。有个&lt;strong&gt;最重要的更新&lt;/strong&gt;是&lt;u&gt;支持docker1.13 swarm mode&lt;/u&gt;。&lt;/p&gt;

&lt;p&gt;安装文档：&lt;a href=&#34;https://github.com/contiv/netplugin/blob/master/install/HowtoSetupContiv.md&#34;&gt;https://github.com/contiv/netplugin/blob/master/install/HowtoSetupContiv.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下载解压后会得到如下几个文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;contivk8s  k8s专用的&lt;/li&gt;
&lt;li&gt;contrib  文件夹，里面有个&lt;code&gt;netctl&lt;/code&gt;的bash脚本&lt;/li&gt;
&lt;li&gt;netcontiv  这个命令就一个-version选项用来查看contiv的版本😓&lt;/li&gt;
&lt;li&gt;netctl  contiv命令行工具，用来配置网络、策略、服务负载均衡，&lt;a href=&#34;http://contiv.github.io/documents/reference/netctlcli.html&#34;&gt;使用说明&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;netmaster  contiv的主节点服务&lt;/li&gt;
&lt;li&gt;netplugin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面的安装中用到的只有netctl、netmaster和netplugin这三个二进制文件。&lt;/p&gt;

&lt;p&gt;我们将这三个文件都copy到/usr/bin目录下。&lt;/p&gt;

&lt;p&gt;我们在docker17.03-ce中安装contiv。&lt;/p&gt;

&lt;h3 id=&#34;contiv安装依赖&#34;&gt;Contiv安装依赖&lt;/h3&gt;

&lt;p&gt;Contiv依赖于consul或etcd，我们选择使用etcd，slack里的人说只支持2.3.x版本，可能不支持3.0+版本的吧，还没实际测过，先使用2.3.7。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;contiv master&lt;/code&gt;启动后自动向etcd中注册信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/contiv.io/oper
/contiv.io/oper/auto-vlan
/contiv.io/oper/auto-vlan/global
/contiv.io/oper/auto-vxlan
/contiv.io/oper/auto-vxlan/global
/contiv.io/oper/global
/contiv.io/oper/global/global
/contiv.io/oper/ovs-driver
/contiv.io/oper/ovs-driver/sz-pg-oam-docker-test-001.tendcloud.com
/contiv.io/master
/contiv.io/master/config
/contiv.io/master/config/global
/contiv.io/obj
/contiv.io/obj/modeldb
/contiv.io/obj/modeldb/global
/contiv.io/obj/modeldb/global/global
/contiv.io/obj/modeldb/tenant
/contiv.io/obj/modeldb/tenant/default
/contiv.io/lock
/contiv.io/lock/netmaster
/contiv.io/lock/netmaster/leader
/contiv.io/service
/contiv.io/service/netmaster
/contiv.io/service/netmaster/172.20.0.113:9999
/contiv.io/service/netmaster.rpc
/contiv.io/service/netmaster.rpc/172.20.0.113:9001
/contiv.io/state
/contiv.io/state/auto-vlan
/contiv.io/state/auto-vlan/global
/contiv.io/state/auto-vxlan
/contiv.io/state/auto-vxlan/global
/contiv.io/state/global
/contiv.io/state/global/global
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;contiv启动&#34;&gt;Contiv启动&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;启动netmaster&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$nohup netmaster -cluster-mode docker -cluster-store etcd://172.20.0.113:2379 -debug -listen-url 172.20.0.113:9999 -plugin-name netplugin &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了突出netmaster命令的使用，我把所有可以使用默认值的参数都明确的写出。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netmaster&lt;/code&gt;监听9999端口。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看已有的contiv网络&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl --netmaster http://172.20.0.113:9999 network ls
Tenant  Network  Nw Type  Encap type  Packet tag  Subnet   Gateway  IPv6Subnet  IPv6Gateway
------  -------  -------  ----------  ----------  -------  ------   ----------  -----------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了以后执行命令方便，不用来回输入&lt;code&gt;$NETMASTER&lt;/code&gt;地址，可以将其设置为环境变量&lt;/p&gt;

&lt;p&gt;&lt;code&gt;export NETMASTER=&amp;quot;http://172.20.0.113:9999&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;netplugin需要使用Open vSwitch，所以你需要先安装&lt;strong&gt;Open vSwitch&lt;/strong&gt;。否则你会遇到这个问题&lt;a href=&#34;https://github.com/contiv/netplugin/issues/760&#34;&gt;netplugin issue-760&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;open-vswitch安装&#34;&gt;Open vSwitch安装&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://supercomputing.caltech.edu/blog/index.php/2016/05/03/open-vswitch-installation-on-centos-7-2/&#34;&gt;Open vSwitch installation on CentOS7.2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考上面链接里的方法。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash
yum -y install make gcc openssl-devel autoconf automake rpm-build redhat-rpm-config python-devel openssl-devel kernel-devel kernel-debug-devel libtool wget
mkdir -p ~/rpmbuild/SOURCES
cp openvswitch-2.5.1.tar.gz ~/rpmbuild/SOURCES/
tar xfz openvswitch-2.5.1.tar.gz
sed &#39;s/openvswitch-kmod, //g&#39; openvswitch-2.5.1/rhel/openvswitch.spec &amp;gt; openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec
rpmbuild -bb --nocheck ~/openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译好的rpm包在&lt;code&gt;~/rpmbuild/RPMS/x86_64/openvswitch-2.5.1-1.x86_64.rpm&lt;/code&gt;目录下。&lt;/p&gt;

&lt;p&gt;安装好Open vSwitch后就可以启动&lt;strong&gt;netplugin&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&#34;创建contiv网络&#34;&gt;创建contiv网络&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;启动netplugin&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nohup netplugin -cluster-store etcd://172.20.0.113:2379 &amp;amp;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netctl --netmaster http://172.20.0.113:9999 network create --subnet=10.1.2.0/24 contiv-net&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;获得以下报错：&lt;/p&gt;

&lt;p&gt;ERRO[0000] Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&lt;/p&gt;

&lt;p&gt;但是执行第二次的时候居然成功了，不过当我查看docker network的时候根本就看不到刚刚创建的contiv-net网络。*这只是一场游戏一场梦。。。*😢&lt;/p&gt;

&lt;p&gt;Creating network default:contiv-net&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl network ls
Tenant   Network     Nw Type  Encap type  Packet tag  Subnet       Gateway  IPv6Subnet  IPv6Gateway
------   -------     -------  ----------  ----------  -------      ------   ----------  -----------
default  contiv-net  data     vxlan       0           10.1.2.0/24  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看刚创建的contiv-net网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$netctl network inspect contiv-net
Inspeting network: contiv-net tenant: default
{
  &amp;quot;Config&amp;quot;: {
    &amp;quot;key&amp;quot;: &amp;quot;default:contiv-net&amp;quot;,
    &amp;quot;encap&amp;quot;: &amp;quot;vxlan&amp;quot;,
    &amp;quot;networkName&amp;quot;: &amp;quot;contiv-net&amp;quot;,
    &amp;quot;nwType&amp;quot;: &amp;quot;data&amp;quot;,
    &amp;quot;subnet&amp;quot;: &amp;quot;10.1.2.0/24&amp;quot;,
    &amp;quot;tenantName&amp;quot;: &amp;quot;default&amp;quot;,
    &amp;quot;link-sets&amp;quot;: {},
    &amp;quot;links&amp;quot;: {
      &amp;quot;Tenant&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;tenant&amp;quot;,
        &amp;quot;key&amp;quot;: &amp;quot;default&amp;quot;
      }
    }
  },
  &amp;quot;Oper&amp;quot;: {
    &amp;quot;availableIPAddresses&amp;quot;: &amp;quot;10.1.2.1-10.1.2.254&amp;quot;,
    &amp;quot;externalPktTag&amp;quot;: 1,
    &amp;quot;pktTag&amp;quot;: 1
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从&lt;strong&gt;netmaster&lt;/strong&gt;日志中可以看到如下报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time=&amp;quot;Mar  9 21:44:14.746627381&amp;quot; level=debug msg=&amp;quot;NwInfra type is default, no ACI&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.750278056&amp;quot; level=info msg=&amp;quot;Creating docker network: {CheckDuplicate:true Driver:netplugin EnableIPv6:false IPAM:0xc4204d8ea0 Internal:false Attachable:true Options:map[tenant:default encap:vxlan pkt-tag:1] Labels:map[]}&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752034749&amp;quot; level=error msg=&amp;quot;Error creating network contiv-net. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752067294&amp;quot; level=error msg=&amp;quot;Error creating network contiv-net.default in docker. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752102735&amp;quot; level=error msg=&amp;quot;Error creating network {&amp;amp;{Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752129195&amp;quot; level=error msg=&amp;quot;NetworkCreate retruned error for: &amp;amp;{Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752155973&amp;quot; level=error msg=&amp;quot;CreateNetwork error for: {Key:default:contiv-net Encap:vxlan Gateway: Ipv6Gateway: Ipv6Subnet: NetworkName:contiv-net NwType:data PktTag:0 Subnet:10.1.2.0/24 TenantName:default LinkSets:{EndpointGroups:map[] Servicelbs:map[] Services:map[]} Links:{Tenant:{ObjType: ObjKey:}}}. Err: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
time=&amp;quot;Mar  9 21:44:14.752172138&amp;quot; level=error msg=&amp;quot;Handler for POST /api/v1/networks/default:contiv-net/ returned error: Error response from daemon: legacy plugin netplugin of type NetworkDriver is not supported in swarm mode&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;从日志中看到一个令人悲痛语句的话*legacy plugin netplugin of type NetworkDriver is not supported in swarm mode*，你们昨天不是刚发的版本说已经支持swarm mode吗？&lt;a href=&#34;https://github.com/contiv/netplugin/commit/8afd1b7718c8424a876760d18484124e0aad3557&#34;&gt;&lt;code&gt;commit-8afd1b7&lt;/code&gt;&lt;/a&gt;不是白纸黑字的写着吗？&lt;/p&gt;

&lt;p&gt;我提了个&lt;a href=&#34;https://github.com/contiv/netplugin/issues/776&#34;&gt;issue-776&lt;/a&gt;，看看怎样解决这个问题，另外netplugin命令怎么用，文档上没写啊？&lt;/p&gt;

&lt;p&gt;&lt;code&gt;netplugin -h&lt;/code&gt;可以中有两个选项我不明白，不知道怎么设置，有知道的人请告诉我一声。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  -vlan-if value
    	VLAN uplink interface
  -vtep-ip string
    	My VTEP ip address
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时我会继续关注contiv的slack和github &lt;a href=&#34;https://github.com/contiv/netplugin/issues/776&#34;&gt;Issue-776&lt;/a&gt;的进展。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contiv Intro</title>
      <link>http://rootsongjc.github.io/post/contiv_guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/contiv_guide/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017021162.jpg&#34; alt=&#34;蓝色港湾&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北京蓝色港湾夜景)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contiv&lt;/strong&gt;是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/1935&#34;&gt;容器网络插件 Calico 与 Contiv Netplugin深入比较&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有篇文章讲解了&lt;a href=&#34;http://blog.dataman-inc.com/shurenyun-docker-133/&#34;&gt;docker网络方案的改进&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contiv-netplugin-简介&#34;&gt;Contiv Netplugin 简介&lt;/h3&gt;

&lt;p&gt;Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34; alt=&#34;9260E9B7-43C0-48B8-B5C7-CF8B952959D2.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址&lt;/li&gt;
&lt;li&gt;Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。&lt;/li&gt;
&lt;li&gt;集群管理依赖 etcd/serf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34; alt=&#34;580469BC-468C-49C8-B29E-8B88143AFE0A.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;netplugin的优势&#34;&gt;Netplugin的优势&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等&lt;/li&gt;
&lt;li&gt;SDN能力，能够对容器的网络访问做更精细的控制&lt;/li&gt;
&lt;li&gt;多租户支持，具备未来向混合云/公有云迁移的潜力&lt;/li&gt;
&lt;li&gt;代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证&lt;/li&gt;
&lt;li&gt;&lt;u&gt;&lt;strong&gt;京东&lt;/strong&gt;基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。&lt;/u&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;next&#34;&gt;Next&lt;/h3&gt;

&lt;p&gt;后续文章会讲解contiv netplugin的环境配置和开发。目前还在1.0-beta版本。&lt;strong&gt;Docker store&lt;/strong&gt;上提供了contiv插件的&lt;a href=&#34;https://store.docker.com/plugins/803eecee-0780-401a-a454-e9523ccf86b3&#34;&gt;下载地址&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker service discovery</title>
      <link>http://rootsongjc.github.io/post/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-service-discovery/</guid>
      <description>&lt;p&gt;Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of &lt;a href=&#34;https://docs.docker.com/v1.11/swarm/discovery/&#34;&gt;service discovery backend&lt;/a&gt;. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34; alt=&#34;pic-intro&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to Docker 1.12 Swarm Mode&lt;/strong&gt;, we don’t have to depend upon these external tools and complex configurations. &lt;a href=&#34;https://github.com/docker/docker/releases/tag/v1.12.0-rc5&#34;&gt;Docker Engine 1.12&lt;/a&gt; runs it’s own internal DNS service to route services by name.Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it help?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you create a service and provide a name for it, you can use just that name as a target hostname, and it’s going to be automatically resolved to the proper container IP of the service. In short, within the swarm, containers can simply reference other services via their names and the built-in DNS will be used to find the appropriate IP and port automatically. It is important to note that if the service has multiple replicas, &lt;strong&gt;the requests would be round-robin load-balanced&lt;/strong&gt;. This would still work if you didn’t forward any ports when you created your docker services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34; alt=&#34;Pic10&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Embedded DNS is not a new concept. It was first included under Docker 1.10 release. Please note that DNS lookup for containers connected to user-defined networks works differently compared to the containers connected to &lt;code&gt;default bridge&lt;/code&gt; network. As of Docker 1.10, the docker daemon implements an embedded DNS server which provides built-in service discovery for any container created with a valid &lt;code&gt;name&lt;/code&gt; or &lt;code&gt;net-alias&lt;/code&gt; or aliased by &lt;code&gt;link&lt;/code&gt;. Moreover,container name configured using &lt;code&gt;--name&lt;/code&gt; is used to discover a container within an user-defined docker network. The embedded DNS server maintains the mapping between the container name and its IP address (on the network the container is connected to).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does Embedded DNS resolve unqualified names?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34; alt=&#34;Pic22&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With Docker 1.12 release, a new API called “service” is being included which clearly talks about the functionality of service discovery.  It is important to note that Service discovery is scoped within the network. What it really means is –  If you have redis application and web client as two separate services , you combine into single application and put them into same network.If you try build your application in such a way that you are trying to reach to redis through name “redis”,it will always resolve to name “redis”. Reason – both of these services are part of the same network. You don’t need to be inside the application trying to resolve this service using FQDN. Reason – FQDN name is not going to be portable which in turn, makes your application non-portable.&lt;/p&gt;

&lt;p&gt;Internally, there is a listener opened inside the container itself. If we try to enter into the container which is providing a service discovery and look at /etc/resolv.conf, we will find that the nameserver entry holds something really different like 127.0.0.11.This is nothing but a loopback address. So, whenever resolver tried to resolve, it will resolve to 127.0.0.11 and this request is rightly trapped.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34; alt=&#34;Pic-12&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this request is trapped, it is sent to particular random UDP / TCP port currently being listened under the docker daemon. Consequently, the socket is to be created inside the namespace. When DNS server and daemon gets the request, it knows that this is coming from which specific network, hence gets aware of  the context of from where it is coming from.Once it knows the context, it can generate the appropriate DNS response.&lt;/p&gt;

&lt;p&gt;To demonstrate Service Discovery  under Docker 1.12, I have upgraded Docker 1.12.rc5 to 1.12.0 GA version. The swarm cluster look like:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34; alt=&#34;Pico01&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have created a network called “collabnet” for the new services as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34; alt=&#34;Pic-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create a service called “wordpressdb” under collabnet network :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34; alt=&#34;pico-mysql&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can list the running tasks(containers) and the node on which these containers are running on:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34; alt=&#34;Pic-4&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create another service called “wordpressapp” under the same network:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34; alt=&#34;pico-app&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, we can list out the number of services running on our swarm cluster as shown below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34; alt=&#34;pico-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have scaled out the number of wordpressapp and wordpressdb just for demonstration purpose.&lt;/p&gt;

&lt;p&gt;Let’s consider my master node where I have two of the containers running as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34; alt=&#34;Pico-1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can reach out one service(wordpressapp) from another service(wordpressapp) through just service-name as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34; alt=&#34;pico-last&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, I can reach out to particular container by its name from other container running different service but on the same network. As shown below, I can reach out to wordpressapp.3.6f8bthp container via wordpressdb.7.e62jl57qqu running wordpressdb.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34; alt=&#34;pico-tasktoo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The below picture depicts the Service Discovery in a nutshell:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic23.png&#34; alt=&#34;Pic23&#34; /&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every service has Virtual IP(VIP) associated which can be derived as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34; alt=&#34;pic-list&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As shown above, each service has an IP address and this IP address maps to multiple container IP address associated with that service. It is important to note that service IP associated with a service does not change even though containers associated with the service dies/ restarts.&lt;/p&gt;

&lt;p&gt;Few important points to remember:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VIP based services use Linux IPVS load balancing to route to the backend containers. This works only for TCP/UDP protocols. When you use DNS-RR mode services don’t have a VIP allocated. Instead service names resolves to one of the backend container IPs randomly.&lt;/li&gt;
&lt;li&gt;Ping not working for VIP is as designed. Technically, IPVS is a TCP/UDP load-balancer, while ping uses ICMP and hence IPVS is not going to load-balance the ping request.&lt;/li&gt;
&lt;li&gt;For VIP based services the reason ping works on the local node is because the VIP is added a 2nd IP address on the overlay network interface&lt;/li&gt;
&lt;li&gt;You can any of the tools like  dig, nslookup or wget -O- &lt;service name&gt; to demonstrate the service discovery functionality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below picture depicts that the network is the scope of service discoverability which means that when you have a service running on one network , it is scoped to that network and won’t be able to reach out to different service running on different network(unless it is part of that network).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34; alt=&#34;SD&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s dig little further introducing Load-balancing aspect too. To see what is basically enabling the load-balancing functionality, we can go into sandbox of each containers and see how it has been resolved.&lt;/p&gt;

&lt;p&gt;Let’s pick up the two containers running on the master node. We can see the sandbox running through the following command:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34; alt=&#34;pico-namespace&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Under /var/run/docker/netns, you will find various namespaces. The namespaces marked with x-{id} represents network namespace managed by the overlay network driver for its operation (such as creating a bridge, terminating vxlan tunnel, etc…). They don’t represent the container network namespace. Since it is managed by the driver, it is not recommended to manipulate anything within this namespace. But if you are curious on the deep dive, then you can use the “nsenter” tool to understand more about this internal namespace.&lt;/p&gt;

&lt;p&gt;We can enter into sandbox through the nsenter utility:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34; alt=&#34;pico-mangle&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case you faced an error stating “nsenter: reassociate to namespace ‘ns/net’ failed: Invalid argument”, I suggest to look at &lt;a href=&#34;http://tinyurl.com/gu5rsw9&#34;&gt;this&lt;/a&gt; workaround.&lt;/p&gt;

&lt;p&gt;10.0.3.4 service IP is marked 0x108 using iptables OUTPUT chain. ipvs uses this marking and load balances it to containers 10.0.3.5 and 10.0.3.6 as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34; alt=&#34;ipvs&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are key takeaways from this entire post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34; alt=&#34;Pic34&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&#34;http://collabnix.com/archives/1504&#34;&gt;http://collabnix.com/archives/1504&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker embedded dns</title>
      <link>http://rootsongjc.github.io/post/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-embedded-dns/</guid>
      <description>

&lt;p&gt;本文主要介绍了&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Docker&lt;/a&gt;容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。&lt;/p&gt;

&lt;h2 id=&#34;configure-container-dns&#34;&gt;Configure container DNS&lt;/h2&gt;

&lt;h3 id=&#34;dns-in-default-bridge-network&#34;&gt;DNS in default bridge network&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-h HOSTNAME or –hostname=HOSTNAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME or ID:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=IP_ADDRESS…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;添加到容器内的/etc/resolv.conf中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：

&lt;ul&gt;
&lt;li&gt;如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.conf内容更新容器内的/etc/resolv.conf.&lt;/li&gt;
&lt;li&gt;如果容器状态为running，则容器内的/etc/resolv.conf将不会改变，直到该容器状态变为stopped.&lt;/li&gt;
&lt;li&gt;如果容器启动后修改过容器内的/etc/resolv.conf，则不会对该容器进行处理，否则可能会丢失已经完成的修改，无论该容器为什么状态。 
如果容器启动时，用了–dns, –dns-search, or –dns-opt选项，其启动时已经修改了宿主机的/etc/resolv.conf过滤后的内容，因此docker daemon永远不会更新这种容器的/etc/resolv.conf。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;: docker daemon监控宿主机/etc/resolv.conf的这个file change notifier的实现是依赖linux内核的inotify特性，而inotfy特性不兼容overlay fs，因此使用overlay fs driver的docker deamon将无法使用该/etc/resolv.conf自动更新的功能。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;embedded-dns-in-user-defined-networks&#34;&gt;Embedded DNS in user-defined networks&lt;/h3&gt;

&lt;p&gt;在docker 1.10版本中，docker daemon实现了一个叫做&lt;code&gt;embedded DNS server&lt;/code&gt;的东西，用来当你创建的容器满足以下条件时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用自定义网络；&lt;/li&gt;
&lt;li&gt;容器创建时候通过&lt;code&gt;--name&lt;/code&gt;,&lt;code&gt;--network-alias&lt;/code&gt; or &lt;code&gt;--link&lt;/code&gt;提供了一个name；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;docker daemon就会利用embedded DNS server对整个自定义网络中所有容器进行名字解析（你可以理解为一个网络中的一种服务发现）。&lt;/p&gt;

&lt;p&gt;因此当你启动容器时候满足以上条件时，该容器的域名解析就不应该去考虑容器内的/etc/hosts, /etc/resolv.conf，应该保持其不变，甚至为空，将需要解析的域名都配置到对应embedded DNS server中。具体配置参数及说明如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;–name=CONTAINER-NAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将CONTAINER-NAME和该容器的IP配置到该容器连接到的自定义网络中的embedded DNS server中，由它提供该自定义网络范围内的域名解析&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–network-alias=ALIAS&lt;/td&gt;
&lt;td&gt;将容器的name-ip map配置到容器连接到的其他网络的embedded DNS server中。PS：一个容器可能连接到多个网络中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP配置到该容器连接到的自定义网络中的embedded DNS server中，但仅限于配置了该link的容器能解析这条rule。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=[IP_ADDRESS…]&lt;/td&gt;
&lt;td&gt;当embedded DNS server无法解析该容器的某个dns query时，会将请求foward到这些–dns配置的IP_ADDRESS DNS Server，由它们进一步进行域名解析。注意，这些–dns配置到&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;全部由对应的embedded DNS server管理，并不会更新到容器内的/etc/resolv.conf.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-search配置的DOMAIN们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-opt配置的OPTION们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;注意容器内/etc/resolv.conf中配置的DNS server，只有当the embedded DNS server无法解析某个name时，才会用到。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;embedded-dns-server源码分析&#34;&gt;embedded DNS server源码分析&lt;/h2&gt;

&lt;p&gt;所有embedded DNS server相关的代码都在libcontainer项目中，几个最主要的文件分别是&lt;code&gt;/libnetwork/resolver.Go&lt;/code&gt;,&lt;code&gt;/libnetwork/resolver_unix.go&lt;/code&gt;,&lt;code&gt;sandbox_dns_unix.go&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;OK, 先来看看embedded DNS server对象在docker中的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go

// resolver implements the Resolver interface
type resolver struct {
    sb         *sandbox
    extDNSList [maxExtDNS]extDNSEntry
    server     *dns.Server
    conn       *net.UDPConn
    tcpServer  *dns.Server
    tcpListen  *net.TCPListener
    err        error
    count      int32
    tStamp     time.Time
    queryLock  sync.Mutex
}

// Resolver represents the embedded DNS server in Docker. It operates
// by listening on container&#39;s loopback interface for DNS queries.
type Resolver interface {
    // Start starts the name server for the container
    Start() error
    // Stop stops the name server for the container. Stopped resolver
    // can be reused after running the SetupFunc again.
    Stop()
    // SetupFunc() provides the setup function that should be run
    // in the container&#39;s network namespace.
    SetupFunc() func()
    // NameServer() returns the IP of the DNS resolver for the
    // containers.
    NameServer() string
    // SetExtServers configures the external nameservers the resolver
    // should use to forward queries
    SetExtServers([]string)
    // ResolverOptions returns resolv.conf options that should be set
    ResolverOptions() []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，resolver就是embedded DNS server，每个resolver都bind一个sandbox，并定义了一个对应的dns.Server，还定义了外部DNS对象列表，但embedded DNS server无法解析某个name时，就会forward到那些外部DNS。&lt;/p&gt;

&lt;p&gt;Resolver Interface定义了embedded DNS server必须实现的接口，这里会重点关注SetupFunc()和Start()，见下文分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;dns.Server的实现，全部交给github.com/miekg/dns，限于篇幅，这里我将不会跟进去分析。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从整个&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Container&lt;/a&gt; create的流程上来看，docker daemon对embedded DNS server的处理是从endpoint Join a sandbox开始的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libnetwork/endpoint.go


func (ep *endpoint) Join(sbox Sandbox, options ...EndpointOption) error {
    ...

    return ep.sbJoin(sb, options...)
}


func (ep *endpoint) sbJoin(sb *sandbox, options ...EndpointOption) error {
    ...

    if err = sb.populateNetworkResources(ep); err != nil {
        return err
    }

    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox join a sandbox的流程中，会调用sandbox. populateNetworkResources做网络资源的设置，这其中就包括了embedded DNS server的启动。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox.go
func (sb *sandbox) populateNetworkResources(ep *endpoint) error {
    ...
    if ep.needResolver() {
        sb.startResolver(false)
    }
    ...
}


libnetwork/sandbox_dns_unix.go
func (sb *sandbox) startResolver(restore bool) {
    sb.resolverOnce.Do(func() {
        var err error
        sb.resolver = NewResolver(sb)
        defer func() {
            if err != nil {
                sb.resolver = nil
            }
        }()

        // In the case of live restore container is already running with
        // right resolv.conf contents created before. Just update the
        // external DNS servers from the restored sandbox for embedded
        // server to use.
        if !restore {
            err = sb.rebuildDNS()
            if err != nil {
                log.Errorf(&amp;quot;Updating resolv.conf failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
                return
            }
        }
        sb.resolver.SetExtServers(sb.extDNS)

        sb.osSbox.InvokeFunc(sb.resolver.SetupFunc())
        if err = sb.resolver.Start(); err != nil {
            log.Errorf(&amp;quot;Resolver Setup/Start failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox.startResolver是流程关键:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过sanbdox.rebuildDNS生成了container内的/etc/resolv.conf&lt;/li&gt;
&lt;li&gt;通过resolver.SetExtServers(sb.extDNS)设置embedded DNS server的forward DNS list&lt;/li&gt;
&lt;li&gt;通过resolver.SetupFunc()启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener&lt;/li&gt;
&lt;li&gt;通过resolver.Start()对容器内的iptable进行设置(见下)，并通过miekg/dns启动一个nameserver在53端口提供服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我将逐一介绍上面的各个步骤。&lt;/p&gt;

&lt;h3 id=&#34;sanbdox-rebuilddns&#34;&gt;sanbdox.rebuildDNS&lt;/h3&gt;

&lt;p&gt;sanbdox.rebuildDNS负责构建容器内的resolv.conf，构建规则就是第一节江参数配置时候提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save the external name servers in resolv.conf in the sandbox&lt;/li&gt;
&lt;li&gt;Add only the embedded server’s IP to container’s resolv.conf&lt;/li&gt;
&lt;li&gt;If the embedded server needs any resolv.conf options add it to the current list&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox_dns_unix.go

func (sb *sandbox) rebuildDNS() error {
    currRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)
    if err != nil {
        return err
    }

    // localhost entries have already been filtered out from the list
    // retain only the v4 servers in sb for forwarding the DNS queries
    sb.extDNS = resolvconf.GetNameservers(currRC.Content, types.IPv4)

    var (
        dnsList        = []string{sb.resolver.NameServer()}
        dnsOptionsList = resolvconf.GetOptions(currRC.Content)
        dnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)
    )

    dnsList = append(dnsList, resolvconf.GetNameservers(currRC.Content, types.IPv6)...)

    resOptions := sb.resolver.ResolverOptions()

dnsOpt:
    ...
    dnsOptionsList = append(dnsOptionsList, resOptions...)

    _, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setextservers&#34;&gt;resolver.SetExtServers&lt;/h3&gt;

&lt;p&gt;设置embedded DNS server的forward DNS list, 当embedded DNS server不能解析某name时，就会将请求forward到ExtServers。代码很简单，不多废话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go
func (r *resolver) SetExtServers(dns []string) {
    l := len(dns)
    if l &amp;gt; maxExtDNS {
        l = maxExtDNS
    }
    for i := 0; i &amp;lt; l; i++ {
        r.extDNSList[i].ipStr = dns[i]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setupfunc&#34;&gt;resolver.SetupFunc&lt;/h3&gt;

&lt;p&gt;启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;libnetwork/resolver.go

func (r *resolver) SetupFunc() func() {
    return (func() {
        var err error

        // DNS operates primarily on UDP
        addr := &amp;amp;net.UDPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.conn, err = net.ListenUDP(&amp;quot;udp&amp;quot;, addr)
        ...

        // Listen on a TCP as well
        tcpaddr := &amp;amp;net.TCPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.tcpListen, err = net.ListenTCP(&amp;quot;tcp&amp;quot;, tcpaddr)
        ...
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-start&#34;&gt;resolver.Start&lt;/h3&gt;

&lt;p&gt;resolver.Start中两个重要步骤，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;setupIPTable设置容器内的iptables&lt;/li&gt;
&lt;li&gt;启动dns nameserver在53端口开始提供域名解析服务&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;func (r *resolver) Start() error {
    ...
    if err := r.setupIPTable(); err != nil {
        return fmt.Errorf(&amp;quot;setting up IP table rules failed: %v&amp;quot;, err)
    }
    ...
    tcpServer := &amp;amp;dns.Server{Handler: r, Listener: r.tcpListen}
    r.tcpServer = tcpServer
    go func() {
        tcpServer.ActivateAndServe()
    }()
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先来看看怎么设置容器内的iptables的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (r *resolver) setupIPTable() error {
    ...
    // 获取setupFunc()时的两个本地随机监听端口
    laddr := r.conn.LocalAddr().String()
    ltcpaddr := r.tcpListen.Addr().String()

    cmd := &amp;amp;exec.Cmd{
        Path:   reexec.Self(),
        // 将这两个端口传给setup-resolver命令并启动执行
        Args:   append([]string{&amp;quot;setup-resolver&amp;quot;}, r.sb.Key(), laddr, ltcpaddr),
        Stdout: os.Stdout,
        Stderr: os.Stderr,
    }
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(&amp;quot;reexec failed: %v&amp;quot;, err)
    }
    return nil
}

// init时就注册setup-resolver对应的handler
func init() {
    reexec.Register(&amp;quot;setup-resolver&amp;quot;, reexecSetupResolver)
}

// setup-resolver对应的handler定义
func reexecSetupResolver() {
    ...
    // 封装iptables数据
    _, ipPort, _ := net.SplitHostPort(os.Args[2])
    _, tcpPort, _ := net.SplitHostPort(os.Args[3])
    rules := [][]string{
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[2]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--sport&amp;quot;, ipPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[3]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--sport&amp;quot;, tcpPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
    }
    ...

    // insert outputChain and postroutingchain
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在reexecSetupResolver()中清楚的定义了iptables添加outputChain 和postroutingchain，将到容器内的dns query请求重定向到embedded DNS server(127.0.0.11)上的udp/tcp两个随机可用端口，embedded DNS server(127.0.0.11)的返回数据则重定向到容器内的53端口，这样完成了整个dns query请求。&lt;/p&gt;

&lt;p&gt;模型图如下： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215440792?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;贴一张实例图： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215310369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20170105215322635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到这里，关于embedded DNS server的源码分析就结束了。当然，其中还有很多细节，就留给读者自己走读代码了。&lt;/p&gt;

&lt;h2 id=&#34;福利&#34;&gt;福利&lt;/h2&gt;

&lt;p&gt;从该时序图中看看embedded DNS server的操作在整个容器create流程中的位置。
&lt;img src=&#34;http://img.blog.csdn.net/20170105215401307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>