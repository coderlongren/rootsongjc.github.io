<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud Computing on Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/tags/cloud-computing/index.xml</link>
    <description>Recent content in Cloud Computing on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://rootsongjc.github.io/tags/cloud-computing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>使用Fluentd和ElasticSearch收集Kubernetes集群日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</link>
      <pubDate>Fri, 07 Apr 2017 20:07:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160430080.jpg&#34; alt=&#34;古北水镇&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：码头@古北水镇 Apr 30,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;安装好了Kubernetes集群&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;配置好了flannel网络&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/&#34;&gt;安装了Kubernetes Dashboard&lt;/a&gt;和&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/&#34;&gt;配置Heapster监控插件&lt;/a&gt;后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;官方文档&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34;&gt;Kubernetes Logging and Monitoring Cluster Activity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/&#34;&gt;Logging Using Elasticsearch and Kibana&lt;/a&gt;：不过这篇文章是在GCE上配置的，参考价值不大。&lt;/p&gt;

&lt;h2 id=&#34;容器日志的存在形式&#34;&gt;容器日志的存在形式&lt;/h2&gt;

&lt;p&gt;目前容器日志有两种输出形式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;stdout,stderr标准输出&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种形式的日志输出我们可以直接使用&lt;code&gt;docker logs&lt;/code&gt;查看日志，kubernetes集群中同样可以使用&lt;code&gt;kubectl logs&lt;/code&gt;类似的形式查看日志。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;日志文件记录&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种日志输出我们无法从以上方法查看日志内容，只能&lt;code&gt;tail&lt;/code&gt;日志文件查看。&lt;/p&gt;

&lt;h2 id=&#34;fluentd介绍&#34;&gt;Fluentd介绍&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/fluent/fluentd&#34;&gt;Fluentd&lt;/a&gt;是使用Ruby编写的，通过在后端系统之间提供&lt;strong&gt;统一的日志记录层&lt;/strong&gt;来从后端系统中解耦数据源。
此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。
统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。
也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/c4abfe337c0b54b36f81bce78481f8965acbc7a9/687474703a2f2f646f63732e666c75656e74642e6f72672f696d616765732f666c75656e74642d6172636869746563747572652e706e67&#34; alt=&#34;arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fluentd收集的&lt;strong&gt;event&lt;/strong&gt;由以下几个方面组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tag&lt;/strong&gt;：字符串，中间用点隔开，如myapp.access&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;：UNIX时间格式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Record&lt;/strong&gt;：JSON格式&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fluentd特点&#34;&gt;Fluentd特点&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;部署简单灵活&lt;/li&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;经过验证的可靠性和性能&lt;/li&gt;
&lt;li&gt;社区支持，插件较多&lt;/li&gt;
&lt;li&gt;使用json格式事件格式&lt;/li&gt;
&lt;li&gt;可拔插的架构设计&lt;/li&gt;
&lt;li&gt;低资源要求&lt;/li&gt;
&lt;li&gt;内置高可靠性&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;查看&lt;code&gt;cluster/addons/fluentd-elasticsearch&lt;/code&gt;插件目录，获取到需要用到的docker镜像名称。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$grep -rn &amp;quot;gcr.io&amp;quot; *.yaml
es-controller.yaml:24:      - image: gcr.io/google_containers/elasticsearch:v2.4.1-2
fluentd-es-ds.yaml:26:        image: gcr.io/google_containers/fluentd-elasticsearch:1.22
kibana-controller.yaml:22:        image: gcr.io/google_containers/kibana:v4.6.1-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;需要用到的镜像&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcr.io/google_containers/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为这些镜像在墙外，所以我特意备份了一份在本地还有时速云上。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;测试环境镜像名称&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;备份到时速云上的镜像名称&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/elasticsearch:v2.4.1-2&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/kibana:v4.6.1-1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/fluentd-elasticsearch:1.22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;修改上面的那三个yaml文件，将其中的镜像名称改成我们测试环境中的。&lt;/p&gt;

&lt;h3 id=&#34;启动集群&#34;&gt;启动集群&lt;/h3&gt;

&lt;p&gt;使用刚修改好yaml文件的那个目录启动fluentd-elasticsearch。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f flucentd-elasticsearch
$kubectl get -f fluentd-elasticsearch/
NAME                          DESIRED   CURRENT   READY     AGE
rc/elasticsearch-logging-v1   2         2         2         13m

NAME                        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/elasticsearch-logging   10.254.107.114   &amp;lt;none&amp;gt;        9200/TCP   13m

NAME                  DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
ds/fluentd-es-v1.22   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   13m

NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kibana-logging   1         1         1            1           13m

NAME                 CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/kibana-logging   10.254.104.215   &amp;lt;none&amp;gt;        5601/TCP   13m

$kubectl cluster-info
Kubernetes master is running at http://sz-pg-oam-docker-test-001:8080
Elasticsearch is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging
monitoring-grafana is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at http://sz-pg-oam-docker-test-001:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动完成，但是查看pod的日志后会发现出错了。&lt;/p&gt;

&lt;p&gt;如何保证每个节点启动一个Fluentd呢？答案是使用DaemonSet。&lt;/p&gt;

&lt;h3 id=&#34;排错&#34;&gt;排错&lt;/h3&gt;

&lt;p&gt;查看启动的pod。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl --namespace=kube-system get all
NAME                                       READY     STATUS    RESTARTS   AGE
po/elasticsearch-logging-v1-nshz2          1/1       Running   0          16m
po/elasticsearch-logging-v1-q515j          1/1       Running   0          16m
po/heapster-3669180046-06n3d               1/1       Running   0          23h
po/kibana-logging-4247188994-h8jxx         1/1       Running   0          16m
po/kubernetes-dashboard-1074266307-hsgxx   1/1       Running   0          1d
po/monitoring-grafana-127711743-xl9v1      1/1       Running   0          23h
po/monitoring-influxdb-1411048194-cvxmm    1/1       Running   0          23h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;应该在个node节点上启动的&lt;strong&gt;fluentd&lt;/strong&gt;没有看到。查看logging pod的日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl -n kube-system logs po/elasticsearch-logging-v1-nshz2
F0406 08:30:05.488197       7 elasticsearch_logging_discovery.go:49] Failed to make client: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
goroutine 1 [running]:
...
[2017-04-06 08:30:23,450][WARN ][discovery.zen.ping.unicast] [elasticsearch-logging-v1-nshz2] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[][127.0.0.1:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:945)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:360)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$4400(ZenDiscovery.java:96)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1296)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected]
	at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1141)
	at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:830)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:329)
	... 12 more
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以看到报错中有这样的描述：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;discovery.zen.ping.unicast failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[ Node not connected]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里面有两个错误：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;无法访问到API Server&lt;/li&gt;
&lt;li&gt;elasticsearch两个节点间互ping失败&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个是镜像中的配置问题，配置文件在&lt;code&gt;fluentd-es-image/td-agent.conf&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;参考&lt;a href=&#34;http://tonybai.com/2017/03/03/implement-kubernetes-cluster-level-logging-with-fluentd-and-elasticsearch-stack/&#34;&gt;使用Fluentd和ElasticSearch Stack实现Kubernetes的集群Logging&lt;/a&gt;，Tony Bai也遇到了这个问题，我们了解下&lt;a href=&#34;https://kubernetes.io/docs/user-guide/configmap/&#34;&gt;ConfigMap&lt;/a&gt;还有&lt;a href=&#34;https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter&#34;&gt;fluent-plugin-kubernetes_metadata_filter&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;参考我的另一片译文&lt;a href=&#34;rootsongjc.github.io/blogs/kubernetes-configmap-introduction&#34;&gt;Kubernetes中ConfigMap解析&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;问题排查&#34;&gt;问题排查&lt;/h2&gt;

&lt;p&gt;前面写的是直接使用&lt;code&gt;kubectl create -f flucentd-elasticsearch&lt;/code&gt;命令启动整个fluentd+elasticsearch集群，这样启动看似很简单，但是对于问题排查的时候不便于我们分析出错原因，因为你根本不知道服务之间的依赖关系和启动顺序，所以现在我们依次启动每个服务，看看背后都做了什么。&lt;/p&gt;

&lt;h3 id=&#34;启动fluentd&#34;&gt;启动fluentd&lt;/h3&gt;

&lt;p&gt;首先启动fluentd收集日志的服务，从&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;的配置中可以看到fluentd是以&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt;方式来运行的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DaemonSet简介&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DaemonSet能够让所有（或者一些特定）的Node节点运行同一个pod。当节点加入到kubernetes集群中，pod会被（DaemonSet）调度到该节点上运行，当节点从kubernetes集群中被移除，被（DaemonSet）调度的pod会被移除，如果删除DaemonSet，所有跟这个DaemonSet相关的pods都会被删除。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dockerinfo.net/1139.html&#34;&gt;DaemonSet详细介绍&lt;/a&gt;，这是官方文档的中文翻译，其中还有示例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动fluentd&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f fluentd-es-ds.yaml
daemonset &amp;quot;fluentd-es-v1.22&amp;quot; created
$kubectl get -f fluentd-es-ds.yaml 
NAME               DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
fluentd-es-v1.22   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在没有修改&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;的情况下直接启动fluentd，实际上一个Pod也没有启动起来，这是为什么呢？因为&lt;strong&gt;NODE-SELECTOR&lt;/strong&gt;选择的label是&lt;code&gt;beta.kubernetes.io/fluentd-ds-ready=true&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;我们再来看下&lt;strong&gt;node&lt;/strong&gt;的&lt;strong&gt;label&lt;/strong&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl describe node sz-pg-oam-docker-test-001.tendcloud.com
Name:			sz-pg-oam-docker-test-001.tendcloud.com
Role:			
Labels:			beta.kubernetes.io/arch=amd64
			beta.kubernetes.io/os=linux
			kubernetes.io/hostname=sz-pg-oam-docker-test-001.tendcloud.com
Annotations:		node.alpha.kubernetes.io/ttl=0
			volumes.kubernetes.io/controller-managed-attach-detach=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们没有给node设置&lt;code&gt;beta.kubernetes.io/fluentd-ds-ready=true&lt;/code&gt;的label，所以DaemonSet没有调度上去。&lt;/p&gt;

&lt;p&gt;我们需要手动给kubernetes集群的三个node添加label。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl label node sz-pg-oam-docker-test-001.tendcloud.com beta.kubernetes.io/fluentd-ds-ready=true
node &amp;quot;sz-pg-oam-docker-test-001.tendcloud.com&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给另外两个node执行同样的操作。&lt;/p&gt;

&lt;p&gt;现在再查看下DaemonSet的状态。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get -f fluentd-es-ds.yaml 
NAME               DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR                              AGE
fluentd-es-v1.22   3         3         0         3            0           beta.kubernetes.io/fluentd-ds-ready=true   31m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以看到三个DeamonSet都启动起来了。&lt;/p&gt;

&lt;p&gt;查看下fluentd的日志&lt;code&gt;/var/log/fluentd.log&lt;/code&gt;，日志是mount到本地的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-04-07 03:53:42 +0000 [info]: adding match pattern=&amp;quot;fluent.**&amp;quot; type=&amp;quot;null&amp;quot;
2017-04-07 03:53:42 +0000 [info]: adding filter pattern=&amp;quot;kubernetes.**&amp;quot; type=&amp;quot;kubernetes_metadata&amp;quot;
2017-04-07 03:53:42 +0000 [error]: config error file=&amp;quot;/etc/td-agent/td-agent.conf&amp;quot; error=&amp;quot;Invalid Kubernetes API v1 endpoint https://10.254.0.1:443/api: SSL_connect returned=1 errno=0 state=error: certificate verify failed&amp;quot;
2017-04-07 03:53:42 +0000 [info]: process finished code=256
2017-04-07 03:53:42 +0000 [warn]: process died within 1 second. exit.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从日志的最后几行中可以看到，&lt;code&gt;Invalid Kubernetes API v1 endpoint https://10.254.0.1:443/api: SSL_connect returned=1 errno=0 state=error: certificate verify failed&lt;/code&gt;这样的错误，这些需要在&lt;code&gt;/etc/td-agent/td-agent.conf&lt;/code&gt;文件中配置的。&lt;/p&gt;

&lt;p&gt;但是这些配置已经在创建&lt;code&gt;gcr.io/google_containers/fluentd-elasticsearch:1.22&lt;/code&gt;镜像（该镜像是运行带有elasticsearch插件的fluentd的）的时候就已经copy进去了，从&lt;code&gt;fluentd-elasticsearch/fluentd-es-image/Dockerfile&lt;/code&gt;文件中就可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Copy the Fluentd configuration file.
COPY td-agent.conf /etc/td-agent/td-agent.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以使用&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/&#34;&gt;ConfigMap&lt;/a&gt;，不用重新再build镜像，通过文件挂载的形式替换镜像中已有的td-agent.conf文件。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;tonybai.com&#34;&gt;Tony Bai&lt;/a&gt;给出的两点建议：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在基于td-agent.conf创建configmap资源之前，需要将td-agent.conf中的注释行都删掉，否则生成的configmap的内容可能不正确；&lt;/li&gt;
&lt;li&gt;fluentd pod将创建在kube-system下，因此ConfigMap资源也需要创建在kube-system namespace下面，否则kubectl create无法找到对应的ConfigMap。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在td-agent.conf的配置文件的&lt;filter kubernetes.**&gt;中增加两条配置配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter kubernetes.**&amp;gt;
  type kubernetes_metadata
  kubernetes_url sz-pg-oam-docker-test-001.tendcloud.com:8080
  verify_ssl false
&amp;lt;/filter&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;创建ConfigMap&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap td-agent-config --from-file=fluentd-elasticsearch/fluentd-es-image/td-agent.conf -n kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看刚创建的ConfigMap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl -n kube-system get configmaps td-agent-config -o yaml
apiVersion: v1
data:
  td-agent.conf: |
    &amp;lt;match fluent.**&amp;gt;
      type null
    &amp;lt;/match&amp;gt;
...
&amp;lt;filter kubernetes.**&amp;gt;
  type kubernetes_metadata
  kubernetes_url http://sz-pg-oam-docker-test-001.tendcloud.com:8080
  verify_ssl false
&amp;lt;/filter&amp;gt;
...

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;⚠️ kubernetes_url地址要加上&lt;strong&gt;http&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;修改&lt;code&gt;fluentd-es-ds.yaml&lt;/code&gt;文件，在其中增加&lt;code&gt;td-agent.conf&lt;/code&gt;文件的volume。&lt;/p&gt;

&lt;p&gt;该文件的部分内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
...
    spec:
     ...
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: td-agent-config
          mountPath: /etc/td-agent
...
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: td-agent-config
        configMap:
          name: td-agent-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动日志收集服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./fluentd-elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在再查看&lt;code&gt;/var/log/fluentd.log&lt;/code&gt;日志里面就没有错误了。&lt;/p&gt;

&lt;p&gt;查看下elasticsearch pod日志，发现里面还有错误，跟以前的一样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2017-04-07 10:54:57,858][WARN ][discovery.zen.ping.unicast] [elasticsearch-logging-v1-wxd5f] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
SendRequestTransportException[[][127.0.0.1:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:340)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:945)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:360)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$4400(ZenDiscovery.java:96)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1296)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][127.0.0.1:9300] Node not connected]
	at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1141)
	at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:830)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:329)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看下elasticsearch:v2.4.1-2镜像的代码，在&lt;code&gt;fluentd-elasticsearch/es-image&lt;/code&gt;目录下，该目录结构：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;config
Dockerfile
elasticsearch_logging_discovery.go
Makefile
run.sh
template-k8s-logstash.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从&lt;strong&gt;Dockerfile&lt;/strong&gt;中可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;RUN mkdir -p /elasticsearch/config/templates
COPY template-k8s-logstash.json /elasticsearch/config/templates/template-k8s-logstash.json

COPY config /elasticsearch/config

COPY run.sh /
COPY elasticsearch_logging_discovery /

RUN useradd --no-create-home --user-group elasticsearch \
    &amp;amp;&amp;amp; mkdir /data \
    &amp;amp;&amp;amp; chown -R elasticsearch:elasticsearch /elasticsearch

VOLUME [&amp;quot;/data&amp;quot;]
EXPOSE 9200 9300

CMD [&amp;quot;/run.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将本地的&lt;code&gt;config&lt;/code&gt;目录作为配置文件拷贝到了镜像里，&lt;code&gt;run.sh&lt;/code&gt;启动脚本中有三行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/elasticsearch_logging_discovery &amp;gt;&amp;gt; /elasticsearch/config/elasticsearch.yml

chown -R elasticsearch:elasticsearch /data

exec gosu elasticsearch /elasticsearch/bin/elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再进入到镜像里查看下&lt;code&gt;/elasticsearch/config/elasticsearch.yml&lt;/code&gt;文件的内容。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;cluster.name: kubernetes-logging

node.name: ${NODE_NAME}
node.master: ${NODE_MASTER}
node.data: ${NODE_DATA}

transport.tcp.port: ${TRANSPORT_PORT}
http.port: ${HTTP_PORT}

path.data: /data

network.host: 0.0.0.0

discovery.zen.minimum_master_nodes: ${MINIMUM_MASTER_NODES}
discovery.zen.ping.multicast.enabled: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;记录几个问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes中的DNS没有配置。&lt;/li&gt;
&lt;li&gt;ElasticSearch的配置有问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;To be continued…&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes heapster监控插件安装文档</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</link>
      <pubDate>Wed, 05 Apr 2017 18:41:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20170403064.jpg&#34; alt=&#34;嗑猫薄荷的白化孟加拉虎&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;前面几篇文章中记录了我们&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;安装好了Kubernetes集群&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;配置好了flannel网络&lt;/a&gt;、&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/&#34;&gt;安装了Kubernetes Dashboard&lt;/a&gt;，但是还没法查看Pod的监控信息，虽然kubelet默认集成了&lt;strong&gt;cAdvisor&lt;/strong&gt;（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。&lt;/p&gt;

&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;下载heapster的代码&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;直接现在Github上的最新代码。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull https://github.com/kubernetes/heapster.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前的最高版本是1.3.0。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;heapster/deploy/kube-config/influxdb&lt;/code&gt;目录下有几个&lt;code&gt;yaml&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grafana-deployment.yaml
grafana-service.yaml
heapster-deployment.yaml
heapster-service.yaml
influxdb-deployment.yaml
influxdb-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们再看下用了哪些镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grafana-deployment.yaml:16:        image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
heapster-deployment.yaml:16:        image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1
influxdb-deployment.yaml:16:        image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;下载镜像&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们下载好了这些images后，存储到私有镜像仓库里：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2&lt;/li&gt;
&lt;li&gt;sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我已经将官方镜像克隆到了&lt;a href=&#34;www.tenxcloud.com&#34;&gt;时速云&lt;/a&gt;上，镜像地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker login index.tendcloud.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;

&lt;p&gt;参考&lt;a href=&#34;https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md&#34;&gt;Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI&lt;/a&gt;和&lt;a href=&#34;https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md&#34;&gt;Configuring Source&lt;/a&gt;，需要修改yaml文件中的几个配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的&lt;/li&gt;
&lt;li&gt;修改heapster-deployment.yaml文件中的&lt;code&gt;--source&lt;/code&gt;参数为&lt;code&gt;—source=kubernetes:http://sz-pg-oam-docker-test-001.tendcloud.com:8080?inClusterConfig=false&amp;amp;useServiceAccount=false&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;修改完配置的&lt;code&gt;heapster-deployment.yaml&lt;/code&gt;文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: heapster
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: heapster
    spec:
      containers:
      - name: heapster
        image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1
        imagePullPolicy: IfNotPresent
        command:
        - /heapster
        - --source=kubernetes:http://sz-pg-oam-docker-test-001.tendcloud.com:8080?inClusterConfig=false&amp;amp;useServiceAccount=false
        - --sink=influxdb:http://monitoring-influxdb:8086
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;启动&#34;&gt;启动&lt;/h2&gt;

&lt;p&gt;在准备好镜像和修改完配置文件后就可以一键启动了，这不就是使用kbuernetes的方便之处吗？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动heaspter&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create -f deploy/kube-config/influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看状态&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get -f deploy/kube-config/influxdb/
NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/monitoring-grafana   1         1         1            1           1h

NAME                     CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
svc/monitoring-grafana   10.254.250.27   &amp;lt;none&amp;gt;        80/TCP    1h

NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/heapster   1         1         1            1           1h

NAME           CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
svc/heapster   10.254.244.187   &amp;lt;none&amp;gt;        80/TCP    1h

NAME                         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/monitoring-influxdb   1         1         1            1           1h

NAME                      CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/monitoring-influxdb   10.254.151.157   &amp;lt;none&amp;gt;        8086/TCP   1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看页面&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;现在再打开Dashboard页面就可以看到CPU和Memory的监控信息了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-heapster-01.jpg&#34; alt=&#34;kubernetes-heapster&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;虽然在安装了heapster插件后可以在dashboard中看到CPU和Memory的监控信息，但是这仅仅是近实时的监控，收集的metrics被保存到了InfluxDB中，还可以通过Kibana或者Grafana来展示更详细的信息和历史数据，还是有很多事情可以做的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard/Web UI安装全记录</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</link>
      <pubDate>Wed, 05 Apr 2017 14:28:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017040301.jpg&#34; alt=&#34;晒太阳的袋鼠&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;前几天&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;在CentOS7.2上安装Kubernetes1.6&lt;/a&gt;和安装好&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-network-config/&#34;&gt;flannel网络配置&lt;/a&gt;，今天我们来安装下kuberentnes的dashboard。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Dashboard&lt;/a&gt;是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟&lt;a href=&#34;https://www.docker.com/enterprise-edition&#34;&gt;Docker Enterprise Edition&lt;/a&gt;的&lt;strong&gt;Docker Datacenter&lt;/strong&gt;很像了。&lt;/p&gt;

&lt;h2 id=&#34;安装dashboard&#34;&gt;安装Dashboard&lt;/h2&gt;

&lt;p&gt;官网的安装文档&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。&#34;&gt;https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先需要一个&lt;strong&gt;kubernetes-dashboard.yaml&lt;/strong&gt;的配置文件，可以直接在&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/src/deploy/kubernetes-dashboard.yaml&#34;&gt;Github的src/deploy/kubernetes-dashboard.yaml&lt;/a&gt;下载。&lt;/p&gt;

&lt;p&gt;我们能看下这个文件的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Configuration to deploy release version of the Dashboard UI.
#
# Example usage: kubectl create -f &amp;lt;this_file&amp;gt;

kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kubernetes-dashboard
  template:
    metadata:
      labels:
        app: kubernetes-dashboard
      # Comment the following annotation if Dashboard must not be deployed on master
      annotations:
        scheduler.alpha.kubernetes.io/tolerations: |
          [
            {
              &amp;quot;key&amp;quot;: &amp;quot;dedicated&amp;quot;,
              &amp;quot;operator&amp;quot;: &amp;quot;Equal&amp;quot;,
              &amp;quot;value&amp;quot;: &amp;quot;master&amp;quot;,
              &amp;quot;effect&amp;quot;: &amp;quot;NoSchedule&amp;quot;
            }
          ]
    spec:
      containers:
      - name: kubernetes-dashboard
        image: sz-pg-oam-docker-hub-001.tendcloud.com/library/kubernetes-dashboard-amd64:v1.6.0
        imagePullPolicy: Always
        ports:
        - containerPort: 9090
          protocol: TCP
        args:
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
          - --apiserver-host=http://sz-pg-oam-docker-test-001.tendcloud.com:8080
        livenessProbe:
          httpGet:
            path: /
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 9090
  selector:
    app: kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;⚠️ 官方提供的image名为&lt;code&gt;gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0&lt;/code&gt;，需要翻墙才能访问，我自己拉下来push到我们的私有镜像仓库了。我将这个镜像push到了docker hub上，如果你无法翻墙的话，可以到下载这个镜像：&lt;code&gt;index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0&lt;/code&gt;。时速云的镜像存储，速度就是快。&lt;/p&gt;

&lt;p&gt;准备好image后就可以部署了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f kubernetes-dashboard.yaml
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
$kubectl get -f kubernetes-dashboard.yaml
NAME                          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/kubernetes-dashboard   1         1         1            1           9s

NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
svc/kubernetes-dashboard   10.254.113.226   &amp;lt;nodes&amp;gt;       80:31370/TCP   8s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以访问&lt;a href=&#34;http://sz-pg-oam-docker-test-001.tendcloud.com:8080/ui了，效果如图：&#34;&gt;http://sz-pg-oam-docker-test-001.tendcloud.com:8080/ui了，效果如图：&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/kubernetes-dashboard-01.jpg&#34; alt=&#34;kubernetes-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;

&lt;h3 id=&#34;如果你没启动service-account身份认证&#34;&gt;如果你没启动Service Account身份认证&lt;/h3&gt;

&lt;p&gt;那就好办了，检查下你的&lt;strong&gt;kubernetes-dashboard.yaml&lt;/strong&gt;文件，看看是不是API Server地址配错了，或者查看下pod的log，我就是在log里发现，原来API Server的主机名无法解析导致服务启动失败。在DNS里添加API Server主机的DNS记录即可。&lt;/p&gt;

&lt;h3 id=&#34;如果你启动api-server的serviceaccount身份认证&#34;&gt;如果你启动API Server的ServiceAccount身份认证&lt;/h3&gt;

&lt;p&gt;启动service的时候出错。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace=kube-system logs kubernetes-dashboard-1680927228-pdv45
Using HTTP port: 9090
Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
Refer to the troubleshooting guide for more information: https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md&#34;&gt;troubleshooting.md&lt;/a&gt;文件已经说明了，这是可能是你配置API server地址或&lt;strong&gt;Service Account&lt;/strong&gt;的问题。&lt;/p&gt;

&lt;p&gt;如果是配置Service Account的问题，可以借鉴Tony Bai的&lt;a href=&#34;http://tonybai.com/2017/01/19/install-dashboard-addon-for-k8s/&#34;&gt;Kubernetes集群Dashboard插件安装&lt;/a&gt;这篇文章。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;启动&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl proxy --address=&#39;0.0.0.0&#39; --accept-hosts=&#39;^*$&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错信息&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {},
  &amp;quot;status&amp;quot;: &amp;quot;Failure&amp;quot;,
  &amp;quot;message&amp;quot;: &amp;quot;no endpoints available for service \&amp;quot;kubernetes-dashboard\&amp;quot;&amp;quot;,
  &amp;quot;reason&amp;quot;: &amp;quot;ServiceUnavailable&amp;quot;,
  &amp;quot;code&amp;quot;: 503
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# start a container that contains curl
$ kubectl run test --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/curl:latest -- sleep 10000
$kubectl get pod
NAME                     READY     STATUS    RESTARTS   AGE
test-2428763157-pxkps    1/1       Running   0          6s
$kubectl exec test-2428763157-pxkps ls /var/run/secrets/kubernetes.io/serviceaccount/
ls: cannot access /var/run/secrets/kubernetes.io/serviceaccount/: No such file or directory
$kubectl get secrets
No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/var/run/secrets/kubernetes.io/serviceaccount/&lt;/code&gt;这个目录还是不存在，我们安装的Kubernetes压根就没有设置secret。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md&#34;&gt;troubleshooting.md&lt;/a&gt;上说需要用&lt;code&gt;—admission-control&lt;/code&gt;配置API Server，在配置这个之前还要了解下&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;Service Accounts&lt;/a&gt;和&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;如何管理Service Accounts&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;一年前我安装Kubernetes Dashboard（那时候好像还叫Kube-UI）的时候没有其功能还极其不完善，经过一年多的发展，已经有模有样了，如果不启用&lt;strong&gt;Service Account&lt;/strong&gt;的话，安装Dashboard还是很简单的。接下来我还要在Dashboard上安装其它Add-on，如Heapster用来监控Pod状态。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>容器技术在大数据场景下的应用——Yarn on Docker</title>
      <link>http://rootsongjc.github.io/projects/yarn-on-docker/</link>
      <pubDate>Tue, 04 Apr 2017 00:19:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/yarn-on-docker/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;作者：&lt;a href=&#34;rootsongjc.github.io/about/&#34;&gt;宋净超&lt;/a&gt; TalkingData云计算及大数据工程师&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;我已就该话题已在2016年上海Qcon上发表过演讲，&lt;a href=&#34;http://www.infoq.com/cn/presentations/yarn-on-docker-container-technology-in-big-data-scenarios&#34;&gt;点此观看&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;另外InfoQ网站上的文字版&lt;a href=&#34;http://www.infoq.com/cn/articles/YarnOnDocker-forDCCluster&#34;&gt;数据中心的Yarn on Docker集群方案&lt;/a&gt;，即本文。&lt;/p&gt;

&lt;p&gt;项目代码开源在Github上：&lt;a href=&#34;github.com/rootsongjc/magpie&#34;&gt;Magpie&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;当前数据中心存在的问题&#34;&gt;当前数据中心存在的问题&lt;/h2&gt;

&lt;p&gt;数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源  数据中心中普遍存在如下问题：&lt;/p&gt;

&lt;p&gt;1.主机资源利用率低&lt;/p&gt;

&lt;p&gt;2.部署和扩展复杂&lt;/p&gt;

&lt;p&gt;3.资源隔离无法动态调整&lt;/p&gt;

&lt;p&gt;4.无法快速响应业务&lt;/p&gt;

&lt;p&gt;为何使用Yarnon Docker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;彻底隔离队列 &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  为了合理利用Hadoopyarn的资源，队列间会互相抢占计算资源，造成重要任务阻塞&lt;/p&gt;

&lt;p&gt;•  根据部门申请的机器数量划分Yarn集群方便财务管理&lt;/p&gt;

&lt;p&gt;•  更细粒度的资源分配 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;统一的资源分配&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  每个NodeManager和容器都可以限定CPU、内存资源&lt;/p&gt;

&lt;p&gt;•  Yarn资源划分精确到CPU核数和内存大小 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;弹性伸缩性服务&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  每个容器中运行一个NodeManager，增减yarn资源只需增减容器个数&lt;/p&gt;

&lt;p&gt;•  可以指定每个NodeManager拥有的计算资源多少，按需申请资源 &lt;/p&gt;

&lt;h2 id=&#34;给我们带来什么好处&#34;&gt;给我们带来什么好处？ &lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Swarm统一集群资源调度&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; •  统一资源&lt;/p&gt;

&lt;p&gt;•  增加Docker虚拟化层，降低运维成本&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;增加Hadoop集群资源利用率&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;•  Fordatacenter：避免了静态资源隔离&lt;/p&gt;

&lt;p&gt;•  Forcluster：加强集群内部资源隔离&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;系统架构&#34;&gt;系统架构&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch.jpg&#34; alt=&#34;td_yarn_arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;  比如数据中心中运行的Hadoop集群，我们将HDFS依然运行在物理机上，即DataNode依然部署在实体机器上，将Yarn计算层运行在Docker容器中，整个系统使用二层资源调度，Spark、Flinek、MapReduce等应用运行在Yarn上。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    Swarm调度最底层的主机硬件资源，CPU和内存封装为Docker容器，容器中运行NodeManager，提供给Yarn集群，一个Swarm集群中可以运行多个Yarn集群，形成圈地式的Yarn计算集群。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch2.jpg&#34; alt=&#34;td_yarn_arch2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体流程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1.swarm node向swarm master注册主机资源并加入到swarmcluster中&lt;/p&gt;

&lt;p&gt;2.swarm master向cluster申请资源请求启动容器&lt;/p&gt;

&lt;p&gt;3.swarm根据调度策略选择在某个node上启动dockercontainer&lt;/p&gt;

&lt;p&gt;4.swarm node的docker deamon根据容器启动参数启动相应资源大小的NodeManager&lt;/p&gt;

&lt;p&gt;5.NodeManager自动向YARN的ResourceManager注册资源一个NodeManager资源添加完成。&lt;/p&gt;

&lt;p&gt;  &lt;/p&gt;

&lt;p&gt;Swarm为数据中心做容器即主机资源调度，每个swarmnode的节点结构如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_arch3.jpg&#34; alt=&#34;td_yarn_arch3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;一个Swarmnode就是一台物理机，每台主机上可以起多个同类型的dockercontainer，每个container的资源都有限制包括CPU、内存NodeManager容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是24核64G，我们可以分给一个容器5核12G，NodeManager占用4核10G的资源提供给Yarn。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KubernetesVS Swarm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;   关于容器集群管理系统的选型，用Kubernetes还是Swarm？我们结合自己的经验和业务需求，对比如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_compare.jpg&#34; alt=&#34;td_yarn_compare&#34; /&gt;&lt;/p&gt;

&lt;p&gt;基于以上四点，我们最终选择了Swarm，它基本满足我们的需求，掌握和开发时常较短。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;镜像制作与发布&#34;&gt;镜像制作与发布&lt;/h2&gt;

&lt;p&gt;镜像制作和发布流程如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_ci.jpg&#34; alt=&#34;td_yarn_ci&#34; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    用户从客户端提交代码到Gitlab中，需要包含Dockerfile文件，通过集成了docker插件的Jenkins的自动编译发布机制，自动build镜像后push到docker镜像仓库中，同一个项目每提交一次代码都会重新build一次镜像，生成不同的tag来标识镜像，Swarm集群使用该镜像仓库就可以直接拉取镜像。&lt;/p&gt;

&lt;p&gt;Dockerfile的编写技巧&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_dockerfile.jpg&#34; alt=&#34;td_yarn_dockerfile&#34; /&gt;&lt;/p&gt;

&lt;p&gt; Dockerfile相当于docker镜像的编译打包流程说明，其中也不乏一些技巧。&lt;/p&gt;

&lt;p&gt;     &lt;/p&gt;

&lt;p&gt;    很多应用需要配置文件，如果想为每次启动容器的时候使用不同的配置参数，可以通过传递环境变量的方式来修改配置文件，前提是需要写一个bash脚本，脚本中来处理配置文件，再将这个脚本作为entrypoint入口，每当容器启动时就会执行这个脚本从而替换配置文件中的参数，也可以通过CMD传递参数给该脚本。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;启动容器的时候通过传递环境变量的方式修改配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run -d 
--net=mynet 
-e NAMESERVICE=nameservice 
-e ACTIVE_NAMENODE_ID=namenode29 \
-e STANDBY_NAMENODE_ID=namenode63 \
-e HA_ZOOKEEPER_QUORUM=zk1:2181,zk2:2181,zk3:2181 \
-e YARN_ZK_DIR=rmstore \
-e YARN_CLUSTER_ID=yarnRM \
-e YARN_RM1_IP=rm1 \
-e YARN_RM2_IP=rm2 \
-e CPU_CORE_NUM=5
-e NODEMANAGER_MEMORY_MB=12288 \
-e YARN_JOBHISTORY_IP=jobhistory \
-e ACTIVE_NAMENODE_IP=active-namenode \
-e STANDBY_NAMENODE_IP=standby-namenode \
-e HA=yes \
docker-registry/library/hadoop-yarn:v0.1 resourcemanager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后传递resourcemanager或者nodemanager参数指定启动相应的服务。&lt;/p&gt;

&lt;p&gt;集群管理&lt;/p&gt;

&lt;p&gt;我开发的命令行工具&lt;a href=&#34;https://github.com/rootsongjc/magpie&#34;&gt;magpie&lt;/a&gt;，也可以通过其他开源可视化页面来管理集群，比如shipyard。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_shipyard.jpg&#34; alt=&#34;td_yarn_shipyard&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;自定义网络&#34;&gt;自定义网络&lt;/h2&gt;

&lt;p&gt; Docker容器跨主机互访一直是一个问题，Docker官方为了避免网络上带来的诸多麻烦，故将跨主机网络开了比较大的口子，而由用户自己去实现。我们开发并开源了Shrike这个docker网络插件，大家可以在这里下载到：&lt;a href=&#34;https://github.com/rootsongjc/docker-ipam-plugin&#34;&gt;https://github.com/rootsongjc/docker-ipam-plugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    目前Docker跨主机的网络实现方案也有很多种, 主要包括端口映射，ovs,fannel等。但是这些方案都无法满足我们的需求，端口映射服务内的内网IP会映射成外网的IP，这样会给开发带来困惑，因为他们往往在跨网络交互时是不需要内网IP的，而ovs与fannel则是在基础网络协议上又包装了一层自定义协议，这样当网络流量大时，却又无端的增加了网络负载，最后我们采取了自主研发扁平化网络插件，也就是说让所有的容器统统在大二层上互通。架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_network.jpg&#34; alt=&#34;td_yarn_network&#34; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;我们首先需要创建一个br0自定义网桥，这个网桥并不是通过系统命令手动建立的原始Linux网桥，而是通过Docker的cerate network命令来建立的自定义网桥，这样避免了一个很重要的问题就是我们可以通过设置DefaultGatewayIPv4参数来设置容器的默认路由，这个解决了原始Linux自建网桥不能解决的问题. 用Docker创建网络时我们可以通过设置subnet参数来设置子网IP范围，默认我们可以把整个网段给这个子网，后面可以用ipamdriver（地址管理插件）来进行控制。还有一个参数gateway是用来设置br0自定义网桥地址的，其实也就是你这台宿主机的地址。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker network create 
--opt=com.docker.network.bridge.enable_icc=true
--opt=com.docker.network.bridge.enable_ip_masquerade=false
--opt=com.docker.network.bridge.host_binding_ipv4=0.0.0.0
--opt=com.docker.network.bridge.name=br0
--opt=com.docker.network.driver.mtu=1500
--ipam-driver=talkingdata
--subnet=容器IP的子网范围
--gateway=br0网桥使用的IP,也就是宿主机的地址
--aux-address=DefaultGatewayIPv4=容器使用的网关地址
mynet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_ipam.jpg&#34; alt=&#34;td_yarn_ipam&#34; /&gt;&lt;/p&gt;

&lt;p&gt;IPAM驱动是专门管理Docker 容器IP的, Docker 每次启停与删除容器都会调用这个驱动提供的IP管理接口，然后IP接口会对存储IP地址的Etcd有一个增删改查的操作。此插件运行时会起一个UnixSocket, 然后会在docker/run/plugins目录下生成一个.sock文件，Dockerdaemon之后会和这个sock 文件进行沟通去调用我们之前实现好的几个接口进行IP管理，以此来达到IP管理的目的，防止IP冲突。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    通过Docker命令去创建一个自定义的网络起名为“mynet”，同时会产生一个网桥br0，之后通过更改网络配置文件（在/etc/sysconfig/network-scripts/下ifcfg-br0、ifcfg-默认网络接口名）将默认网络接口桥接到br0上，重启网络后，桥接网络就会生效。Docker默认在每次启动容器时都会将容器内的默认网卡桥接到br0上，而且宿主机的物理网卡也同样桥接到了br0上了。其实桥接的原理就好像是一台交换机，Docker 容器和宿主机物理网络接口都是服务器，通过vethpair这个网络设备像一根网线插到交换机上。至此，所有的容器网络已经在同一个网络上可以通信了，每一个Docker容器就好比是一台独立的虚拟机，拥有和宿主机同一网段的IP，可以实现跨主机访问了。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;性能瓶颈与优化&#34;&gt;性能瓶颈与优化&lt;/h2&gt;

&lt;p&gt;    大家可能会担心自定义网络的性能问题，为此我们用iperf进行了网络性能测试。我们对比了不同主机容器间的网速，同一主机上的不同容器和不同主机间的网速，结果如下表：&lt;/p&gt;

&lt;p&gt; &lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_iperf.jpg&#34; alt=&#34;td_yarn_iperf&#34; /&gt;&lt;/p&gt;

&lt;p&gt; 从表中我们可以看到，在这一组测试中，容器间的网速与容器是在想通主机还是在不同主机上的差别不大，说明我们的网络插件性能还是很优异的。&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&#34;hadoop配置优化&#34;&gt;Hadoop配置优化 &lt;/h2&gt;

&lt;p&gt;    因为使用docker将原来一台机器一个nodemanager给细化为了多个，会造成nodemanager个数的成倍增加，因此hadoop的一些配置需要相应优化。&lt;/p&gt;

&lt;p&gt;•  yarn.nodemanager.localizer.fetch.thread-count 随着容器数量增加，需要相应调整该参数&lt;/p&gt;

&lt;p&gt;•  yarn.resourcemanager.amliveliness-monitor.interval-ms默认1秒，改为10秒，否则时间太短可能导致有些节点无法注册&lt;/p&gt;

&lt;p&gt;•  yarn.resourcemanager.resource-tracker.client.thread-count默认50，改为100，随着容器数量增加，需要相应调整该参数&lt;/p&gt;

&lt;p&gt;•  yarn.nodemanager.pmem-check-enabled默认true，改为false，不检查任务正在使用的物理内存量&lt;/p&gt;

&lt;p&gt;•  容器中hadoop ulimit值修改，默认4096，改成655350&lt;/p&gt;

&lt;p&gt;集群监控&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;如果使用shipyard管理集群会有一个单独的监控页面，可以看到一定时间段内的CPU、内存、IO、网络使用状况。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_monitor.jpg&#34; alt=&#34;td_yarn_monitor&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;关于未来&#34;&gt;关于未来&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/img/yarn-on-docker/td_yarn_os.jpg&#34; alt=&#34;td_yarn_os&#34; /&gt; &lt;/p&gt;

&lt;p&gt;    我们未来规划做的是DC／OS，基于Docker的应用自动打包编译分发系统，让开发人员可以很便捷的申请资源，上下线服务，管理应用。要达到这个目标还有很多事情要做：&lt;/p&gt;

&lt;p&gt;•  Service Control Panel：统一的根据服务来管理的web页面&lt;/p&gt;

&lt;p&gt;•  Loadbalance：容器根据机器负载情况自动迁移&lt;/p&gt;

&lt;p&gt;•  Scheduler：swarm调度策略优化&lt;/p&gt;

&lt;p&gt;•  服务配置文件：提供镜像启动参数的配置文件，所有启动参数可通过文件配置&lt;/p&gt;

&lt;p&gt;•  监控：服务级别的监控&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;这篇文章写好的时候是2016年10月，距离现在我添加&lt;strong&gt;前言&lt;/strong&gt;和&lt;strong&gt;后记&lt;/strong&gt;的已经快半年时间了，这段时间内业界也发生了很多变化，比如docker推出CE和SE版本，Google的kubernetes发布了1.6版本，人工智能依然大热，在可预见的未来，可以说&lt;u&gt;Kubernetes一定会远远超越Docker成为容器编排领域的王者&lt;/u&gt;，这是毋庸置疑的，对于docker 17.03-CE我也研究过了一段时间，其disgusting的plugin让我对于docker的编排已经失去信心。&lt;/p&gt;

&lt;p&gt;其实容器在大数据场景下的应用并不是很多，毕竟Hadoop那套笨重的东西放在容器下运行，上生产环境? Are you kidding me?如果说做原型验证、研发测试那还可以。这样就大大限制了容器技术在大数据场景下的应用场景。使用容器的编排调度来实现大数据集群的资源优化有点舍本逐末，&lt;u&gt;如果真的要优化集群资源利用率的话，应该让不同的应用混跑，而不应该让集群之间资源隔离，比如Web应用跟大数据应用混布。&lt;/u&gt;目前的这种&lt;strong&gt;Yarn on Docker&lt;/strong&gt;方案实质上是将原来的整体Hadoop Yarn集群划分成多个不同的Yarn，将存储和计算分离了。其实这跟&lt;strong&gt;Nutanix&lt;/strong&gt;的超融合架构有点像，Nutanix是由前Google的工程师创立的，解决虚拟化计算环境下的存储问题，也是将存储和计算分离，共享存储，计算根据需要调度。事实上Yahoo已经有解决Hadoop集群的资源细粒度分配和调度问题的方案，这应该是从Yarn的scheduler层来处理。&lt;/p&gt;

&lt;p&gt;Swarm已死，Swarmkit将继续发展，Docker的Swarm Mode还会在艰难中前行，目前看到的趋势仍然是模仿Kubernentes为主，没有自己鲜明的特色（除了部署管理方便意外，谁让它就集成在了docker里呢，就像当年windows集成IE打败Netscape，不过这不会再此上演了），Kubernentes又是一个通用的资源调度框架，它的最小资源划分是&lt;strong&gt;Pod&lt;/strong&gt;而不是docker，它还可以运行rkt、containerd。&lt;/p&gt;

&lt;p&gt;上周起我开始将注意力转移到kubernentes，以后请关注我的&lt;a href=&#34;http://rootsongjc.github.io/tags/kubernetes/&#34;&gt;Kuberentes实践&lt;/a&gt;相关文章。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014100402.jpg&#34; alt=&#34;西安鼓楼&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：西安鼓楼 Oct 4,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;书接上文&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;在CentOS中安装Kubernetes详细指南&lt;/a&gt;，这是一个系列文章，作为学习Kubernetes的心路历程吧。&lt;/p&gt;

&lt;p&gt;本文主要讲解&lt;strong&gt;Kubernetes的网络配置&lt;/strong&gt;，👆文中有一个安装&lt;strong&gt;Flannel&lt;/strong&gt;的步骤，但是安装好后并没有相应的配置说明。&lt;/p&gt;

&lt;h2 id=&#34;配置flannel&#34;&gt;配置flannel&lt;/h2&gt;

&lt;p&gt;我们直接使用的yum安装的flannle，安装好后会生成&lt;code&gt;/usr/lib/systemd/system/flanneld.service&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到flannel环境变量配置文件在&lt;code&gt;/etc/sysconfig/flanneld&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;etcd的地址&lt;code&gt;FLANNEL_ETCD_ENDPOINT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。&lt;code&gt;FLANNEL_ETCD_PREFIX&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;在etcd中创建网络配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;执行下面的命令为docker分配IP地址段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;etcdctl mkdir /kube-centos/network
etcdctl mk /kube-centos/network/config &amp;quot;{ \&amp;quot;Network\&amp;quot;: \&amp;quot;172.30.0.0/16\&amp;quot;, \&amp;quot;SubnetLen\&amp;quot;: 24, \&amp;quot;Backend\&amp;quot;: { \&amp;quot;Type\&amp;quot;: \&amp;quot;vxlan\&amp;quot; } }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Flannel的&lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/running.md&#34;&gt;文档&lt;/a&gt;中有写&lt;strong&gt;Docker Integration&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;Docker daemon accepts &lt;code&gt;--bip&lt;/code&gt; argument to configure the subnet of the docker0 bridge. It also accepts &lt;code&gt;--mtu&lt;/code&gt; to set the MTU for docker0 and veth devices that it will be creating. Since flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Systemd users can use &lt;code&gt;EnvironmentFile&lt;/code&gt; directive in the .service file to pull in &lt;code&gt;/run/flannel/subnet.env&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;下载flannel github release中的tar包，解压后会获得一个&lt;strong&gt;mk-docker-opts.sh&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;这个文件是用来&lt;code&gt;Generate Docker daemon options based on flannel env file&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;执行&lt;code&gt;./mk-docker-opts.sh -i&lt;/code&gt;将会生成如下两个文件环境变量文件。&lt;/p&gt;

&lt;p&gt;/run/flannel/subnet.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.46.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/run/docker_opts.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_OPT_BIP=&amp;quot;--bip=172.30.46.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$etcdctl ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.14.0-24
/kube-centos/network/subnets/172.30.38.0-24
/kube-centos/network/subnets/172.30.46.0-24
$etcdctl get /kube-centos/network/config
{ &amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; } }
$etcdctl get /kube-centos/network/subnets/172.30.14.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.114&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;56:27:7d:1c:08:22&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.38.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.115&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;12:82:83:59:cf:b8&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.46.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.113&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;e6:b2:fd:f6:66:96&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;设置docker0网桥的IP地址&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;source /run/flannel/subnet.env
ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;6: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:da:bf:83:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.1/24 brd 172.30.38.255 scope global docker0
       valid_lft forever preferred_lft forever
7: flannel.1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN 
    link/ether 9a:29:46:61:03:44 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以重启docker了。&lt;/p&gt;

&lt;p&gt;重启了docker后还要重启kubelet，这时又遇到问题，kubelet启动失败。报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mar 31 16:44:41 sz-pg-oam-docker-test-002.tendcloud.com kubelet[81047]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &amp;quot;cgroupfs&amp;quot; is different from docker cgroup driver: &amp;quot;systemd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是kubelet与docker的&lt;strong&gt;cgroup driver&lt;/strong&gt;不一致导致的，kubelet启动的时候有个&lt;code&gt;—cgroup-driver&lt;/code&gt;参数可以指定为&amp;rdquo;cgroupfs&amp;rdquo;或者“systemd”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--cgroup-driver string                                    Driver that the kubelet uses to manipulate cgroups on the host.  Possible values: &#39;cgroupfs&#39;, &#39;systemd&#39; (default &amp;quot;cgroupfs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动flannel&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;systemctl daemon-reload
systemctl start flanneld
systemctl status flanneld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新登录这三台主机，可以看到每台主机都多了一个IP。&lt;/p&gt;

&lt;p&gt;参考Kubernetes官方文档的&lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/&#34;&gt;Exposing an External IP Address to Access an Application in a Cluster&lt;/a&gt;，官方使用的Hello World测试，我们启动Nginx服务测试。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;#启动nginx的pod
kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/nginx:1.9  --port=80
#创建名为example-service的服务
kubectl expose deployment nginx --type=NodePort --name=example-service
#查看状态
kubectl get deployments nginx
kubectl describe deployments nginx
kubectl get replicasets
kubectl describe replicasets
kubectl describe svc example-service
###################################################
Name:			example-service
Namespace:		default
Labels:			run=load-balancer-example
Annotations:		&amp;lt;none&amp;gt;
Selector:		run=load-balancer-example
Type:			NodePort
IP:			10.254.180.209
Port:			&amp;lt;unset&amp;gt;	80/TCP
NodePort:		&amp;lt;unset&amp;gt;	32663/TCP
Endpoints:		172.30.14.2:80,172.30.46.2:80
Session Affinity:	None
Events:			&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们上面启动的serivce的type是&lt;strong&gt;NodePort&lt;/strong&gt;，Kubernetes的service支持三种类型的service，参考&lt;a href=&#34;http://www.cnblogs.com/xuxinkun/p/5331728.html&#34;&gt;Kubernetes Serivce分析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;现在访问三台物理机的IP:80端口就可以看到nginx的页面了。&lt;/p&gt;

&lt;p&gt;稍等一会在访问ClusterIP + Port也可以访问到nginx。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl 10.254.180.209:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;虚拟地址&#34;&gt;虚拟地址&lt;/h2&gt;

&lt;p&gt;Kubernetes中的Service了使用了虚拟地址；该地址无法ping通过，但可以访问其端口。通过下面的命令可以看到，该虚拟地址是若干条iptables的规则。到10.254.124.145:8080端口的请求会被重定向到172.30.38.2或172.30.46.2的8080端口。这些规则是由kube-proxy生成；如果需要某台机器可以访问Service，则需要在该主机启动kube-proxy。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看service的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables-save|grep example-service
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SEP-NCPBOLUH5XTTHG3E -s 172.30.46.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-NCPBOLUH5XTTHG3E -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.46.2:80
-A KUBE-SEP-ONEKQBIWICF7RAR3 -s 172.30.14.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-ONEKQBIWICF7RAR3 -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.14.2:80
-A KUBE-SERVICES -d 10.254.180.209/32 -p tcp -m comment --comment &amp;quot;default/example-service: cluster IP&amp;quot; -m tcp --dport 80 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ONEKQBIWICF7RAR3
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-SEP-NCPBOLUH5XTTHG3E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看clusterIP的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables -t nat -nL|grep 10.254
KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0/0            10.254.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到在PREROUTING环节，k8s设置了一个target: KUBE-SERVICES。而KUBE-SERVICES下面又设置了许多target，一旦destination和dstport匹配，就会沿着chain进行处理。&lt;/p&gt;

&lt;p&gt;比如：当我们在pod网络curl 10.254.198.44 80时，匹配到下面的KUBE-SVC-BR4KARPIGKMRMN3E target：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考&lt;a href=&#34;http://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/&#34;&gt;理解Kubernetes网络之Flannel网络&lt;/a&gt;，Tony Bai的文章中有对flannel的详细介绍。&lt;/p&gt;

&lt;h2 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h2&gt;

&lt;p&gt;在设置网络的过程中遇到了很多问题，记录如下。&lt;/p&gt;

&lt;h3 id=&#34;问题一&#34;&gt;问题一&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kube-proxy开放的&lt;strong&gt;NodePort&lt;/strong&gt;端口无法访问。即无法使用NodeIP加NodePort的方式访问service，而且本地telnet也不通，但是端口确确实实在那。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题状态&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;已解决&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其实这不是问题，是因为从上面的操作记录中我们可以看到，&lt;strong&gt;在启动Nginx的Pod&lt;/strong&gt;时，指定port为80即可。以ClusterIP + Port的方式访问serivce需要等一段时间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;反思&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个问题困扰了我们差不多两天时间，出现这个问题的根源还是因为&lt;u&gt;思想观念没有从运行docker的命令中解放出来&lt;/u&gt;,还把&lt;code&gt;kubelet run —port&lt;/code&gt;当成是docker run中的端口映射，这种想法是大错特错的，该端口是image中的应用实际暴露的端口，如nginx的80端口。😔&lt;/p&gt;

&lt;h3 id=&#34;问题二&#34;&gt;问题二&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在没有删除service和deploy的情况下就重启kubelet的时候，会遇到kubelet启动失败的情况。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;出错信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apr 01 14:24:08 sz-pg-oam-docker-test-001.tendcloud.com kubelet[103932]: I0401 14:24:08.359839  103932 kubelet.go:1752] skipping pod synchronization - [Failed to start ContainerManager failed to initialise top level QOS containers: failed to create top level Burstable QOS cgroup : Unit kubepods-burstable.slice already exists.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://www.osbaike.net/article-show-id-229028.html&#34;&gt;Kubernetes Resource QoS机制解读&lt;/a&gt;，这篇文章详细介绍了QoS的机制。&lt;/p&gt;

&lt;p&gt;Kubernetes根据Pod中Containers Resource的&lt;code&gt;request&lt;/code&gt;和&lt;code&gt;limit&lt;/code&gt;的值来定义Pod的QoS Class。&lt;/p&gt;

&lt;p&gt;对于每一种Resource都可以将容器分为3中QoS Classes: Guaranteed, Burstable, and Best-Effort，它们的QoS级别依次递减。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guaranteed&lt;/strong&gt;：如果Pod中所有Container的所有Resource的&lt;code&gt;limit&lt;/code&gt;和&lt;code&gt;request&lt;/code&gt;都相等且不为0，则这个Pod的QoS Class就是Guaranteed。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Burstable&lt;/strong&gt;：除了符合Guaranteed和Best-Effort的场景，其他场景的Pod QoS Class都属于Burstable。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-Effort&lt;/strong&gt;：如果Pod中所有容器的所有Resource的request和limit都没有赋值，则这个Pod的QoS Class就是Best-Effort。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个暂时还没找到根本的解决办法，参考Github上的&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43856&#34;&gt;Failed to start ContainerManager failed to initialize top level QOS containers #43856&lt;/a&gt;，重启主机后确实正常了，不过这只是临时解决方法。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;其实昨天就已经安装完毕了，是我们使用的姿势不对，白白耽误这么长时间，身边差个老司机啊，滴～学生卡。&lt;/p&gt;

&lt;p&gt;感谢&lt;a href=&#34;tonybai.com&#34;&gt;Tony Bai&lt;/a&gt;、&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;的大力支持。&lt;/p&gt;

&lt;p&gt;Apr 1,2017 愚人节，东直门&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014082501.jpg&#34; alt=&#34;圆明园&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：北京圆明园 Aug 25,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;作者：&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;Jimmy Song&lt;/a&gt;，&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;，2017年3月30日&lt;/p&gt;

&lt;p&gt;最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是&lt;a href=&#34;https://www.cncf.io/&#34;&gt;CNCF&lt;/a&gt;的成员之一。&lt;/p&gt;

&lt;p&gt;这篇是根据&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/#prerequisites&#34;&gt;官方安装文档&lt;/a&gt;实践整理的，操作系统是纯净的CentOS7.2。&lt;/p&gt;

&lt;p&gt;另外还有一个Peter Ma写的&lt;a href=&#34;https://godliness.github.io/2017/03/29/%E5%9C%A8CentOS7%E4%B8%8A%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Kubernetes/&#34;&gt;在CentOS上手动安装kubernetes的文档&lt;/a&gt;可以参考。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;角色分配&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面以在三台主机上安装Kubernetes为例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel
172.20.0.114 node kubectl kube-proxy flannel
172.20.0.115 node kubectl kube-proxy flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一台主机既作为master也作为node。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;系统环境&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Centos 7.2.1511&lt;/li&gt;
&lt;li&gt;docker 1.12.6&lt;/li&gt;
&lt;li&gt;etcd 3.1.5&lt;/li&gt;
&lt;li&gt;kubernetes 1.6.0&lt;/li&gt;
&lt;li&gt;flannel 0.7.0-1&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;下面给出两种安装方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。&lt;/li&gt;
&lt;li&gt;使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们最终选择使用第二种方式安装。&lt;/p&gt;

&lt;p&gt;本文的很多安装步骤和命令是参考的Kubernetes官网&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/&#34;&gt;CentOS Manual Config&lt;/a&gt;文档。&lt;/p&gt;

&lt;h2 id=&#34;第一种方式-centos系统中直接使用yum安装&#34;&gt;第一种方式：CentOS系统中直接使用yum安装&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;给yum源增加一个Repo&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[virt7-docker-common-release]
name=virt7-docker-common-release
baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装docker、kubernetes、etcd、flannel一步到位&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装好了之后需要修改一系列配置文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这个repo在CentOS7.3下是毫无意义的，因为CentOS官方源的extras中已经包含了Kubernetes1.5.2，如果你使用的是CentOS7.3的话，会自动下载安装Kubernetes1.5.2（Till March 30,2017）。如果你使用的是CentOS7.2的化，这个源就有用了，但是不幸的是，它会自动下载安装Kubernentes1.1。我们现在要安装目前的最新版本Kubernetes1.6，而使用的又是CentOS7.2，所以我们不使用yum安装（当前yum源支持的最高版本的kuberentes是1.5.2）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;第二种方式-使用二进制文件安装&#34;&gt;第二种方式：使用二进制文件安装&lt;/h2&gt;

&lt;p&gt;这种方式安装的话，需要自己一个一个组件的安装。&lt;/p&gt;

&lt;h3 id=&#34;安装docker&#34;&gt;安装Docker&lt;/h3&gt;

&lt;p&gt;yum localinstall ./docker-engine*&lt;/p&gt;

&lt;p&gt;将使用CentOS的&lt;strong&gt;extras&lt;/strong&gt; repo下载。&lt;/p&gt;

&lt;h3 id=&#34;关闭防火墙和selinux&#34;&gt;关闭防火墙和SELinux&lt;/h3&gt;

&lt;p&gt;这是官网上建议的，我是直接将iptables-services和firewlld卸载掉了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;setenforce 0
systemctl disable iptables-services firewalld
systemctl stop iptables-services firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;安装etcd&#34;&gt;安装etcd&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;下载二进制文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DOWNLOAD_URL=https://storage.googleapis.com/etcd  #etcd存储地址
ETCD_VER=v3.1.5  #设置etcd版本号
wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xvf etcd-${ETCD_VER}-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;部署文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将如下内容写入文件 /etc/etcd/etcd.conf 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;# [member]
ETCD_NAME=default
ETCD_DATA_DIR=&amp;quot;/var/lib/etcd/default.etcd&amp;quot;
# ETCD_WAL_DIR=&amp;quot;&amp;quot;
# ETCD_SNAPSHOT_COUNT=&amp;quot;10000&amp;quot;
# ETCD_HEARTBEAT_INTERVAL=&amp;quot;100&amp;quot;
# ETCD_ELECTION_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_LISTEN_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_MAX_SNAPSHOTS=&amp;quot;5&amp;quot;
# ETCD_MAX_WALS=&amp;quot;5&amp;quot;
# ETCD_CORS=&amp;quot;&amp;quot;
#
# [cluster]
# ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &amp;quot;test=http://...&amp;quot;
# ETCD_INITIAL_CLUSTER=&amp;quot;default=http://localhost:2380&amp;quot;
# ETCD_INITIAL_CLUSTER_STATE=&amp;quot;new&amp;quot;
# ETCD_INITIAL_CLUSTER_TOKEN=&amp;quot;etcd-cluster&amp;quot;
ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_DISCOVERY=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_SRV=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_FALLBACK=&amp;quot;proxy&amp;quot;
# ETCD_DISCOVERY_PROXY=&amp;quot;&amp;quot;
#
# [proxy]
# ETCD_PROXY=&amp;quot;off&amp;quot;
# ETCD_PROXY_FAILURE_WAIT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_REFRESH_INTERVAL=&amp;quot;30000&amp;quot;
# ETCD_PROXY_DIAL_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_PROXY_WRITE_TIMEOUT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_READ_TIMEOUT=&amp;quot;0&amp;quot;
#
# [security]
# ETCD_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_PEER_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# [logging]
# ETCD_DEBUG=&amp;quot;false&amp;quot;
# examples for -log-package-levels etcdserver=WARNING,security=DEBUG
# ETCD_LOG_PACKAGE_LEVELS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 etcd, etcdctl放入 /usr/bin/下，并将如下内容写进/usr/lib/systemd/system/etcd.service文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${ETCD_LISTEN_CLIENT_URLS}\&amp;quot;&amp;quot;
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动并校验&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;systemctl start etcd
systemctl enable etcd
systemctl status etcd
etcdctl ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;若要部署多节点集群也比较简单，只要更改etcd.conf文件以及etcd.service添加相应配置即可&lt;/p&gt;

&lt;p&gt;可以参考链接：&lt;a href=&#34;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&#34;&gt;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装flannel&#34;&gt;安装flannel&lt;/h3&gt;

&lt;p&gt;可以直接使用&lt;code&gt;yum install flannel&lt;/code&gt;安装。&lt;/p&gt;

&lt;p&gt;因为网络这块的配置比较复杂，我将在后续文章中说明。&lt;/p&gt;

&lt;h3 id=&#34;安装kubernetes&#34;&gt;安装Kubernetes&lt;/h3&gt;

&lt;p&gt;根据《Kubernetes权威指南（第二版）》中的介绍，直接使用GitHub上的release里的二进制文件安装。&lt;/p&gt;

&lt;p&gt;执行下面的命令安装。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
tar kubernetes.tar.gz
cd kubernetes
./cluster/get-kube-binaries.sh
cd server
tar xvf kubernetes-server-linux-amd64.tar.gz
rm -f *_tag *.tar
chmod 755 *
mv * /usr/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际下载kubernetes-server-linux-amd64.tar.gz from &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;解压完后获得的二进制文件有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cloud-controller-manager
hyperkube
kubeadm
kube-aggregator
kube-apiserver
kube-controller-manager
kubectl
kubefed
kubelet
kube-proxy
kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;cluster/juju/layers/kubernetes-master/templates&lt;/code&gt;目录下有service和环境变量配置文件的模板，这个模板本来是为了使用&lt;a href=&#34;https://jujucharms.com/&#34;&gt;juju&lt;/a&gt;安装写的。&lt;/p&gt;

&lt;h4 id=&#34;master节点的配置&#34;&gt;Master节点的配置&lt;/h4&gt;

&lt;p&gt;Master节点需要配置的kubernetes的组件有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;配置kube-apiserver&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-apiserver.service&lt;/code&gt;文件。&lt;a href=&#34;http://blog.csdn.net/yuesichiu/article/details/51485147&#34;&gt;CentOS中的service配置文件参考&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_ETCD_SERVERS \
	    $KUBE_API_ADDRESS \
	    $KUBE_API_PORT \
	    $KUBELET_PORT \
	    $KUBE_ALLOW_PRIV \
	    $KUBE_SERVICE_ADDRESSES \
	    $KUBE_ADMISSION_CONTROL \
	    $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建kubernetes的配置文件目录&lt;code&gt;/etc/kubernetes&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;添加&lt;code&gt;config&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=false&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&amp;quot;--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加&lt;code&gt;apiserver&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS=&amp;quot;--address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## The port on the local server to listen on.
KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=http://127.0.0.1:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDREKUBELET_POD_INFRA_CONTAINERSSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&amp;quot;
#
## Add your own!
#KUBE_API_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;—admission-control&lt;/code&gt;参数是Kubernetes的安全机制配置，这些安全机制都是以插件的形式用来对API Serve进行准入控制，一开始我们没有配置&lt;code&gt;ServiceAccount&lt;/code&gt;，这是为了方便集群之间的通信，不需要进行身份验证。如果你需要更高级的身份验证和鉴权的话就需要加上它了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;配置kube-controller-manager&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-controller.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;controller-manager&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-scheduler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-scheduler.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;scheduler&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-proxy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-proxy.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;proxy&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kubelet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kubelet.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBELET_API_SERVER \
	    $KUBELET_ADDRESS \
	    $KUBELET_PORT \
	    $KUBELET_HOSTNAME \
	    $KUBE_ALLOW_PRIV \
	    $KUBELET_POD_INFRA_CONTAINER \
	    $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;kubelet&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=0.0.0.0&amp;quot;
#
## The port for the info server to serve on
KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;⚠️&lt;code&gt;KUBELET_POD_INFRA_CONTAINER&lt;/code&gt;在生产环境中配置成自己私有仓库里的image。&lt;/p&gt;

&lt;h4 id=&#34;node节点配置&#34;&gt;Node节点配置&lt;/h4&gt;

&lt;p&gt;Node节点需要配置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt;的配置与master节点的kube-proxy配置相同。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt;的配置需要修改&lt;code&gt;KUBELET_HOST&lt;/code&gt;为本机的hostname，其它配置相同。&lt;/p&gt;

&lt;h1 id=&#34;启动&#34;&gt;启动&lt;/h1&gt;

&lt;p&gt;在&lt;strong&gt;Master&lt;/strong&gt;节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另外两台Node节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;在Master节点上运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get all
NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   10.254.0.1   &amp;lt;none&amp;gt;        443/TCP   1h
$kubectl get nodes
NAME                                      STATUS    AGE       VERSION
sz-pg-oam-docker-test-001.tendcloud.com   Ready     7m        v1.6.0
sz-pg-oam-docker-test-002.tendcloud.com   Ready     4m        v1.6.0
sz-pg-oam-docker-test-003.tendcloud.com   Ready     10s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以正常使用啦。&lt;/p&gt;

&lt;h3 id=&#34;后记&#34;&gt;后记&lt;/h3&gt;

&lt;p&gt;另外Kuberntes还提供第三中安装方式，请看Tony Bai写的&lt;a href=&#34;http://tonybai.com/2017/01/24/explore-kubernetes-cluster-installed-by-kubeadm/&#34;&gt;使用Kubeadm方式安装Kubernetes集群的探索&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;时隔一年重新捡起kubernetes，正好现在KubeCon正在德国柏林举行，IDC 发布的报告显示，2017年大数据全球市场规模将达324亿美元，年复合增长率为27%，其中市场增长最快的领域是数据存储领域（53.4%）&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践》读后感</title>
      <link>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</link>
      <pubDate>Wed, 01 Mar 2017 18:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</guid>
      <description>&lt;p&gt;最近友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，&lt;a href=&#34;http://olz1di9xf.bkt.clouddn.com/docs/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E8%B7%B5%E7%AC%AC2%E7%89%88.pdf&#34;&gt;点这里下载&lt;/a&gt;，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。&lt;/p&gt;

&lt;p&gt;后续我将会陆续分享这本书的读后感。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>