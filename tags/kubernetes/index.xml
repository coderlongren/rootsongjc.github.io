<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Jimmy&#39;s blog</title>
    <link>http://rootsongjc.github.io/tags/kubernetes/index.xml</link>
    <description>Recent content in Kubernetes on Jimmy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://rootsongjc.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014100402.jpg&#34; alt=&#34;西安鼓楼&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：西安鼓楼 Oct 4,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;书接上文&lt;a href=&#34;http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/&#34;&gt;在CentOS中安装Kubernetes详细指南&lt;/a&gt;，这是一个系列文章，作为学习Kubernetes的心路历程吧。&lt;/p&gt;

&lt;p&gt;本文主要讲解&lt;strong&gt;Kubernetes的网络配置&lt;/strong&gt;，👆文中有一个安装&lt;strong&gt;Flannel&lt;/strong&gt;的步骤，但是安装好后并没有相应的配置说明。&lt;/p&gt;

&lt;h2 id=&#34;配置flannel&#34;&gt;配置flannel&lt;/h2&gt;

&lt;p&gt;我们直接使用的yum安装的flannle，安装好后会生成&lt;code&gt;/usr/lib/systemd/system/flanneld.service&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到flannel环境变量配置文件在&lt;code&gt;/etc/sysconfig/flanneld&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot;

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot;

# Any additional options that you want to pass
#FLANNEL_OPTIONS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;etcd的地址&lt;code&gt;FLANNEL_ETCD_ENDPOINT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;etcd查询的目录，包含docker的IP地址段配置。&lt;code&gt;FLANNEL_ETCD_PREFIX&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;在etcd中创建网络配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;执行下面的命令为docker分配IP地址段。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;etcdctl mkdir /kube-centos/network
etcdctl mk /kube-centos/network/config &amp;quot;{ \&amp;quot;Network\&amp;quot;: \&amp;quot;172.30.0.0/16\&amp;quot;, \&amp;quot;SubnetLen\&amp;quot;: 24, \&amp;quot;Backend\&amp;quot;: { \&amp;quot;Type\&amp;quot;: \&amp;quot;vxlan\&amp;quot; } }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Flannel的&lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/running.md&#34;&gt;文档&lt;/a&gt;中有写&lt;strong&gt;Docker Integration&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;Docker daemon accepts &lt;code&gt;--bip&lt;/code&gt; argument to configure the subnet of the docker0 bridge. It also accepts &lt;code&gt;--mtu&lt;/code&gt; to set the MTU for docker0 and veth devices that it will be creating. Since flannel writes out the acquired subnet and MTU values into a file, the script starting Docker can source in the values and pass them to Docker daemon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source /run/flannel/subnet.env
docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Systemd users can use &lt;code&gt;EnvironmentFile&lt;/code&gt; directive in the .service file to pull in &lt;code&gt;/run/flannel/subnet.env&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;下载flannel github release中的tar包，解压后会获得一个&lt;strong&gt;mk-docker-opts.sh&lt;/strong&gt;文件。&lt;/p&gt;

&lt;p&gt;这个文件是用来&lt;code&gt;Generate Docker daemon options based on flannel env file&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;执行&lt;code&gt;./mk-docker-opts.sh -i&lt;/code&gt;将会生成如下两个文件环境变量文件。&lt;/p&gt;

&lt;p&gt;/run/flannel/subnet.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FLANNEL_NETWORK=172.30.0.0/16
FLANNEL_SUBNET=172.30.46.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/run/docker_opts.env&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_OPT_BIP=&amp;quot;--bip=172.30.46.1/24&amp;quot;
DOCKER_OPT_IPMASQ=&amp;quot;--ip-masq=true&amp;quot;
DOCKER_OPT_MTU=&amp;quot;--mtu=1450&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在查询etcd中的内容可以看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$etcdctl ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.14.0-24
/kube-centos/network/subnets/172.30.38.0-24
/kube-centos/network/subnets/172.30.46.0-24
$etcdctl get /kube-centos/network/config
{ &amp;quot;Network&amp;quot;: &amp;quot;172.30.0.0/16&amp;quot;, &amp;quot;SubnetLen&amp;quot;: 24, &amp;quot;Backend&amp;quot;: { &amp;quot;Type&amp;quot;: &amp;quot;vxlan&amp;quot; } }
$etcdctl get /kube-centos/network/subnets/172.30.14.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.114&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;56:27:7d:1c:08:22&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.38.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.115&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;12:82:83:59:cf:b8&amp;quot;}}
$etcdctl get /kube-centos/network/subnets/172.30.46.0-24
{&amp;quot;PublicIP&amp;quot;:&amp;quot;172.20.0.113&amp;quot;,&amp;quot;BackendType&amp;quot;:&amp;quot;vxlan&amp;quot;,&amp;quot;BackendData&amp;quot;:{&amp;quot;VtepMAC&amp;quot;:&amp;quot;e6:b2:fd:f6:66:96&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;设置docker0网桥的IP地址&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;source /run/flannel/subnet.env
ifconfig docker0 $FLANNEL_SUBNET
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样docker0和flannel网桥会在同一个子网中，如&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;6: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:da:bf:83:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.1/24 brd 172.30.38.255 scope global docker0
       valid_lft forever preferred_lft forever
7: flannel.1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc noqueue state UNKNOWN 
    link/ether 9a:29:46:61:03:44 brd ff:ff:ff:ff:ff:ff
    inet 172.30.38.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在就可以重启docker了。&lt;/p&gt;

&lt;p&gt;重启了docker后还要重启kubelet，这时又遇到问题，kubelet启动失败。报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mar 31 16:44:41 sz-pg-oam-docker-test-002.tendcloud.com kubelet[81047]: error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: &amp;quot;cgroupfs&amp;quot; is different from docker cgroup driver: &amp;quot;systemd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是kubelet与docker的&lt;strong&gt;cgroup driver&lt;/strong&gt;不一致导致的，kubelet启动的时候有个&lt;code&gt;—cgroup-driver&lt;/code&gt;参数可以指定为&amp;rdquo;cgroupfs&amp;rdquo;或者“systemd”。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--cgroup-driver string                                    Driver that the kubelet uses to manipulate cgroups on the host.  Possible values: &#39;cgroupfs&#39;, &#39;systemd&#39; (default &amp;quot;cgroupfs&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动flannel&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;systemctl daemon-reload
systemctl start flanneld
systemctl status flanneld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重新登录这三台主机，可以看到每台主机都多了一个IP。&lt;/p&gt;

&lt;p&gt;参考Kubernetes官方文档的&lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/&#34;&gt;Exposing an External IP Address to Access an Application in a Cluster&lt;/a&gt;，官方使用的Hello World测试，我们启动Nginx服务测试。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;#启动nginx的pod
kubectl run nginx --replicas=2 --labels=&amp;quot;run=load-balancer-example&amp;quot; --image=sz-pg-oam-docker-hub-001.tendcloud.com/library/nginx:1.9  --port=80
#创建名为example-service的服务
kubectl expose deployment nginx --type=NodePort --name=example-service
#查看状态
kubectl get deployments nginx
kubectl describe deployments nginx
kubectl get replicasets
kubectl describe replicasets
kubectl describe svc example-service
###################################################
Name:			example-service
Namespace:		default
Labels:			run=load-balancer-example
Annotations:		&amp;lt;none&amp;gt;
Selector:		run=load-balancer-example
Type:			NodePort
IP:			10.254.180.209
Port:			&amp;lt;unset&amp;gt;	80/TCP
NodePort:		&amp;lt;unset&amp;gt;	32663/TCP
Endpoints:		172.30.14.2:80,172.30.46.2:80
Session Affinity:	None
Events:			&amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们上面启动的serivce的type是&lt;strong&gt;NodePort&lt;/strong&gt;，Kubernetes的service支持三种类型的service，参考&lt;a href=&#34;http://www.cnblogs.com/xuxinkun/p/5331728.html&#34;&gt;Kubernetes Serivce分析&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;现在访问三台物理机的IP:80端口就可以看到nginx的页面了。&lt;/p&gt;

&lt;p&gt;稍等一会在访问ClusterIP + Port也可以访问到nginx。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$curl 10.254.180.209:80
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a href=&amp;quot;http://nginx.org/&amp;quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a href=&amp;quot;http://nginx.com/&amp;quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;虚拟地址&#34;&gt;虚拟地址&lt;/h2&gt;

&lt;p&gt;Kubernetes中的Service了使用了虚拟地址；该地址无法ping通过，但可以访问其端口。通过下面的命令可以看到，该虚拟地址是若干条iptables的规则。到10.254.124.145:8080端口的请求会被重定向到172.30.38.2或172.30.46.2的8080端口。这些规则是由kube-proxy生成；如果需要某台机器可以访问Service，则需要在该主机启动kube-proxy。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看service的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables-save|grep example-service
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp --dport 32663 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SEP-NCPBOLUH5XTTHG3E -s 172.30.46.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-NCPBOLUH5XTTHG3E -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.46.2:80
-A KUBE-SEP-ONEKQBIWICF7RAR3 -s 172.30.14.2/32 -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-ONEKQBIWICF7RAR3 -p tcp -m comment --comment &amp;quot;default/example-service:&amp;quot; -m tcp -j DNAT --to-destination 172.30.14.2:80
-A KUBE-SERVICES -d 10.254.180.209/32 -p tcp -m comment --comment &amp;quot;default/example-service: cluster IP&amp;quot; -m tcp --dport 80 -j KUBE-SVC-BR4KARPIGKMRMN3E
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ONEKQBIWICF7RAR3
-A KUBE-SVC-BR4KARPIGKMRMN3E -m comment --comment &amp;quot;default/example-service:&amp;quot; -j KUBE-SEP-NCPBOLUH5XTTHG3E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看clusterIP的iptables&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$iptables -t nat -nL|grep 10.254
KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0/0            10.254.0.1           /* default/kubernetes:https cluster IP */ tcp dpt:443
KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到在PREROUTING环节，k8s设置了一个target: KUBE-SERVICES。而KUBE-SERVICES下面又设置了许多target，一旦destination和dstport匹配，就会沿着chain进行处理。&lt;/p&gt;

&lt;p&gt;比如：当我们在pod网络curl 10.254.198.44 80时，匹配到下面的KUBE-SVC-BR4KARPIGKMRMN3E target：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBE-SVC-BR4KARPIGKMRMN3E  tcp  --  0.0.0.0/0            10.254.180.209       /* default/example-service: cluster IP */ tcp dpt:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考&lt;a href=&#34;http://tonybai.com/2017/01/17/understanding-flannel-network-for-kubernetes/&#34;&gt;理解Kubernetes网络之Flannel网络&lt;/a&gt;，Tony Bai的文章中有对flannel的详细介绍。&lt;/p&gt;

&lt;h2 id=&#34;遇到的问题&#34;&gt;遇到的问题&lt;/h2&gt;

&lt;p&gt;在设置网络的过程中遇到了很多问题，记录如下。&lt;/p&gt;

&lt;h3 id=&#34;问题一&#34;&gt;问题一&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kube-proxy开放的&lt;strong&gt;NodePort&lt;/strong&gt;端口无法访问。即无法使用NodeIP加NodePort的方式访问service，而且本地telnet也不通，但是端口确确实实在那。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题状态&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;已解决&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其实这不是问题，是因为从上面的操作记录中我们可以看到，&lt;strong&gt;在启动Nginx的Pod&lt;/strong&gt;时，指定port为80即可。以ClusterIP + Port的方式访问serivce需要等一段时间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;反思&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个问题困扰了我们差不多两天时间，出现这个问题的根源还是因为&lt;u&gt;思想观念没有从运行docker的命令中解放出来&lt;/u&gt;,还把&lt;code&gt;kubelet run —port&lt;/code&gt;当成是docker run中的端口映射，这种想法是大错特错的，该端口是image中的应用实际暴露的端口，如nginx的80端口。😔&lt;/p&gt;

&lt;h3 id=&#34;问题二&#34;&gt;问题二&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题描述&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在没有删除service和deploy的情况下就重启kubelet的时候，会遇到kubelet启动失败的情况。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;出错信息&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apr 01 14:24:08 sz-pg-oam-docker-test-001.tendcloud.com kubelet[103932]: I0401 14:24:08.359839  103932 kubelet.go:1752] skipping pod synchronization - [Failed to start ContainerManager failed to initialise top level QOS containers: failed to create top level Burstable QOS cgroup : Unit kubepods-burstable.slice already exists.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://www.osbaike.net/article-show-id-229028.html&#34;&gt;Kubernetes Resource QoS机制解读&lt;/a&gt;，这篇文章详细介绍了QoS的机制。&lt;/p&gt;

&lt;p&gt;Kubernetes根据Pod中Containers Resource的&lt;code&gt;request&lt;/code&gt;和&lt;code&gt;limit&lt;/code&gt;的值来定义Pod的QoS Class。&lt;/p&gt;

&lt;p&gt;对于每一种Resource都可以将容器分为3中QoS Classes: Guaranteed, Burstable, and Best-Effort，它们的QoS级别依次递减。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guaranteed&lt;/strong&gt;：如果Pod中所有Container的所有Resource的&lt;code&gt;limit&lt;/code&gt;和&lt;code&gt;request&lt;/code&gt;都相等且不为0，则这个Pod的QoS Class就是Guaranteed。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Burstable&lt;/strong&gt;：除了符合Guaranteed和Best-Effort的场景，其他场景的Pod QoS Class都属于Burstable。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-Effort&lt;/strong&gt;：如果Pod中所有容器的所有Resource的request和limit都没有赋值，则这个Pod的QoS Class就是Best-Effort。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;解决方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个暂时还没找到根本的解决办法，参考Github上的&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43856&#34;&gt;Failed to start ContainerManager failed to initialize top level QOS containers #43856&lt;/a&gt;，重启主机后确实正常了，不过这只是临时解决方法。&lt;/p&gt;

&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;

&lt;p&gt;其实昨天就已经安装完毕了，是我们使用的姿势不对，白白耽误这么长时间，身边差个老司机啊，滴～学生卡。&lt;/p&gt;

&lt;p&gt;感谢&lt;a href=&#34;tonybai.com&#34;&gt;Tony Bai&lt;/a&gt;、&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;的大力支持。&lt;/p&gt;

&lt;p&gt;Apr 1,2017 愚人节，东直门&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2014082501.jpg&#34; alt=&#34;圆明园&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：北京圆明园 Aug 25,2014）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;作者：&lt;a href=&#34;rootsongjc.github.io/about&#34;&gt;Jimmy Song&lt;/a&gt;，&lt;a href=&#34;https://godliness.github.io/&#34;&gt;Peter Ma&lt;/a&gt;，2017年3月30日&lt;/p&gt;

&lt;p&gt;最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是&lt;a href=&#34;https://www.cncf.io/&#34;&gt;CNCF&lt;/a&gt;的成员之一。&lt;/p&gt;

&lt;p&gt;这篇是根据&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/#prerequisites&#34;&gt;官方安装文档&lt;/a&gt;实践整理的，操作系统是纯净的CentOS7.2。&lt;/p&gt;

&lt;p&gt;另外还有一个Peter Ma写的&lt;a href=&#34;https://godliness.github.io/2017/03/29/%E5%9C%A8CentOS7%E4%B8%8A%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Kubernetes/&#34;&gt;在CentOS上手动安装kubernetes的文档&lt;/a&gt;可以参考。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;角色分配&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;下面以在三台主机上安装Kubernetes为例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel
172.20.0.114 node kubectl kube-proxy flannel
172.20.0.115 node kubectl kube-proxy flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一台主机既作为master也作为node。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;系统环境&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Centos 7.2.1511&lt;/li&gt;
&lt;li&gt;docker 1.12.6&lt;/li&gt;
&lt;li&gt;etcd 3.1.5&lt;/li&gt;
&lt;li&gt;kubernetes 1.6.0&lt;/li&gt;
&lt;li&gt;flannel 0.7.0-1&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;安装&#34;&gt;安装&lt;/h1&gt;

&lt;p&gt;下面给出两种安装方式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。&lt;/li&gt;
&lt;li&gt;使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们最终选择使用第二种方式安装。&lt;/p&gt;

&lt;p&gt;本文的很多安装步骤和命令是参考的Kubernetes官网&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/&#34;&gt;CentOS Manual Config&lt;/a&gt;文档。&lt;/p&gt;

&lt;h2 id=&#34;第一种方式-centos系统中直接使用yum安装&#34;&gt;第一种方式：CentOS系统中直接使用yum安装&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;给yum源增加一个Repo&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[virt7-docker-common-release]
name=virt7-docker-common-release
baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/
gpgcheck=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;安装docker、kubernetes、etcd、flannel一步到位&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装好了之后需要修改一系列配置文件。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;这个repo在CentOS7.3下是毫无意义的，因为CentOS官方源的extras中已经包含了Kubernetes1.5.2，如果你使用的是CentOS7.3的话，会自动下载安装Kubernetes1.5.2（Till March 30,2017）。如果你使用的是CentOS7.2的化，这个源就有用了，但是不幸的是，它会自动下载安装Kubernentes1.1。我们现在要安装目前的最新版本Kubernetes1.6，而使用的又是CentOS7.2，所以我们不使用yum安装（当前yum源支持的最高版本的kuberentes是1.5.2）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;第二种方式-使用二进制文件安装&#34;&gt;第二种方式：使用二进制文件安装&lt;/h2&gt;

&lt;p&gt;这种方式安装的话，需要自己一个一个组件的安装。&lt;/p&gt;

&lt;h3 id=&#34;安装docker&#34;&gt;安装Docker&lt;/h3&gt;

&lt;p&gt;yum localinstall ./docker-engine*&lt;/p&gt;

&lt;p&gt;将使用CentOS的&lt;strong&gt;extras&lt;/strong&gt; repo下载。&lt;/p&gt;

&lt;h3 id=&#34;关闭防火墙和selinux&#34;&gt;关闭防火墙和SELinux&lt;/h3&gt;

&lt;p&gt;这是官网上建议的，我是直接将iptables-services和firewlld卸载掉了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;setenforce 0
systemctl disable iptables-services firewalld
systemctl stop iptables-services firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;安装etcd&#34;&gt;安装etcd&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;下载二进制文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DOWNLOAD_URL=https://storage.googleapis.com/etcd  #etcd存储地址
ETCD_VER=v3.1.5  #设置etcd版本号
wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xvf etcd-${ETCD_VER}-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;部署文件&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;将如下内容写入文件 /etc/etcd/etcd.conf 中：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;# [member]
ETCD_NAME=default
ETCD_DATA_DIR=&amp;quot;/var/lib/etcd/default.etcd&amp;quot;
# ETCD_WAL_DIR=&amp;quot;&amp;quot;
# ETCD_SNAPSHOT_COUNT=&amp;quot;10000&amp;quot;
# ETCD_HEARTBEAT_INTERVAL=&amp;quot;100&amp;quot;
# ETCD_ELECTION_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_LISTEN_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
ETCD_LISTEN_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_MAX_SNAPSHOTS=&amp;quot;5&amp;quot;
# ETCD_MAX_WALS=&amp;quot;5&amp;quot;
# ETCD_CORS=&amp;quot;&amp;quot;
#
# [cluster]
# ETCD_INITIAL_ADVERTISE_PEER_URLS=&amp;quot;http://localhost:2380&amp;quot;
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &amp;quot;test=http://...&amp;quot;
# ETCD_INITIAL_CLUSTER=&amp;quot;default=http://localhost:2380&amp;quot;
# ETCD_INITIAL_CLUSTER_STATE=&amp;quot;new&amp;quot;
# ETCD_INITIAL_CLUSTER_TOKEN=&amp;quot;etcd-cluster&amp;quot;
ETCD_ADVERTISE_CLIENT_URLS=&amp;quot;http://0.0.0.0:2379&amp;quot;
# ETCD_DISCOVERY=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_SRV=&amp;quot;&amp;quot;
# ETCD_DISCOVERY_FALLBACK=&amp;quot;proxy&amp;quot;
# ETCD_DISCOVERY_PROXY=&amp;quot;&amp;quot;
#
# [proxy]
# ETCD_PROXY=&amp;quot;off&amp;quot;
# ETCD_PROXY_FAILURE_WAIT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_REFRESH_INTERVAL=&amp;quot;30000&amp;quot;
# ETCD_PROXY_DIAL_TIMEOUT=&amp;quot;1000&amp;quot;
# ETCD_PROXY_WRITE_TIMEOUT=&amp;quot;5000&amp;quot;
# ETCD_PROXY_READ_TIMEOUT=&amp;quot;0&amp;quot;
#
# [security]
# ETCD_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CERT_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_KEY_FILE=&amp;quot;&amp;quot;
# ETCD_PEER_CLIENT_CERT_AUTH=&amp;quot;false&amp;quot;
# ETCD_PEER_TRUSTED_CA_FILE=&amp;quot;&amp;quot;
# [logging]
# ETCD_DEBUG=&amp;quot;false&amp;quot;
# examples for -log-package-levels etcdserver=WARNING,security=DEBUG
# ETCD_LOG_PACKAGE_LEVELS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 etcd, etcdctl放入 /usr/bin/下，并将如下内容写进/usr/lib/systemd/system/etcd.service文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c &amp;quot;GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\&amp;quot;${ETCD_NAME}\&amp;quot; --data-dir=\&amp;quot;${ETCD_DATA_DIR}\&amp;quot; --listen-client-urls=\&amp;quot;${ETCD_LISTEN_CLIENT_URLS}\&amp;quot;&amp;quot;
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;启动并校验&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;systemctl start etcd
systemctl enable etcd
systemctl status etcd
etcdctl ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;集群&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;若要部署多节点集群也比较简单，只要更改etcd.conf文件以及etcd.service添加相应配置即可&lt;/p&gt;

&lt;p&gt;可以参考链接：&lt;a href=&#34;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&#34;&gt;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装flannel&#34;&gt;安装flannel&lt;/h3&gt;

&lt;p&gt;可以直接使用&lt;code&gt;yum install flannel&lt;/code&gt;安装。&lt;/p&gt;

&lt;p&gt;因为网络这块的配置比较复杂，我将在后续文章中说明。&lt;/p&gt;

&lt;h3 id=&#34;安装kubernetes&#34;&gt;安装Kubernetes&lt;/h3&gt;

&lt;p&gt;根据《Kubernetes权威指南（第二版）》中的介绍，直接使用GitHub上的release里的二进制文件安装。&lt;/p&gt;

&lt;p&gt;执行下面的命令安装。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz
tar kubernetes.tar.gz
cd kubernetes
./cluster/get-kube-binaries.sh
cd server
tar xvf kubernetes-server-linux-amd64.tar.gz
rm -f *_tag *.tar
chmod 755 *
mv * /usr/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际下载kubernetes-server-linux-amd64.tar.gz from &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.6.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;解压完后获得的二进制文件有：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cloud-controller-manager
hyperkube
kubeadm
kube-aggregator
kube-apiserver
kube-controller-manager
kubectl
kubefed
kubelet
kube-proxy
kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;cluster/juju/layers/kubernetes-master/templates&lt;/code&gt;目录下有service和环境变量配置文件的模板，这个模板本来是为了使用&lt;a href=&#34;https://jujucharms.com/&#34;&gt;juju&lt;/a&gt;安装写的。&lt;/p&gt;

&lt;h4 id=&#34;master节点的配置&#34;&gt;Master节点的配置&lt;/h4&gt;

&lt;p&gt;Master节点需要配置的kubernetes的组件有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;kube-controller-manager&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;配置kube-apiserver&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-apiserver.service&lt;/code&gt;文件。&lt;a href=&#34;http://blog.csdn.net/yuesichiu/article/details/51485147&#34;&gt;CentOS中的service配置文件参考&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_ETCD_SERVERS \
	    $KUBE_API_ADDRESS \
	    $KUBE_API_PORT \
	    $KUBELET_PORT \
	    $KUBE_ALLOW_PRIV \
	    $KUBE_SERVICE_ADDRESSES \
	    $KUBE_ADMISSION_CONTROL \
	    $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建kubernetes的配置文件目录&lt;code&gt;/etc/kubernetes&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;添加&lt;code&gt;config&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR=&amp;quot;--logtostderr=true&amp;quot;

# journal message level, 0 is debug
KUBE_LOG_LEVEL=&amp;quot;--v=0&amp;quot;

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV=&amp;quot;--allow-privileged=false&amp;quot;

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER=&amp;quot;--master=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加&lt;code&gt;apiserver&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
KUBE_API_ADDRESS=&amp;quot;--address=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## The port on the local server to listen on.
KUBE_API_PORT=&amp;quot;--port=8080&amp;quot;
#
## Port minions listen on
KUBELET_PORT=&amp;quot;--kubelet-port=10250&amp;quot;
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS=&amp;quot;--etcd-servers=http://127.0.0.1:2379&amp;quot;
#
## Address range to use for services
KUBE_SERVICE_ADDREKUBELET_POD_INFRA_CONTAINERSSES=&amp;quot;--service-cluster-ip-range=10.254.0.0/16&amp;quot;
#
## default admission control policies
KUBE_ADMISSION_CONTROL=&amp;quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&amp;quot;
#
## Add your own!
#KUBE_API_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-controller-manager&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-controller.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Ini&#34;&gt;Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;controller-manager&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-scheduler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-scheduler.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;scheduler&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kube-proxy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kube-proxy.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBE_MASTER \
	    $KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;proxy&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置kubelet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编写&lt;code&gt;/usr/lib/systemd/system/kubelet.service&lt;/code&gt;文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \
	    $KUBE_LOGTOSTDERR \
	    $KUBE_LOG_LEVEL \
	    $KUBELET_API_SERVER \
	    $KUBELET_ADDRESS \
	    $KUBELET_PORT \
	    $KUBELET_HOSTNAME \
	    $KUBE_ALLOW_PRIV \
	    $KUBELET_POD_INFRA_CONTAINER \
	    $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;/etc/kubernetes&lt;/code&gt;目录下添加&lt;code&gt;kubelet&lt;/code&gt;配置文件。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or &amp;quot;&amp;quot; for all interfaces)
KUBELET_ADDRESS=&amp;quot;--address=0.0.0.0&amp;quot;
#
## The port for the info server to serve on
KUBELET_PORT=&amp;quot;--port=10250&amp;quot;
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=&amp;quot;--hostname-override=sz-pg-oam-docker-test-001.tendcloud.com&amp;quot;
#
## location of the api-server
KUBELET_API_SERVER=&amp;quot;--api-servers=http://sz-pg-oam-docker-test-001.tendcloud.com:8080&amp;quot;
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER=&amp;quot;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&amp;quot;
#
## Add your own!
KUBELET_ARGS=&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;⚠️&lt;code&gt;KUBELET_POD_INFRA_CONTAINER&lt;/code&gt;在生产环境中配置成自己私有仓库里的image。&lt;/p&gt;

&lt;h4 id=&#34;node节点配置&#34;&gt;Node节点配置&lt;/h4&gt;

&lt;p&gt;Node节点需要配置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;kubectl&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt;的配置与master节点的kube-proxy配置相同。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt;的配置需要修改&lt;code&gt;KUBELET_HOST&lt;/code&gt;为本机的hostname，其它配置相同。&lt;/p&gt;

&lt;h1 id=&#34;启动&#34;&gt;启动&lt;/h1&gt;

&lt;p&gt;在&lt;strong&gt;Master&lt;/strong&gt;节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在另外两台Node节点上执行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;for SERVICES in kube-proxy kubelet flanneld; do
    systemctl restart $SERVICES
    systemctl enable $SERVICES
    systemctl status $SERVICES
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;验证&#34;&gt;验证&lt;/h1&gt;

&lt;p&gt;在Master节点上运行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get all
NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes   10.254.0.1   &amp;lt;none&amp;gt;        443/TCP   1h
$kubectl get nodes
NAME                                      STATUS    AGE       VERSION
sz-pg-oam-docker-test-001.tendcloud.com   Ready     7m        v1.6.0
sz-pg-oam-docker-test-002.tendcloud.com   Ready     4m        v1.6.0
sz-pg-oam-docker-test-003.tendcloud.com   Ready     10s       v1.6.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在可以正常使用啦。&lt;/p&gt;

&lt;h3 id=&#34;后记&#34;&gt;后记&lt;/h3&gt;

&lt;p&gt;另外Kuberntes还提供第三中安装方式，请看Tony Bai写的&lt;a href=&#34;http://tonybai.com/2017/01/24/explore-kubernetes-cluster-installed-by-kubeadm/&#34;&gt;使用Kubeadm方式安装Kubernetes集群的探索&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;时隔一年重新捡起kubernetes，正好现在KubeCon正在德国柏林举行，IDC 发布的报告显示，2017年大数据全球市场规模将达324亿美元，年复合增长率为27%，其中市场增长最快的领域是数据存储领域（53.4%）&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part2</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</link>
      <pubDate>Fri, 10 Mar 2017 22:06:32 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20160709044.jpg&#34; alt=&#34;承德兴隆星空&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;本文是&lt;code&gt;Docker v.s Kubernetes&lt;/code&gt;第二篇，续接上文&lt;a href=&#34;http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/&#34;&gt;Docker v.s Kuberntes Part1&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Kubernetes是典型的&lt;strong&gt;Master/Slave&lt;/strong&gt;架构模式，本文简要的介绍kubenetes的架构和组件构成。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes核心架构&#34;&gt;Kubernetes核心架构&lt;/h2&gt;

&lt;h3 id=&#34;master节点&#34;&gt;master节点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。&lt;/li&gt;
&lt;li&gt;scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。&lt;/li&gt;
&lt;li&gt;controller-manager：负责执行各种控制器，目前有两类：

&lt;ol&gt;
&lt;li&gt;endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。&lt;/li&gt;
&lt;li&gt;replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;node节点&#34;&gt;node节点&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。&lt;/li&gt;
&lt;li&gt;proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://valleylord.github.io/images/201601-kubernetes-concepts/kubernetes-masterslave.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes组件详细介绍&#34;&gt;Kubernetes组件详细介绍&lt;/h2&gt;

&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;

&lt;p&gt;虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，&lt;code&gt;CoreOS&lt;/code&gt;公司出品，使用&lt;a href=&#34;http://rootsongjc.github.io/blogs/raft/&#34;&gt;raft一致性算法&lt;/a&gt;协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的&lt;code&gt;docker swarm&lt;/code&gt;也用到了它。目前主要使用的是&lt;code&gt;2.7.x&lt;/code&gt;版本，&lt;code&gt;3.0+&lt;/code&gt;版本的API变化太大。&lt;/p&gt;

&lt;h3 id=&#34;apiserver&#34;&gt;APIServer&lt;/h3&gt;

&lt;p&gt;APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。&lt;/p&gt;

&lt;h3 id=&#34;scheduler&#34;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;Scheduler的作用是&lt;strong&gt;根据特定的调度算法将pod调度到node节点上&lt;/strong&gt;，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。&lt;/p&gt;

&lt;p&gt;工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。&lt;/p&gt;

&lt;p&gt;工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：&lt;code&gt;HostIp&lt;/code&gt;、&lt;code&gt;NodePhase&lt;/code&gt;和&lt;code&gt;Node Condition&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;工作节点的动态维护过程依靠&lt;strong&gt;Node Controller&lt;/strong&gt;来完成，它是&lt;code&gt;Kubernetes Controller Manager&lt;/code&gt;下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。&lt;/p&gt;

&lt;h3 id=&#34;controller-manager&#34;&gt;Controller Manager&lt;/h3&gt;

&lt;p&gt;Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它&lt;strong&gt;重点实现service Endpoint（服务端点）的动态更新&lt;/strong&gt;。管理着Kubernetes集群中各种控制节点，包括&lt;strong&gt;replication Controller&lt;/strong&gt;和&lt;strong&gt;node Controller&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态&lt;/strong&gt;。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。&lt;/p&gt;

&lt;h3 id=&#34;kubelet&#34;&gt;kubelet&lt;/h3&gt;

&lt;p&gt;kubelet组件工作在Kubernetes的node上，&lt;strong&gt;负责管理和维护在这台主机上运行着的所有容器&lt;/strong&gt;。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。&lt;/p&gt;

&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h3&gt;

&lt;p&gt;kube-proxy提供两种功能:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提供算法将客服端流量负载均衡到service对应的一组后端pod。&lt;/li&gt;
&lt;li&gt;使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part1</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</link>
      <pubDate>Fri, 10 Mar 2017 21:09:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/20161016031.jpg&#34; alt=&#34;杭州西湖&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;（题图：杭州西湖 Oct 16,2016）&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;前言&#34;&gt;前言&lt;/h3&gt;

&lt;p&gt;这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看&lt;strong&gt;Kunernetes权威指南（第二版）&lt;/strong&gt;这本书（P.S感谢&lt;u&gt;电子工业出版社&lt;/u&gt;的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，&lt;strong&gt;适合自己的才是最好的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;此系列文章中所说的&lt;strong&gt;docker&lt;/strong&gt;指的是*17.03-ce*版本。&lt;/p&gt;

&lt;h3 id=&#34;概念性的差别&#34;&gt;概念性的差别&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/abstractions/pod/&#34;&gt;Pod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Service&lt;/li&gt;
&lt;li&gt;Volume&lt;/li&gt;
&lt;li&gt;Namespace&lt;/li&gt;
&lt;li&gt;ReplicaSet&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/&#34;&gt;StatefulSet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DaemonSet&lt;/li&gt;
&lt;li&gt;Job&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Service&lt;/li&gt;
&lt;li&gt;Node&lt;/li&gt;
&lt;li&gt;Stack&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。&lt;/p&gt;

&lt;h3 id=&#34;功能性差别&#34;&gt;功能性差别&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。&lt;/li&gt;
&lt;li&gt;Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。&lt;/li&gt;
&lt;li&gt;Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过）&lt;/li&gt;
&lt;li&gt;Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。&lt;/li&gt;
&lt;li&gt;Kubernetes对象选择机制继续通过label selector，用于对象调度。&lt;/li&gt;
&lt;li&gt;Kubernetes中有一个比较特别的镜像，叫做&lt;code&gt;google_containers/pause&lt;/code&gt;，这个镜像是用来实现Pod概念的。&lt;/li&gt;
&lt;li&gt;HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。&lt;/li&gt;
&lt;li&gt;Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。&lt;/li&gt;
&lt;li&gt;持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用&lt;code&gt;volume plugin&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>