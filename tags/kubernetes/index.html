<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">
    <meta name="baidu-site-verification" content="g8IYR9SNLF" />
    <meta name="uyan_auth" content="419f63884b" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="Kubernetes : rootsongjc.github.io"/>
<meta property="og:site_name" content="rootsongjc is Jimmy Song"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="http://rootsongjc.github.io/tags/kubernetes/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-04-11"/>
<meta property="article:modified_time" content="2017-04-11"/>





<meta name="twitter:card" content="summary">

<meta name="twitter:site" content="@rootsongjc">
<meta name="twitter:title" content="Kubernetes : rootsongjc.github.io">
<meta name="twitter:creator" content="@rootsongjc">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="">
<meta name="twitter:domain" content="rootsongjc.github.io">

    <base href="http://rootsongjc.github.io/">
    <title> Kubernetes - Jimmy Song </title>
    <link rel="canonical" href="http://rootsongjc.github.io/tags/kubernetes/">
    <link href="http://rootsongjc.github.io/tags/kubernetes/index.xml" rel="alternate" type="application/rss+xml" title="Kubernetes" />

    
<link rel="stylesheet" href="/static/css/style.css">
<script src="https://yandex.st/highlightjs/8.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://yandex.st/highlightjs/8.0/styles/default.min.css">
<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?11f7d254cfa4e0ca44b175c66d379ecc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
    <script type="text/javascript" src="http://tajs.qq.com/stats?sId=61142324" charset="UTF-8"></script>
</head>

<body lang="en">
<header id="header">
    <figure>
      <a href="/" border=0 id="logolink"><div class="icon-octocat" id="logo"> </div></a>
    </figure>
    <div id="byline">by Jimmy Song</div>
    <nav id="nav">
            <ul id="mainnav">
            <li>
                <a href="/blogs/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> blogs </span>
            </a>
            </li>
            <li>
            <a href="/projects/">
                <span class="icon"> <i aria-hidden="true" class="icon-console"></i></span>
                <span> projects </span>
            </a>
            </li>
            <li>
            <a href="/talks/">
                <span class="icon"> <i aria-hidden="true" class="icon-stats"></i></span>
                <span> talks </span>
            </a>
            </li>
            <li>
            <a href="http://www.linkedin.com/in/rootsongjc">
                <span class="icon"> <i aria-hidden="true" class="icon-linkedin"></i></span>
                <span> me </span>
            </a>
            </li>
        </ul>

            <ul id="social">
            <li id="share">
                <span class="icon icon-bubbles"> </span>
                <span class="title"> share </span>
                <div class="dropdown share">
                    <ul class="social">
                      <li> <a href="https://twitter.com/intent/tweet?status=Kubernetes-http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="https://plus.google.com/share?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Google+" class="googleplus"><span class="icon icon-google-plus"></span>Google+</a> </li>
                        <li> <a href="http://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f&title=Kubernetes&source=spf13" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://del.icio.us/post?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Delicious" class="delicious"><span class="icon icon-delicious"></span>Delicious</a> </li>
                        <li> <a href="http://www.reddit.com/submit?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Reddit" class="reddit"><span class="icon icon-reddit"></span>Reddit</a> </li>
                    </ul>
                    <span class="subcount">sharing is caring</span>
                </div>
            </li>
            <li id="follow">
                <span class="icon icon-rocket"> </span>
                <span class="title"> follow </span>
                <div class="dropdown follow">
                    <ul class="social">
                        <li> <a href="http://www.twitter.com/rootsongjc" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="http://www.facebook.com/rootsongjc" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="http://www.linkedin.com/in/rootsongjc" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://github.com/rootsongjc" target="_blank" title="GitHub" class="github"><span class="icon icon-github"></span>GitHub</a> </li>
                        <li> <a href="https://www.douban.com/people/deamonj/" target="_blank" title="豆瓣" class="facebook"><span class="icon icon-idea"></span>Douban</a> </li>
                        <li> <a href="https://tuchong.com/1425795/" target="_blank" title="图虫" class="github"><span class="icon icon-cc-2"></span>Tuchong</a> </li>                    </ul>
                    <span class="subcount">join 10k+ subscribers &amp; followers</span>
                </div>
            </li>
          </ul>

    </nav>
</header>


<section id="main">
  <div>
   <h1 id="title">Kubernetes</h1>
    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/">Kubernetes安装之etcd高可用配置 </a> </h2>
      <div class="post-meta">Tue, Apr 11, 2017 </div>
    </header>

    （题图：北京夜景@西山）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。
创建高可用 etcd 集群 kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为sz-pg-oam-docker-test-001.tendcloud.com、sz-pg-oam-docker-test-002.tendcloud.com、sz-pg-oam-docker-test-003.tendcloud.com：
 sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113 sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114 sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115  TLS 认证文件 需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl   kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；  下载二进制文件 到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件
$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz $ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz $ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin  创建 etcd 的 systemd unit 文件 注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/">Kubernetes安装之创建kubeconfig文件 </a> </h2>
      <div class="post-meta">Tue, Apr 11, 2017 </div>
    </header>

    (题图：北海公园 May 8,2016)
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。 kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权； kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。
创建 TLS Bootstrapping Token Token auth file
Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ') cat &gt; token.csv &lt;&lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; EOF   后三行是一句，直接复制上面的脚本运行即可。
 将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。
$cp token.csv /etc/kubernetes/  创建 kubelet bootstrapping kubeconfig 文件 $ cd /etc/kubernetes $ export KUBE_APISERVER=&quot;https://172.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/">Kubernetes安装之证书验证 </a> </h2>
      <div class="post-meta">Mon, Apr 10, 2017 </div>
    </header>

    （题图：铜牛@颐和园 Aug 25,2014）
前言 昨晚（Apr 9,2017）金山软件的opsnull发布了一个开源项目和我一步步部署kubernetes集群，下文是结合我之前部署kubernetes的过程打造的kubernetes环境和opsnull的文章创建 kubernetes 各组件 TLS 加密通信的证书和秘钥的实践。之前安装过程中一直使用的是非加密方式，一直到后来使用Fluentd和ElasticSearch收集Kubernetes集群日志时发现有权限验证问题，所以为了深入研究kubernentes。
Kubernentes中的身份验证 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
生成的 CA 证书和秘钥文件如下：
 ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem  使用证书的组件如下：
 etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem；  kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。
安装 CFSSL 方式一：直接使用二进制源码包安装
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 $ chmod +x cfssl_linux-amd64 $ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl $ wget https://pkg.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/">使用Fluentd和ElasticSearch收集Kubernetes集群日志 </a> </h2>
      <div class="post-meta">Fri, Apr 7, 2017 </div>
    </header>

    （题图：码头@古北水镇 Apr 30,2016）
前言 在安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard和配置Heapster监控插件后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。
官方文档
Kubernetes Logging and Monitoring Cluster Activity
Logging Using Elasticsearch and Kibana：不过这篇文章是在GCE上配置的，参考价值不大。
容器日志的存在形式 目前容器日志有两种输出形式：
stdout,stderr标准输出
这种形式的日志输出我们可以直接使用docker logs查看日志，kubernetes集群中同样可以使用kubectl logs类似的形式查看日志。
日志文件记录
这种日志输出我们无法从以上方法查看日志内容，只能tail日志文件查看。
Fluentd介绍 Fluentd是使用Ruby编写的，通过在后端系统之间提供统一的日志记录层来从后端系统中解耦数据源。 此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。 统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。 也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。
Fluentd收集的event由以下几个方面组成：
 Tag：字符串，中间用点隔开，如myapp.access Time：UNIX时间格式 Record：JSON格式  Fluentd特点  部署简单灵活 开源 经过验证的可靠性和性能 社区支持，插件较多 使用json格式事件格式 可拔插的架构设计 低资源要求 内置高可靠性  安装 查看cluster/addons/fluentd-elasticsearch插件目录，获取到需要用到的docker镜像名称。
$grep -rn &quot;gcr.io&quot; *.yaml es-controller.yaml:24: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 fluentd-es-ds.yaml:26: image: gcr.io/google_containers/fluentd-elasticsearch:1.22 kibana-controller.yaml:22: image: gcr.io/google_containers/kibana:v4.6.1-1  需要用到的镜像
 gcr.io/google_containers/kibana:v4.6.1-1 gcr.io/google_containers/elasticsearch:v2.4.1-2 gcr.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/">Kubernetes的ConfigMap解析 </a> </h2>
      <div class="post-meta">Thu, Apr 6, 2017 </div>
    </header>

    （题图：龙形灯笼@古北水镇 Apr 30,2016）
前言 为什么要翻译这篇文章，是因为我在使用Fluentd和ElasticSearch收集Kubernetes集群日志的时候遇到了需要修改镜像中配置的问题，fluent-plugin-kubernetes_metadata里的需要的td-agent.conf文件。
其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
ConfigMap概览 The ConfigMap API resource holds key-value pairs of configuration data that can be consumed in pods or used to store configuration data for system components such as controllers. ConfigMap is similar to [Secrets](), but designed to more conveniently support working with strings that do not contain sensitive information.
ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。
Note: ConfigMaps are not intended to act as a replacement for a properties file.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/">Kubernetes heapster监控插件安装文档 </a> </h2>
      <div class="post-meta">Wed, Apr 5, 2017 </div>
    </header>

    （题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）
前言 前面几篇文章中记录了我们安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard，但是还没法查看Pod的监控信息，虽然kubelet默认集成了cAdvisor（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。
安装 下载heapster的代码
直接现在Github上的最新代码。
git pull https://github.com/kubernetes/heapster.git  目前的最高版本是1.3.0。
在heapster/deploy/kube-config/influxdb目录下有几个yaml文件：
grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml  我们再看下用了哪些镜像：
grafana-deployment.yaml:16: image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 heapster-deployment.yaml:16: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 influxdb-deployment.yaml:16: image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1  下载镜像
我们下载好了这些images后，存储到私有镜像仓库里：
 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1  我已经将官方镜像克隆到了时速云上，镜像地址：
 index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1 index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2  需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。
docker login index.tendcloud.com  配置 参考Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI和Configuring Source，需要修改yaml文件中的几个配置。
 首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的 修改heapster-deployment.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/">Kubernetes Dashboard/Web UI安装全记录 </a> </h2>
      <div class="post-meta">Wed, Apr 5, 2017 </div>
    </header>

    （题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）
前言 前几天在CentOS7.2上安装Kubernetes1.6和安装好flannel网络配置，今天我们来安装下kuberentnes的dashboard。
Dashboard是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟Docker Enterprise Edition的Docker Datacenter很像了。
安装Dashboard 官网的安装文档https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。
首先需要一个kubernetes-dashboard.yaml的配置文件，可以直接在Github的src/deploy/kubernetes-dashboard.yaml下载。
我们能看下这个文件的内容：
# Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-network-config/">Kubernetes基于flannel的网络配置 </a> </h2>
      <div class="post-meta">Fri, Mar 31, 2017 </div>
    </header>

    （题图：西安鼓楼 Oct 4,2014）
书接上文在CentOS中安装Kubernetes详细指南，这是一个系列文章，作为学习Kubernetes的心路历程吧。
本文主要讲解Kubernetes的网络配置，👆文中有一个安装Flannel的步骤，但是安装好后并没有相应的配置说明。
配置flannel 我们直接使用的yum安装的flannle，安装好后会生成/usr/lib/systemd/system/flanneld.service配置文件。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service  可以看到flannel环境变量配置文件在/etc/sysconfig/flanneld。
# Flanneld configuration options # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=&quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&quot; # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_PREFIX=&quot;/kube-centos/network&quot; # Any additional options that you want to pass #FLANNEL_OPTIONS=&quot;&quot;   etcd的地址FLANNEL_ETCD_ENDPOINT etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX  在etcd中创建网络配置
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-network-config/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/">在CentOS上安装kubernetes详细指南 </a> </h2>
      <div class="post-meta">Thu, Mar 30, 2017 </div>
    </header>

    （题图：北京圆明园 Aug 25,2014）
作者：Jimmy Song，Peter Ma，2017年3月30日
最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是CNCF的成员之一。
这篇是根据官方安装文档实践整理的，操作系统是纯净的CentOS7.2。
另外还有一个Peter Ma写的在CentOS上手动安装kubernetes的文档可以参考。
角色分配
下面以在三台主机上安装Kubernetes为例。
172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel 172.20.0.114 node kubectl kube-proxy flannel 172.20.0.115 node kubectl kube-proxy flannel  第一台主机既作为master也作为node。
系统环境
 Centos 7.2.1511 docker 1.12.6 etcd 3.1.5 kubernetes 1.6.0 flannel 0.7.0-1  安装 下面给出两种安装方式：
 配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。 使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。  我们最终选择使用第二种方式安装。
本文的很多安装步骤和命令是参考的Kubernetes官网CentOS Manual Config文档。
第一种方式：CentOS系统中直接使用yum安装 给yum源增加一个Repo
[virt7-docker-common-release] name=virt7-docker-common-release baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/ gpgcheck=0  安装docker、kubernetes、etcd、flannel一步到位
yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel  安装好了之后需要修改一系列配置文件。
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/">Docker v.s Kubernetes part2 </a> </h2>
      <div class="post-meta">Fri, Mar 10, 2017 </div>
    </header>

     （题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)
本文是Docker v.s Kubernetes第二篇，续接上文Docker v.s Kuberntes Part1。
Kubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。
Kubernetes核心架构 master节点  apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类：  endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。   node节点  kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。  Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。
APIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。
Scheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。
工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。
工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。
工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。
Controller Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。
与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。
kubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。
kube-proxy kube-proxy提供两种功能:
 提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。  
    <footer>
        <a href='http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/">Docker v.s Kubernetes part1 </a> </h2>
      <div class="post-meta">Fri, Mar 10, 2017 </div>
    </header>

     （题图：杭州西湖 Oct 16,2016）
前言 这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看Kunernetes权威指南（第二版）这本书（P.S感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。
此系列文章中所说的docker指的是*17.03-ce*版本。
概念性的差别 Kubernetes
了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：
 Pod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job  Docker
Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：
 Service Node Stack Docker  就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。
功能性差别  Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。 Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。 Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。 Kubernetes对象选择机制继续通过label selector，用于对象调度。 Kubernetes中有一个比较特别的镜像，叫做google_containers/pause，这个镜像是用来实现Pod概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。 Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。 持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用volume plugin。 多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。  总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。
 以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。
 
    <footer>
        <a href='http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
  </div>
</section>

<aside id="meta"> </aside>

<footer>
  <div>
    <p>
    &copy; 2013-2017 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Jimmy Song</span></span>
    Powered by <a href="http://gohugo.io">Hugo</a>.
    </p>
  </div>
</footer>
</body>
</html>

