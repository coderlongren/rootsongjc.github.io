<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">
    <meta name="baidu-site-verification" content="g8IYR9SNLF" />
    <meta name="uyan_auth" content="419f63884b" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="Kubernetes : rootsongjc.github.io"/>
<meta property="og:site_name" content="rootsongjc is Jimmy Song"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="http://rootsongjc.github.io/tags/kubernetes/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-04-20"/>
<meta property="article:modified_time" content="2017-04-20"/>





<meta name="twitter:card" content="summary">

<meta name="twitter:site" content="@rootsongjc">
<meta name="twitter:title" content="Kubernetes : rootsongjc.github.io">
<meta name="twitter:creator" content="@rootsongjc">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="">
<meta name="twitter:domain" content="rootsongjc.github.io">

    <base href="http://rootsongjc.github.io/">
    <title> Kubernetes - Jimmy Song </title>
    <link rel="canonical" href="http://rootsongjc.github.io/tags/kubernetes/">
    <link href="http://rootsongjc.github.io/tags/kubernetes/index.xml" rel="alternate" type="application/rss+xml" title="Kubernetes" />

    
<link rel="stylesheet" href="/static/css/style.css">
<script src="https://yandex.st/highlightjs/8.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://yandex.st/highlightjs/8.0/styles/default.min.css">
<script>hljs.initHighlightingOnLoad();</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?11f7d254cfa4e0ca44b175c66d379ecc";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https'){
   bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
    <script type="text/javascript" src="http://tajs.qq.com/stats?sId=61142324" charset="UTF-8"></script>
</head>

<body lang="en">
<header id="header">
    <figure>
      <a href="/" border=0 id="logolink"><div class="icon-octocat" id="logo"> </div></a>
    </figure>
    <div id="byline">by Jimmy Song</div>
    <nav id="nav">
            <ul id="mainnav">
            <li>
                <a href="/blogs/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> blogs </span>
            </a>
            </li>
            <li>
            <a href="/projects/">
                <span class="icon"> <i aria-hidden="true" class="icon-console"></i></span>
                <span> projects </span>
            </a>
            </li>
            <li>
            <a href="/talks/">
                <span class="icon"> <i aria-hidden="true" class="icon-stats"></i></span>
                <span> talks </span>
            </a>
            </li>
            <li>
            <a href="http://www.linkedin.com/in/rootsongjc">
                <span class="icon"> <i aria-hidden="true" class="icon-linkedin"></i></span>
                <span> me </span>
            </a>
            </li>
        </ul>

            <ul id="social">
            <li id="share">
                <span class="icon icon-bubbles"> </span>
                <span class="title"> share </span>
                <div class="dropdown share">
                    <ul class="social">
                      <li> <a href="https://twitter.com/intent/tweet?status=Kubernetes-http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="https://plus.google.com/share?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Google+" class="googleplus"><span class="icon icon-google-plus"></span>Google+</a> </li>
                        <li> <a href="http://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f&title=Kubernetes&source=spf13" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://del.icio.us/post?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Delicious" class="delicious"><span class="icon icon-delicious"></span>Delicious</a> </li>
                        <li> <a href="http://www.reddit.com/submit?url=http%3a%2f%2frootsongjc.github.io%2ftags%2fkubernetes%2f" target="_blank" title="Reddit" class="reddit"><span class="icon icon-reddit"></span>Reddit</a> </li>
                    </ul>
                    <span class="subcount">sharing is caring</span>
                </div>
            </li>
            <li id="follow">
                <span class="icon icon-rocket"> </span>
                <span class="title"> follow </span>
                <div class="dropdown follow">
                    <ul class="social">
                        <li> <a href="http://www.twitter.com/rootsongjc" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="http://www.facebook.com/rootsongjc" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="http://www.linkedin.com/in/rootsongjc" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://github.com/rootsongjc" target="_blank" title="GitHub" class="github"><span class="icon icon-github"></span>GitHub</a> </li>
                        <li> <a href="https://www.douban.com/people/deamonj/" target="_blank" title="豆瓣" class="facebook"><span class="icon icon-idea"></span>Douban</a> </li>
                        <li> <a href="https://tuchong.com/1425795/" target="_blank" title="图虫" class="github"><span class="icon icon-cc-2"></span>Tuchong</a> </li>                    </ul>
                    <span class="subcount">join 10k+ subscribers &amp; followers</span>
                </div>
            </li>
          </ul>

    </nav>
</header>


<section id="main">
  <div>
   <h1 id="title">Kubernetes</h1>
    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/traefik-ingress-installation/">Kubernetes traefik ingress安装试用 </a> </h2>
      <div class="post-meta">Thu, Apr 20, 2017 </div>
    </header>

    （题图：🐟@鱼缸 Sep 15,2016）
前言 昨天翻了下Ingress解析，然后安装试用了下traefik，过程已同步到kubernetes-handbook上，Github地址https://github.com/rootsongjc/kubernetes-handbook。
Ingress简介 如果你还不了解，ingress是什么，可以先看下我翻译的Kubernetes官网上ingress的介绍Kubernetes Ingress解析。
理解Ingress
简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。
理解Ingress Controller
Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。
部署Traefik 介绍traefik
Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。
以下配置文件可以在kubernetes-handbookGitHub仓库中的manifests/traefik-ingress/目录下找到。
创建ingress-rbac.yaml
将用于service account验证。
apiVersion: v1 kind: ServiceAccount metadata: name: ingress namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ingress subjects: - kind: ServiceAccount name: ingress namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/traefik-ingress-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/">Kubernetes ingress解析 </a> </h2>
      <div class="post-meta">Wed, Apr 19, 2017 </div>
    </header>

    （题图：朝阳门银河SOHO Jan 31,2016）
前言 这是kubernete官方文档中Ingress Resource的翻译，因为最近工作中用到，文章也不长，也很好理解，索性翻译一下，也便于自己加深理解，同时造福kubernetes中文社区。后续准备使用Traefik来做Ingress controller，文章末尾给出了几个相关链接，实际使用案例正在摸索中，届时相关安装文档和配置说明将同步更新到kubernetes-handbook中。
术语
在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。
 节点：Kubernetes集群中的一台物理机或者虚拟机。 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。
 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。
 集群网络：一组逻辑或物理链接，可根据Kubernetes网络模型实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的OVS。
 服务：使用标签选择器标识一组pod成为的Kubernetes服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。
  什么是Ingress？ 通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：
 internet | ------------ [ Services ]  Ingress是授权入站连接到达集群服务的规则集合。
 internet | [ Ingress ] --|-----|-- [ Services ]  你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。
先决条件 在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。
GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 运行多个ingress controller 和 关闭glbc.
确定你已经阅读了Ingress controller的beta版本限制。在非GCE/GKE的环境中，你需要在pod中部署一个controller。
Ingress Resource 最简化的Ingress配置：
1: apiVersion: extensions/v1beta1 2: kind: Ingress 3: metadata: 4: name: test-ingress 5: spec: 6: rules: 7: - http: 8: paths: 9: - path: /testpath 10: backend: 11: serviceName: test 12: servicePort: 80  如果你没有配置Ingress controller就将其POST到API server不会有任何用处
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/projects/kubernetes-handbook-startup/">Kubernetes Handbook发起 </a> </h2>
      <div class="post-meta">Fri, Apr 14, 2017 </div>
    </header>

    （题图：地坛公园 Sep 27,2015）
玩转Kubernetes，我就看kubernetes handbook！
GitHub地址：https://github.com/rootsongjc/kubernetes-handbook
文章同步更新到gitbook，方便大家浏览和下载PDF。
说明 文中涉及的配置文件和代码链接在gitbook中会无法打开，请下载github源码后，在MarkDown编辑器中打开，点击链接将跳转到你的本地目录。
如何使用 方式一：在线浏览
访问gitbook：https://www.gitbook.com/book/rootsongjc/kubernetes-handbook/
方式二：本地查看
 将代码克隆到本地 安装gitbook：Setup and Installation of GitBook 执行gitbook serve 在浏览器中访问http://localhost:4000  P.S 本书中也将记录业界动态和经验分享，欢迎分享你的独到见解。
加入Kubernetes交流群，请添加我微信jimmysong。
    <footer>
        <a href='http://rootsongjc.github.io/projects/kubernetes-handbook-startup/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/projects/kubernetes-installation-document/">Kubernetes1.6集群部署完全指南 ——二进制文件部署开启TLS基于CentOS7发布 </a> </h2>
      <div class="post-meta">Thu, Apr 13, 2017 </div>
    </header>

    （题图：清晨@首都机场 Aug 13,2016）
这可能是目前为止最详细的kubernetes安装文档了。
经过几天的安装、调试、整理，今天该文档终于发布了。
你可以在这里看到文档和配置文件和我一步步部署 kubernetes1.6 集群。
或者直接下载pdf版本（2.92M）。
Kubernetes的安装繁琐，步骤复杂，该文档能够帮助跳过很多坑，节约不少时间，我在本地环境上已经安装完成，有问题欢迎在GitHub上提issue。
安装的集群详情
 Kubernetes 1.6.0 Docker 1.12.5（使用yum安装） Etcd 3.1.5 Flanneld 0.7 vxlan 网络 TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node) RBAC 授权 kublet TLS BootStrapping kubedns、dashboard、heapster(influxdb、grafana)、EFK(elasticsearch、fluentd、kibana) 集群插件 私有docker镜像仓库harbor（请自行部署，harbor提供离线安装包，直接使用docker-compose启动即可）  该文档中包括以下几个步骤
 创建 TLS 通信所需的证书和秘钥 创建 kubeconfig 文件 创建三节点的高可用 etcd 集群 kubectl命令行工具 部署高可用 master 集群 部署 node 节点 kubedns 插件 Dashboard 插件 Heapster 插件 EFK 插件  
    <footer>
        <a href='http://rootsongjc.github.io/projects/kubernetes-installation-document/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/">在开启TLS的Kubernetes1.6集群上安装EFK </a> </h2>
      <div class="post-meta">Thu, Apr 13, 2017 </div>
    </header>

    （题图：簋街 Jun 17,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署EFK日志收集监控插件。
配置和安装 EFK 官方文件目录：cluster/addons/fluentd-elasticsearch
$ ls *.yaml es-controller.yaml es-service.yaml fluentd-es-ds.yaml kibana-controller.yaml kibana-service.yaml efk-rbac.yaml  同样EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。
已经修改好的 yaml 文件见：EFK
配置 es-controller.yaml $ diff es-controller.yaml.orig es-controller.yaml 24c24 &lt; - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 --- &gt; - image: sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2  配置 es-service.yaml 无需配置；
配置 fluentd-es-ds.yaml $ diff fluentd-es-ds.yaml.orig fluentd-es-ds.yaml 26c26 &lt; image: gcr.io/google_containers/fluentd-elasticsearch:1.22 --- &gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22  配置 kibana-controller.yaml $ diff kibana-controller.yaml.orig kibana-controller.yaml 22c22 &lt; image: gcr.io/google_containers/kibana:v4.6.1-1 --- &gt; image: sz-pg-oam-docker-hub-001.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/">在开启TLS的Kubernetes1.6集群上安装heapster </a> </h2>
      <div class="post-meta">Wed, Apr 12, 2017 </div>
    </header>

    （题图：大喵 Aug 8,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署heapster，包括influxdb、grafana等组件。
配置和安装Heapster 到 heapster release 页面 下载最新版本的 heapster。
$ wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip $ unzip v1.3.0.zip $ mv v1.3.0.zip heapster-1.3.0  文件目录： heapster-1.3.0/deploy/kube-config/influxdb
$ cd heapster-1.3.0/deploy/kube-config/influxdb $ ls *.yaml grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml heapster-rbac.yaml  我们自己创建了heapster的rbac配置heapster-rbac.yaml。
已经修改好的 yaml 文件见：heapster
配置 grafana-deployment $ diff grafana-deployment.yaml.orig grafana-deployment.yaml 16c16 &lt; image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 --- &gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 40,41c40,41 &lt; # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &lt; value: / --- &gt; value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &gt; #value: /  如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为 /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://10.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/">在开启TLS的Kubernetes1.6集群上安装dashboard </a> </h2>
      <div class="post-meta">Wed, Apr 12, 2017 </div>
    </header>

    （题图：东直门桥 Aug 20,2016）
*感谢opsnull和ipchy的细心解答*。
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署dashboard。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  配置和安装 dashboard 官方文件目录：kubernetes/cluster/addons/dashboard
我们使用的文件
$ ls *.yaml dashboard-controller.yaml dashboard-service.yaml dashboard-rbac.yaml  已经修改好的 yaml 文件见：dashboard
由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：
Forbidden (403) User &quot;system:serviceaccount:kube-system:default&quot; cannot list jobs.batch in the namespace &quot;default&quot;. (get jobs.batch)  增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。
配置dashboard-service $ diff dashboard-service.yaml.orig dashboard-service.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/">Kubernetes安装之kubedns配置 </a> </h2>
      <div class="post-meta">Wed, Apr 12, 2017 </div>
    </header>

    （题图：雨过天晴@北京定福庄 Aug 27,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，使用yaml文件部署kubedns。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  安装和配置 kubedns 插件 官方的yaml文件目录：kubernetes/cluster/addons/dns。
该插件直接使用kubernetes部署，官方的配置文件中包含以下镜像：
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1  我clone了上述镜像，上传到我的私有镜像仓库：
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:1.14.1  同时上传了一份到时速云备份：
index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1  以下yaml配置文件中使用的是私有镜像仓库中的镜像。
kubedns-cm.yaml kubedns-sa.yaml kubedns-controller.yaml kubedns-svc.yaml  已经修改好的 yaml 文件见：github项目中的manifest/kubedns/目录。
系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；
$ kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/">Kubernetes高可用master节点安装 </a> </h2>
      <div class="post-meta">Tue, Apr 11, 2017 </div>
    </header>

    （题图：鬼见愁@北京西山 Sep 14,2015）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署master节点的kube-apiserver、kube-controller-manager和kube-scheduler的过程。
高可用kubernetes master节点安装 kubernetes master 节点包含的组件：
 kube-apiserver kube-scheduler kube-controller-manager  目前这三个组件需要部署在同一台机器上。
 kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关； 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；  本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）
TLS 证书文件 pem和token.csv证书文件我们在TLS证书和秘钥这一步中已经创建过了。我们再检查一下。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem  下载最新版本的二进制文件 有两种下载方式
方式一
从 github release 页面 下载发布版 tarball，解压后再执行下载脚本
$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz $ tar -xzvf kubernetes.tar.gz ... $ cd kubernetes $ .
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/">Kubernetes安装之etcd高可用配置 </a> </h2>
      <div class="post-meta">Tue, Apr 11, 2017 </div>
    </header>

    （题图：北京夜景@西山）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。
创建高可用 etcd 集群 kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为sz-pg-oam-docker-test-001.tendcloud.com、sz-pg-oam-docker-test-002.tendcloud.com、sz-pg-oam-docker-test-003.tendcloud.com：
 sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113 sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114 sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115  TLS 认证文件 需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl   kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；  下载二进制文件 到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件
$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz $ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz $ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin  创建 etcd 的 systemd unit 文件 注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/">Kubernetes安装之创建kubeconfig文件 </a> </h2>
      <div class="post-meta">Tue, Apr 11, 2017 </div>
    </header>

    (题图：北海公园 May 8,2016)
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。 kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权； kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。
创建 TLS Bootstrapping Token Token auth file
Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ') cat &gt; token.csv &lt;&lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; EOF   后三行是一句，直接复制上面的脚本运行即可。
 将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。
$cp token.csv /etc/kubernetes/  创建 kubelet bootstrapping kubeconfig 文件 $ cd /etc/kubernetes $ export KUBE_APISERVER=&quot;https://172.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/">Kubernetes安装之证书验证 </a> </h2>
      <div class="post-meta">Mon, Apr 10, 2017 </div>
    </header>

    （题图：铜牛@颐和园 Aug 25,2014）
前言 昨晚（Apr 9,2017）金山软件的opsnull发布了一个开源项目和我一步步部署kubernetes集群，下文是结合我之前部署kubernetes的过程打造的kubernetes环境和opsnull的文章创建 kubernetes 各组件 TLS 加密通信的证书和秘钥的实践。之前安装过程中一直使用的是非加密方式，一直到后来使用Fluentd和ElasticSearch收集Kubernetes集群日志时发现有权限验证问题，所以为了深入研究kubernentes。
Kubernentes中的身份验证 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
生成的 CA 证书和秘钥文件如下：
 ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem  使用证书的组件如下：
 etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem；  kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。
安装 CFSSL 方式一：直接使用二进制源码包安装
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 $ chmod +x cfssl_linux-amd64 $ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl $ wget https://pkg.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/">使用Fluentd和ElasticSearch收集Kubernetes集群日志 </a> </h2>
      <div class="post-meta">Fri, Apr 7, 2017 </div>
    </header>

    （题图：码头@古北水镇 Apr 30,2016）
前言 在安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard和配置Heapster监控插件后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。
官方文档
Kubernetes Logging and Monitoring Cluster Activity
Logging Using Elasticsearch and Kibana：不过这篇文章是在GCE上配置的，参考价值不大。
容器日志的存在形式 目前容器日志有两种输出形式：
stdout,stderr标准输出
这种形式的日志输出我们可以直接使用docker logs查看日志，kubernetes集群中同样可以使用kubectl logs类似的形式查看日志。
日志文件记录
这种日志输出我们无法从以上方法查看日志内容，只能tail日志文件查看。
Fluentd介绍 Fluentd是使用Ruby编写的，通过在后端系统之间提供统一的日志记录层来从后端系统中解耦数据源。 此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。 统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。 也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。
Fluentd收集的event由以下几个方面组成：
 Tag：字符串，中间用点隔开，如myapp.access Time：UNIX时间格式 Record：JSON格式  Fluentd特点  部署简单灵活 开源 经过验证的可靠性和性能 社区支持，插件较多 使用json格式事件格式 可拔插的架构设计 低资源要求 内置高可靠性  安装 查看cluster/addons/fluentd-elasticsearch插件目录，获取到需要用到的docker镜像名称。
$grep -rn &quot;gcr.io&quot; *.yaml es-controller.yaml:24: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 fluentd-es-ds.yaml:26: image: gcr.io/google_containers/fluentd-elasticsearch:1.22 kibana-controller.yaml:22: image: gcr.io/google_containers/kibana:v4.6.1-1  需要用到的镜像
 gcr.io/google_containers/kibana:v4.6.1-1 gcr.io/google_containers/elasticsearch:v2.4.1-2 gcr.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/">Kubernetes的ConfigMap解析 </a> </h2>
      <div class="post-meta">Thu, Apr 6, 2017 </div>
    </header>

    （题图：龙形灯笼@古北水镇 Apr 30,2016）
前言 为什么要翻译这篇文章，是因为我在使用Fluentd和ElasticSearch收集Kubernetes集群日志的时候遇到了需要修改镜像中配置的问题，fluent-plugin-kubernetes_metadata里的需要的td-agent.conf文件。
其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
ConfigMap概览 ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。
kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3  data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：
 设置环境变量的值 在容器里设置命令行参数 在数据卷里面创建config文件  用户和系统组件两者都可以在ConfigMap里面存储配置数据。
其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。
Examples: # Create a new configmap named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/">Kubernetes heapster监控插件安装文档 </a> </h2>
      <div class="post-meta">Wed, Apr 5, 2017 </div>
    </header>

    （题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）
前言 前面几篇文章中记录了我们安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard，但是还没法查看Pod的监控信息，虽然kubelet默认集成了cAdvisor（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。
安装 下载heapster的代码
直接现在Github上的最新代码。
git pull https://github.com/kubernetes/heapster.git  目前的最高版本是1.3.0。
在heapster/deploy/kube-config/influxdb目录下有几个yaml文件：
grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml  我们再看下用了哪些镜像：
grafana-deployment.yaml:16: image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 heapster-deployment.yaml:16: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 influxdb-deployment.yaml:16: image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1  下载镜像
我们下载好了这些images后，存储到私有镜像仓库里：
 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1  我已经将官方镜像克隆到了时速云上，镜像地址：
 index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1 index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2  需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。
docker login index.tendcloud.com  配置 参考Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI和Configuring Source，需要修改yaml文件中的几个配置。
 首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的 修改heapster-deployment.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/">Kubernetes Dashboard/Web UI安装全记录 </a> </h2>
      <div class="post-meta">Wed, Apr 5, 2017 </div>
    </header>

    （题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）
前言 前几天在CentOS7.2上安装Kubernetes1.6和安装好flannel网络配置，今天我们来安装下kuberentnes的dashboard。
Dashboard是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟Docker Enterprise Edition的Docker Datacenter很像了。
安装Dashboard 官网的安装文档https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。
首先需要一个kubernetes-dashboard.yaml的配置文件，可以直接在Github的src/deploy/kubernetes-dashboard.yaml下载。
我们能看下这个文件的内容：
# Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-network-config/">Kubernetes基于flannel的网络配置 </a> </h2>
      <div class="post-meta">Fri, Mar 31, 2017 </div>
    </header>

    （题图：西安鼓楼 Oct 4,2014）
书接上文在CentOS中安装Kubernetes详细指南，这是一个系列文章，作为学习Kubernetes的心路历程吧。
本文主要讲解Kubernetes的网络配置，👆文中有一个安装Flannel的步骤，但是安装好后并没有相应的配置说明。
配置flannel 我们直接使用的yum安装的flannle，安装好后会生成/usr/lib/systemd/system/flanneld.service配置文件。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service  可以看到flannel环境变量配置文件在/etc/sysconfig/flanneld。
# Flanneld configuration options # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=&quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&quot; # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_PREFIX=&quot;/kube-centos/network&quot; # Any additional options that you want to pass #FLANNEL_OPTIONS=&quot;&quot;   etcd的地址FLANNEL_ETCD_ENDPOINT etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX  在etcd中创建网络配置
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-network-config/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/">在CentOS上安装kubernetes详细指南 </a> </h2>
      <div class="post-meta">Thu, Mar 30, 2017 </div>
    </header>

    （题图：北京圆明园 Aug 25,2014）
作者：Jimmy Song，Peter Ma，2017年3月30日
最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是CNCF的成员之一。
这篇是根据官方安装文档实践整理的，操作系统是纯净的CentOS7.2。
另外还有一个Peter Ma写的在CentOS上手动安装kubernetes的文档可以参考。
角色分配
下面以在三台主机上安装Kubernetes为例。
172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel 172.20.0.114 node kubectl kube-proxy flannel 172.20.0.115 node kubectl kube-proxy flannel  第一台主机既作为master也作为node。
系统环境
 Centos 7.2.1511 docker 1.12.6 etcd 3.1.5 kubernetes 1.6.0 flannel 0.7.0-1  安装 下面给出两种安装方式：
 配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。 使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。  我们最终选择使用第二种方式安装。
本文的很多安装步骤和命令是参考的Kubernetes官网CentOS Manual Config文档。
第一种方式：CentOS系统中直接使用yum安装 给yum源增加一个Repo
[virt7-docker-common-release] name=virt7-docker-common-release baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/ gpgcheck=0  安装docker、kubernetes、etcd、flannel一步到位
yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel  安装好了之后需要修改一系列配置文件。
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/">Docker v.s Kubernetes part2 </a> </h2>
      <div class="post-meta">Fri, Mar 10, 2017 </div>
    </header>

     （题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)
本文是Docker v.s Kubernetes第二篇，续接上文Docker v.s Kuberntes Part1。
Kubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。
Kubernetes核心架构 master节点  apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类：  endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。   node节点  kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。  Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。
APIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。
Scheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。
工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。
工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。
工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。
Controller Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。
与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。
kubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。
kube-proxy kube-proxy提供两种功能:
 提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。  
    <footer>
        <a href='http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/">Docker v.s Kubernetes part1 </a> </h2>
      <div class="post-meta">Fri, Mar 10, 2017 </div>
    </header>

     （题图：杭州西湖 Oct 16,2016）
前言 这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看Kunernetes权威指南（第二版）这本书（P.S感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。
此系列文章中所说的docker指的是*17.03-ce*版本。
概念性的差别 Kubernetes
了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：
 Pod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job  Docker
Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：
 Service Node Stack Docker  就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。
功能性差别  Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。 Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。 Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。 Kubernetes对象选择机制继续通过label selector，用于对象调度。 Kubernetes中有一个比较特别的镜像，叫做google_containers/pause，这个镜像是用来实现Pod概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。 Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。 持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用volume plugin。 多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。  总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。
 以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。
 
    <footer>
        <a href='http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
  </div>
</section>

<aside id="meta"> </aside>

<footer>
  <div>
    <p>
    &copy; 2013-2017 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Jimmy Song</span></span>
    Powered by <a href="http://gohugo.io">Hugo</a>.
    </p>
  </div>
</footer>
</body>
</html>

