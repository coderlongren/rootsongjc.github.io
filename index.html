<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
	<meta name="generator" content="Hugo 0.21-DEV" />
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="Jimmy Song&#39;s Blog : rootsongjc.github.io"/>
<meta property="og:site_name" content="rootsongjc is Jimmy Song"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="http://rootsongjc.github.io/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-06-15"/>
<meta property="article:modified_time" content="2017-06-15"/>





<meta name="twitter:card" content="summary">

<meta name="twitter:site" content="@rootsongjc">
<meta name="twitter:title" content="Jimmy Song&#39;s Blog : rootsongjc.github.io">
<meta name="twitter:creator" content="@rootsongjc">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="">
<meta name="twitter:domain" content="rootsongjc.github.io">


    <base href="http://rootsongjc.github.io/">
    <title>Jimmy Song&#39;s Blog</title>
    <link rel="canonical" href="http://rootsongjc.github.io/">

    
<link rel="stylesheet" href="/static/css/style.css">
<script src="https://yandex.st/highlightjs/8.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://yandex.st/highlightjs/8.0/styles/default.min.css">
<script>
    hljs.initHighlightingOnLoad();
</script>

<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?11f7d254cfa4e0ca44b175c66d379ecc";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

<script>
    (function() {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
<div style='margin:0 auto;width:0px;height:0px;overflow:hidden; '>
    <img src='http://olz1di9xf.bkt.clouddn.com/jimmy.jpg' />
</div>
</head>
<body lang="en">

<header id="header">
    <figure>
      <a href="/" border=0 id="logolink"><div class="icon-octocat" id="logo"> </div></a>
    </figure>
    <div id="byline">by Jimmy Song</div>
    <nav id="nav">
            <ul id="mainnav">
            <li>
                <a href="/blogs/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> blogs </span>
            </a>
            </li>
            <li>
            <a href="/projects/">
                <span class="icon"> <i aria-hidden="true" class="icon-console"></i></span>
                <span> projects </span>
            </a>
            </li>
            <li>
            <a href="/talks/">
                <span class="icon"> <i aria-hidden="true" class="icon-stats"></i></span>
                <span> talks </span>
            </a>
            </li>
            <li>
            <a href="http://www.linkedin.com/in/rootsongjc">
                <span class="icon"> <i aria-hidden="true" class="icon-linkedin"></i></span>
                <span> me </span>
            </a>
            </li>
        </ul>

            <ul id="social">
            <li id="share">
                <span class="icon icon-bubbles"> </span>
                <span class="title"> share </span>
                <div class="dropdown share">
                    <ul class="social">
                      <li> <a href="https://twitter.com/intent/tweet?status=Jimmy%20Song%27s%20Blog-http%3a%2f%2frootsongjc.github.io%2f" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frootsongjc.github.io%2f" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="https://plus.google.com/share?url=http%3a%2f%2frootsongjc.github.io%2f" target="_blank" title="Google+" class="googleplus"><span class="icon icon-google-plus"></span>Google+</a> </li>
                        <li> <a href="http://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2frootsongjc.github.io%2f&title=Jimmy%20Song%27s%20Blog&source=spf13" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://del.icio.us/post?url=http%3a%2f%2frootsongjc.github.io%2f" target="_blank" title="Delicious" class="delicious"><span class="icon icon-delicious"></span>Delicious</a> </li>
                        <li> <a href="http://www.reddit.com/submit?url=http%3a%2f%2frootsongjc.github.io%2f" target="_blank" title="Reddit" class="reddit"><span class="icon icon-reddit"></span>Reddit</a> </li>
                    </ul>
                    <span class="subcount">sharing is caring</span>
                </div>
            </li>
            <li id="follow">
                <span class="icon icon-rocket"> </span>
                <span class="title"> follow </span>
                <div class="dropdown follow">
                    <ul class="social">
                        <li> <a href="http://www.twitter.com/rootsongjc" target="_blank" title="Follow me on Twitter" class="twitter"><span class="icon icon-twitter"></span>Twitter</a> </li>
                        <li> <a href="http://www.facebook.com/rootsongjc" target="_blank" title="Join me on Facebook" class="facebook"><span class="icon icon-facebook"></span>Facebook</a> </li>
                        <li> <a href="http://www.linkedin.com/in/rootsongjc" target="_blank" title="LinkedIn" class="linkedin"><span class="icon icon-linkedin"></span>LinkedIn</a> </li>
                        <li> <a href="http://github.com/rootsongjc" target="_blank" title="GitHub" class="github"><span class="icon icon-github"></span>GitHub</a> </li>
                        <li> <a href="https://www.douban.com/people/deamonj/" target="_blank" title="豆瓣" class="facebook"><span class="icon icon-idea"></span>Douban</a> </li>
                        <li> <a href="https://tuchong.com/1425795/" target="_blank" title="图虫" class="github"><span class="icon icon-cc-2"></span>Tuchong</a> </li>                    </ul>
                    <span class="subcount">join 10k+ subscribers &amp; followers</span>
                </div>
            </li>
          </ul>

    </nav>
</header>


<section id="main">
  <div>
      <h1 id="title">Jimmy Song&#39;s Blog</h1>
    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/">kubernetes管理的容器命名规则解析 </a> </h2>
      <div class="post-meta">Thu, Jun 15, 2017 </div>
    </header>

    本文将归档到kubernetes-handbook的【运维管理-监控】章节中，最终版本以kubernetes-handbook中为准。
当我们通过cAdvisor获取到了容器的信息后，例如访问${NODE_IP}:4194/api/v1.3/docker获取的json结果中的某个容器包含如下字段：
&quot;labels&quot;: { &quot;annotation.io.kubernetes.container.hash&quot;: &quot;f47f0602&quot;, &quot;annotation.io.kubernetes.container.ports&quot;: &quot;[{\&quot;containerPort\&quot;:80,\&quot;protocol\&quot;:\&quot;TCP\&quot;}]&quot;, &quot;annotation.io.kubernetes.container.restartCount&quot;: &quot;0&quot;, &quot;annotation.io.kubernetes.container.terminationMessagePath&quot;: &quot;/dev/termination-log&quot;, &quot;annotation.io.kubernetes.container.terminationMessagePolicy&quot;: &quot;File&quot;, &quot;annotation.io.kubernetes.pod.terminationGracePeriod&quot;: &quot;30&quot;, &quot;io.kubernetes.container.logpath&quot;: &quot;/var/log/pods/d8a2e995-3617-11e7-a4b0-ecf4bbe5d414/php-redis_0.log&quot;, &quot;io.kubernetes.container.name&quot;: &quot;php-redis&quot;, &quot;io.kubernetes.docker.type&quot;: &quot;container&quot;, &quot;io.kubernetes.pod.name&quot;: &quot;frontend-2337258262-771lz&quot;, &quot;io.kubernetes.pod.namespace&quot;: &quot;default&quot;, &quot;io.kubernetes.pod.uid&quot;: &quot;d8a2e995-3617-11e7-a4b0-ecf4bbe5d414&quot;, &quot;io.kubernetes.sandbox.id&quot;: &quot;843a0f018c0cef2a5451434713ea3f409f0debc2101d2264227e814ca0745677&quot; },  这些信息其实都是kubernetes创建容器时给docker container打的Labels。
你是否想过这些label跟容器的名字有什么关系？当你在node节点上执行docker ps看到的容器名字又对应哪个应用的Pod呢？
在kubernetes代码中pkg/kubelet/dockertools/docker.go中的BuildDockerName方法定义了容器的名称规范。
这段容器名称定义代码如下：
// Creates a name which can be reversed to identify both full pod name and container name. // This function returns stable name, unique name and a unique id. // Although rand.Uint32() is not really unique, but it's enough for us because error will // only occur when instances of the same container in the same pod have the same UID.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/configuration-best-practice/">Kubernetes配置最佳实践 </a> </h2>
      <div class="post-meta">Wed, Jun 14, 2017 </div>
    </header>

    （题图：青岛 May 26,2017）
前言 本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提Pull Request。
本文已上传到kubernetes-handbook中的第四章最佳实践章节，本文仅作归档，更新以kubernetes-handbook为准。
通用配置建议  定义配置文件的时候，指定最新的稳定API版本（目前是V1）。 在配置文件push到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。 使用YAML格式而不是JSON格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是YAML格式比起JSON更易读和配置。 尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考guestbook-all-in-one.yaml文件中的配置（注意，尽管你可以在使用kubectl命令时指定配置文件目录，你也可以在配置文件目录下执行kubectl create——查看下面的详细信息）。 为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉ReplicationController的selector和label，如果你希望它们跟podTemplate中的label一样的话，因为那些配置默认是podTemplate的label产生的。更多信息请查看 guestbook app 的yaml文件和 examples 。 将资源对象的描述放在一个annotation中可以更好的内省。  裸奔的Pods vs Replication Controllers和 Jobs  如果有其他方式替代“裸奔的pod”（如没有绑定到replication controller 上的pod），那么就使用其他选择。在node节点出现故障时，裸奔的pod不会被重新调度。Replication Controller总是会重新创建pod，除了明确指定了restartPolicy: Never 的场景。Job 也许是比较合适的选择。  Services  通常最好在创建相关的replication controllers之前先创建service（没有这个必要吧？）你也可以在创建Replication Controller的时候不指定replica数量（默认是1），创建service后，在通过Replication Controller来扩容。这样可以在扩容很多个replica之前先确认pod是正常的。 除非时分必要的情况下（如运行一个node daemon），不要使用hostPort（用来指定暴露在主机上的端口号）。当你给Pod绑定了一个hostPort，该pod可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 kubectl proxy and apiserver proxy 或者 kubectl port-forward。你可使用 Service 来对外暴露服务。如果你确实需要将pod的端口暴露到主机上，考虑使用 NodePort service。 跟hostPort一样的原因，避免使用 hostNetwork。 如果你不需要kube-proxy的负载均衡的话，可以考虑使用使用headless services。  使用Label  定义 labels 来指定应用或Deployment的 semantic attributes 。例如，不是将label附加到一组pod来显式表示某些服务（例如，service:myservice），或者显式地表示管理pod的replication controller（例如，controller:mycontroller），附加label应该是标示语义属性的标签， 例如{app:myapp,tier:frontend,phase:test,deployment:v3}。 这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod的服务或app是“myapp”的所有“测试”阶段组件。 有关此方法的示例，请参阅guestbook应用程序。  可以通过简单地从其service的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配replication controller的选择器，来实现跨越多个部署的服务，例如滚动更新。
    <footer>
        <a href='http://rootsongjc.github.io/blogs/configuration-best-practice/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/talks/cloud-native-go/">Cloud Native Go - 基于Go和React的web云服务构建指南 </a> </h2>
      <div class="post-meta">Tue, Jun 6, 2017 </div>
    </header>

    (题图：北京植物园桃花 Mar 26,2016)
最近在翻译Kevin Hoffman和Dan Nemeth的书《Cloud Native Go - 基于Go和React的web云原生应用构建指南》。正在进行最终的校对。本书将由电子工业出版社出版。
Cloud Native Go这本书中设计到的理论、模式、方法、工具、框架有：Micro Services、CQRS、Event Sourcing、TDD、Test-first、Docker、Wercker、Go、Blueprint、Markdown、Apiary、CloudFoundry、Pivotal Web Services、MongoDB、RabbitMQ、WebSockets、Auth0、JavaScripts、Webpack、React、Flux、Postman、Github等，从头开始教你如何使用Go语言构建云原生Web应用程序。
简介 Cloud Native Go向开发人员展示如何构建大规模云应用程序，在满足当今客户的强大需求的同时还可以动态扩展来处理几乎任何规模的数据量、流量或用户。
Kevin Hoffman和Dan Nemeth详细描述了现代云原生应用程序，阐明了与快速、可靠的云原生开发相关的因素、规则和习惯。他们还介绍了Go这种“简单优雅”的高性能语言，它特别适合于云开发。
在本书中你将使用Go语言创建微服务，使用ReactJS和Flux添加前端Web组件，并掌握基于Go的高级云原生技术。Hoffman和Nemeth展示了如何使用Wercker、Docker和Dockerhub等工具构建持续交付管道; 自动推送应用程序到平台上; 并系统地监控生产中的应用程序性能。
 学习“云之道”：为什么开发好的云软件基本上是关于心态和规则 了解为什么使用Go语言是云本地微服务开发的理想选择 规划支持持续交付和部署的云应用程序 设计服务生态系统，然后以test-first的方式构建它们
 将正在进行的工作推送到云
 使用事件源和CQRS模式来响应大规模和高吞吐量
 安全的基于云的Web应用程序：做与不做的选择
 使用第三方消息传递供应商创建响应式云应用程序
 使用React和Flux构建大规模，云友好的GUI
 监控云中的动态扩展，故障转移和容错
  下面先罗列下目录，以飨读者。P.S 该目录跟出版时候的目录可能有所不同。
目录 Cloud Native Go. 1
构建基于Go和React的云原生Web应用&hellip; 1
云服务构建完全指南&hellip; 1
目录&hellip; 4
前言&hellip; 8
关于作者&hellip; 9
致谢&hellip; 9
第1章 云之道&hellip; 10 云的优势&hellip; 10
    <footer>
        <a href='http://rootsongjc.github.io/talks/cloud-native-go/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/istio-overview/">Istio简介 </a> </h2>
      <div class="post-meta">Fri, Jun 2, 2017 </div>
    </header>

    （题图：威海朱口 May 29,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
Istio是由Google、IBM和Lyft开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。
简介 使用istio可以很简单的创建具有负载均衡、服务间认证、监控等功能的服务网络，而不需要对服务的代码进行任何修改。你只需要在部署环境中，例如Kubernetes的pod里注入一个特别的sidecar proxy来增加对istio的支持，用来截获微服务之间的网络流量。
目前版本的istio只支持kubernetes，未来计划支持其他其他环境。
特性 使用istio的进行微服务管理有如下特性：
 流量管理：控制服务间的流量和API调用流，使调用更可靠，增强不同环境下的网络鲁棒性。 可观测性：了解服务之间的依赖关系和它们之间的性质和流量，提供快速识别定位问题的能力。 策略实施：通过配置mesh而不是以改变代码的方式来控制服务之间的访问策略。 服务识别和安全：提供在mesh里的服务可识别性和安全性保护。  未来将支持多种平台，不论是kubernetes、Mesos、还是云。同时可以集成已有的ACL、日志、监控、配额、审计等。
架构 Istio架构分为控制层和数据层。
 数据层：由一组智能代理（Envoy）作为sidecar部署，协调和控制所有microservices之间的网络通信。 控制层：负责管理和配置代理路由流量，以及在运行时执行的政策。  Envoy Istio使用Envoy代理的扩展版本，该代理是以C++开发的高性能代理，用于调解service mesh中所有服务的所有入站和出站流量。 Istio利用了Envoy的许多内置功能，例如动态服务发现，负载平衡，TLS终止，HTTP/2＆gRPC代理，断路器，运行状况检查，基于百分比的流量拆分分阶段上线，故障注入和丰富指标。
Envoy在kubernetes中作为pod的sidecar来部署。 这允许Istio将大量关于流量行为的信号作为属性提取出来，这些属性又可以在Mixer中用于执行策略决策，并发送给监控系统以提供有关整个mesh的行为的信息。 Sidecar代理模型还允许你将Istio功能添加到现有部署中，无需重新构建或重写代码。 更多信息参见设计目标。
Mixer Mixer负责在service mesh上执行访问控制和使用策略，并收集Envoy代理和其他服务的遥测数据。代理提取请求级属性，发送到mixer进行评估。有关此属性提取和策略评估的更多信息，请参见Mixer配置。 混音器包括一个灵活的插件模型，使其能够与各种主机环境和基础架构后端进行接口，从这些细节中抽象出Envoy代理和Istio管理的服务。
Istio Manager Istio-Manager用作用户和Istio之间的接口，收集和验证配置，并将其传播到各种Istio组件。它从Mixer和Envoy中抽取环境特定的实现细节，为他们提供独立于底层平台的用户服务的抽象表示。 此外，流量管理规则（即通用4层规则和七层HTTP/gRPC路由规则）可以在运行时通过Istio-Manager进行编程。
Istio-auth Istio-Auth提供强大的服务间和最终用户认证，使用相互TLS，内置身份和凭据管理。它可用于升级service mesh中的未加密流量，并为运营商提供基于服务身份而不是网络控制的策略的能力。 Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括属性和基于角色的访问控制以及授权hook）来控制和监控访问你服务、API或资源的人员。
参考 Istio开源平台发布，Google、IBM和Lyft分别承担什么角色？
Istio：用于微服务的服务啮合层
Istio Overview
    <footer>
        <a href='http://rootsongjc.github.io/blogs/istio-overview/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/istio-installation/">istio安装笔记 </a> </h2>
      <div class="post-meta">Thu, Jun 1, 2017 </div>
    </header>

    （题图：威海东部海湾 May 28,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
本文根据官网的文档整理而成，步骤包括安装istio 0.1.5并创建一个bookinfo的微服务来测试istio的功能。
文中使用的yaml文件可以在kubernetes-handbook的manifests/istio目录中找到，所有的镜像都换成了我的私有镜像仓库地址，请根据官网的镜像自行修改。
安装环境  CentOS 7.3.1611 Docker 1.12.6 Kubernetes 1.6.0  安装 1.下载安装包
下载地址：https://github.com/istio/istio/releases
下载Linux版本的当前最新版安装包
wget https://github.com/istio/istio/releases/download/0.1.5/istio-0.1.5-linux.tar.gz  2.解压
解压后，得到的目录结构如下：
. ├── bin │ └── istioctl ├── install │ └── kubernetes │ ├── addons │ │ ├── grafana.yaml │ │ ├── prometheus.yaml │ │ ├── servicegraph.yaml │ │ └── zipkin.yaml │ ├── istio-auth.yaml │ ├── istio-rbac-alpha.yaml │ ├── istio-rbac-beta.yaml │ ├── istio.yaml │ ├── README.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/istio-installation/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-filebeat/">使用filebeat收集kubernetes中的应用日志 </a> </h2>
      <div class="post-meta">Wed, May 17, 2017 </div>
    </header>

    （题图：民生现代美术馆 May 14,2017）
前言 本文已同步更新到Github仓库kubernetes-handbook中。
昨天写了篇文章使用Logstash收集Kubernetes的应用日志，发现logstash十分消耗内存（大约500M），经人提醒改用filebeat（大约消耗10几M内存），因此重写一篇使用filebeat收集kubernetes中的应用日志。
在进行日志收集的过程中，我们首先想到的是使用Logstash，因为它是ELK stack中的重要成员，但是在测试过程中发现，Logstash是基于JDK的，在没有产生日志的情况单纯启动Logstash就大概要消耗500M内存，在每个Pod中都启动一个日志收集组件的情况下，使用logstash有点浪费系统资源，经人推荐我们选择使用Filebeat替代，经测试单独启动Filebeat容器大约会消耗12M内存，比起logstash相当轻量级。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash，现使用filebeat替代 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/filebeat:5.4.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件fielbeat-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: filebeat-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: filebeat-test spec: containers: - image: sz-pg-oam-docker-hub-001.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-filebeat/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-logstash/">使用Logstash收集Kubernetes的应用日志 </a> </h2>
      <div class="post-meta">Tue, May 16, 2017 </div>
    </header>

    （题图：798艺术区 May 14,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
很多企业内部都有自己的ElasticSearch集群，我们没有必要在kubernetes集群内部再部署一个，而且这样还难于管理，因此我们考虑在容器里部署logstash收集日志到已有的ElasticSearch集群中。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/logstash:5.3.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件logstash-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: logstash-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: logstash-test spec: containers: - image: sz-pg-oam-docker-hub-001.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-logstash/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/">Kubernete概念解析之Deployment </a> </h2>
      <div class="post-meta">Sat, May 13, 2017 </div>
    </header>

    （题图：京广桥@北京国贸 Apr 30,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
本文翻译自kubernetes官方文档：https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/controllers/deployment.md
本文章根据2017年5月10日的Commit 8481c02翻译。
Deployment是Kubernetes中的一个非常重要的概念，从它开始是了解kubernetes中资源概念的一个很好的切入点，看到网上也没什么详细的说明文档，我就随手翻译了一下官方文档（Github中的文档），kubernetes官网上的文档还没有这个新。这篇文章对Deployment的概念解释的面面俱到十分详尽。
Deployment是什么？ Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。
你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。
一个典型的用例如下：
 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清楚旧的不必要的ReplicaSet。  创建Deployment 下面是一个Deployment示例，它创建了一个Replica Set来启动3个nginx pod。
下载示例文件并执行命令：
$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record deployment &quot;nginx-deployment&quot; created  将kubectl的 —record 的flag设置为 true可以在annotation中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个Deployment revision中执行了哪些命令。
然后立即执行getí将获得如下结果：
$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s  输出结果表明我们希望的repalica数是3（根据deployment中的.spec.replicas配置）当前replica数（ .status.replicas）是0, 最新的replica数（.status.updatedReplicas）是0，可用的replica数（.status.availableReplicas）是0。
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/">Kubernetes中的Rolling Update服务滚动升级 </a> </h2>
      <div class="post-meta">Wed, May 10, 2017 </div>
    </header>

    （题图：后海夜色 Apr 30,2017）
前言 本文已同步到gitbook kubernetes-handbook的第8章第1节。
本文说明在Kubernetes1.6中服务如何滚动升级，并对其进行测试。
当有镜像发布新版本，新版本服务上线时如何实现服务的滚动和平滑升级？
如果你使用ReplicationController创建的pod可以使用kubectl rollingupdate命令滚动升级，如果使用的是Deployment创建的Pod可以直接修改yaml文件后执行kubectl apply即可。
Deployment已经内置了RollingUpdate strategy，因此不用再调用kubectl rollingupdate命令，升级的过程是先创建新版的pod将流量导入到新pod上后销毁原来的旧的pod。
Rolling Update适用于Deployment、Replication Controller，官方推荐使用Deployment而不再使用Replication Controller。
使用ReplicationController时的滚动升级请参考官网说明：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/
ReplicationController与Deployment的关系 ReplicationController和Deployment的RollingUpdate命令有些不同，但是实现的机制是一样的，关于这两个kind的关系我引用了ReplicationController与Deployment的区别中的部分内容如下，详细区别请查看原文。
ReplicationController Replication Controller为Kubernetes的一个核心内容，应用托管到Kubernetes之后，需要保证应用能够持续的运行，Replication Controller就是这个保证的key，主要的功能如下：
 确保pod数量：它会确保Kubernetes中有指定数量的Pod在运行。如果少于指定数量的pod，Replication Controller会创建新的，反之则会删除掉多余的以保证Pod数量不变。 确保pod健康：当pod不健康，运行出错或者无法提供服务时，Replication Controller也会杀死不健康的pod，重新创建新的。 弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过Replication Controller动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取Replication Controller关联pod的整体资源使用情况，做到自动伸缩。 滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。  Deployment Deployment同样为Kubernetes的一个核心内容，主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：
 Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。 事件和状态查看：可以查看Deployment的升级详细进度和状态。 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动：对于每一次升级，都能够随时暂停和启动。 多种升级方案：Recreate：删除所有已存在的pod,重新创建新的; RollingUpdate：滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。  创建测试镜像 我们来创建一个特别简单的web服务，当你访问网页时，将输出一句版本信息。通过区分这句版本信息输出我们就可以断定升级是否完成。
所有配置和代码见Github上的manifests/test/rolling-update-test目录。
Web服务的代码main.go
package main import ( &quot;fmt&quot; &quot;log&quot; &quot;net/http&quot; ) func sayhello(w http.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
        <article class="post">
    <header>
      <h2><a href="http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/">Kubernetes的边缘节点配置 </a> </h2>
      <div class="post-meta">Tue, May 9, 2017 </div>
    </header>

    （题图：南屏晚钟@圆明园 May 6,2017）
前言 为了配置kubernetes中的traefik ingress的高可用，对于kubernetes集群以外只暴露一个访问入口，需要使用keepalived排除单点问题。本文参考了kube-keepalived-vip，但并没有使用容器方式安装，而是直接在node节点上安装。
本文已同步到gitbook kubernetes-handbook的第2章第5节。
定义 首先解释下什么叫边缘节点（Edge Node），所谓的边缘节点即集群内部用来向集群外暴露服务能力的节点，集群外部的服务通过该节点来调用集群内部的服务，边缘节点是集群内外交流的一个Endpoint。
边缘节点要考虑两个问题
 边缘节点的高可用，不能有单点故障，否则整个kubernetes集群将不可用 对外的一致暴露端口，即只能有一个外网访问IP和端口  架构 为了满足边缘节点的以上需求，我们使用keepalived来实现。
在Kubernetes集群外部配置nginx来访问边缘节点的VIP。
选择Kubernetes的三个node作为边缘节点，并安装keepalived。
准备 复用kubernetes测试集群的三台主机。
172.20.0.113
172.20.0.114
172.20.0.115
安装 使用keepalived管理VIP，VIP是使用IPVS创建的，IPVS已经成为linux内核的模块，不需要安装
LVS的工作原理请参考：http://www.cnblogs.com/codebean/archive/2011/07/25/2116043.html
不使用镜像方式安装了，直接手动安装，指定三个节点为边缘节点（Edge node）。
因为我们的测试集群一共只有三个node，所有在在三个node上都要安装keepalived和ipvsadmin。
yum install keepalived ipvsadm  配置说明 需要对原先的traefik ingress进行改造，从以Deployment方式启动改成DeamonSet。还需要指定一个与node在同一网段的IP地址作为VIP，我们指定成172.20.0.119，配置keepalived前需要先保证这个IP没有被分配。。
 Traefik以DaemonSet的方式启动 通过nodeSelector选择边缘节点 通过hostPort暴露端口 当前VIP漂移到了172.20.0.115上 Traefik根据访问的host和path配置，将流量转发到相应的service上  配置keepalived 参考基于keepalived 实现VIP转移，lvs，nginx的高可用，配置keepalived。
keepalived的官方配置文档见：http://keepalived.org/pdf/UserGuide.pdf
配置文件/etc/keepalived/keepalived.conf文件内容如下：
! Configuration File for keepalived global_defs { notification_email { root@localhost } notification_email_from kaadmin@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 172.
    <footer>
        <a href='http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/'><nobr>Read more →</nobr></a>
    </footer>
</article>

    
  </div>
</section>

<aside id="meta"> </aside>

<footer>
  <div>
    <p>
    &copy; 2013-2017 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Jimmy Song</span></span>
    Powered by <a href="http://gohugo.io">Hugo</a>.
    </p>
  </div>
</footer>
</body>
</html>

